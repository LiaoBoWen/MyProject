<<<=====================================================>>>
初定掌握   词=》语法=》语义
<<<=====================================================>>>
# 跟随《兜哥带你入门nlp》路线学习、还有‘战斗名族’的Github项目、微信收藏的入门和进阶方法、配合黄鸿波的学习思维导==>《文本分类如何选择模型》、《21个项目搞定深度学习》，再到github寻找项目资源(star)
# 熟悉模型：FastText、TextCnn、TextRnn、TextRCnn、TextRank、Elmo(https://blog.csdn.net/triplemeng/article/details/82380202)、HAN（分层注意网络）、具有注意的seq2seq模型（seq2seq with attention）、胶囊网络、glove、N-gram(与tf-tdf有关联……需要深入了解，教程未完……)      模型:最新的语言模型：transformer，bert、 深度强化学习之自然语言处理
# transformer xl-trnasformer bert elmo GPT
#
# 了解一下LSTM的变种,比如TreeLSTM
# 对流行的分词工具（哈工大的pyltp）的了解
# 熟悉对 hmm、crf做了解
# 使用不同的词向量的生成方法
# 文本分类、词性标注、新词发现、词义消歧、文本聚类、情感分析、语义分析、问答系统、知识图谱
# 文本的预处理，去除停用词(因为可有可无，对语句不会造成太大的影响，可以下载停用词文本？)
# 预训练模型的使用
# oov的处理方法（参考fasttext的解决办法：https://blog.csdn.net/sinat_26917383/article/details/83041424#35_fasttextOOV_225 ）
# 尝试模型保存时候使用移动平均（https://www.cnblogs.com/hrlnw/p/8067214.html）
# 解决显存带来的句长限制：Dynamic Batching

# laywer-wise Relevance Propagation 可视化端到端机器翻译系统

# spaCy处理NER任务

# GBDT 的分裂标准

# Dataset的使用

FSA(有限状态机)

<<<=====================================================>>>
任务类型
<<<=====================================================>>>

#### 多标签怎么处理？？？？？？
* 文本分类
* 中文自动分词
* 情感分析
* #机器翻译  （使用多层的LSTM-Attention或者多层的transformer或者bert。推荐开源的transformer：Marian、Nematus、Sockeye）
* #自然语言生成
#####  参考微信收藏 https://mp.weixin.qq.com/s/BBSXi0Zj5fe61bZG0zeIiQ
* #实体识别 实体统一 实体消歧 指代消歧 信息抽取     CRF:https://www.cnblogs.com/pinard/p/7068574.html
 知识图谱 实体与关系
 句法分析 （通过统计的方法的话，一般使用LSTM-CRF的模型进行建模）
 词性标注 （其任务类型与实体识别的方法一致，LSTM用来捕捉长距离的限制，CRF用来整句上的特征提取,根据特征模板进行标注，可以有效的保留整句的上下文关系）

 自动摘要 （page rank）
 问答系统 （对于语料库可以使用倒排表调高效率）
 意图理解
 文本关键句提取 关键词提取方法(KEA算法)
 指代消解 （可以使用上下文关系与自身词性作为特征，达到消歧的目的。或者通过句法分析的手段（我认为））
 文字蕴涵
 信息检索
 文字校对
 新词发现


<<<=====================================================>>>
Q && A
<<<=====================================================>>>
Q:为什么使用LSTM-CRF的结构
A:在解决序列标注问题时，待标注序列的前后关系是研究的重点，而标注序列的前后关系，可以通过BiLSTM(Bidirectional LSTM)来获得。
  BiLSTM借助存储单元的结构来保存较长的依赖关系，并且通过输入门、输出门和遗忘门来调整之前状态对当前存储单元状态的影响。
  然而BiLSTM缺乏在整句层面的特征分析，所以需要借助条件随机场(Conditional Random Fields, CRF)的帮助。
  CRF将序列标注的重点放在句子级别上，根据特征模板来进行标注，通过Viterbi解码来获得最优解。
  然而CRF有提取特征困难，适用性不够广的问题，因此可以将CRF和LSTM结合起来，这样在保证能够提取足够整句特征的同时，使用有效的序列标注方法进行标注。

  文本输入经过BiLSTM之后，会把前后向的隐藏态结果进行结合，生成BiLSTM的输出。
  最后，将BiLSTM的输出喂给CRF作为输入，这样就形成了BiLSTM- CRF结构。
  这种结构结合了BiLSTM和CRF的特点和优势：作为BiLSTM，它可以有效地保存整句的前后信息，提取句子中的特征信息；
  作为CRF，它能够利用上下文的信息，进行具有很高准确率的序列标注。

<<<=====================================================>>>
项目类型
<<<=====================================================>>>
问答系统 :用户输入纠错, 文本处理,过滤,针对于语料库要使用倒排表,基于相似度来计算获得最优的答案
翻译 :lstm-attention 或者 多层transformer,bert
知识图谱 : 非结构haunted数据提取实体,以及词典库的构建,关系的提取(指定的关系),实体同意以及实体消歧,知识图谱的构建以及查询
任务导向型聊天机器人 :文本预处理,意图识别与关键信息提取,对于每一个意图设计对话管理状态机,设计处理上下文处理的方法,对话生成模块, 处理常见的boundary case
对话系统的NLU(是否包含intent与Entity Extraction的NLU模块) :文本特征的提取,搭建CRF来识别关键词,搭建lstm-crf识别关键词




<<<=====================================================>>>
已学
<<<=====================================================>>>
1、词袋模型和TF-IDF   ==>  内部算法
2、Word2Vec训练词向量 Doc2Vec训练文本  ==>案例练习 ==>(内部算法
3、test_classification_rnn_cnn  ==》分别采用RNN，CNN的字符级别的文本分类器，该项目中CNN表现的效果更加好一点，这种基本不是原意的字符级别的特征，也能从统计意义上表征文本从而作为特征
4、LDA的学习（未完成！）   还有LSA（未学习！）        https://www.mlln.cn/2018/02/07/gensim-lda-%E6%96%87%E6%A1%A3%E4%B8%BB%E9%A2%98%E6%8F%90%E5%8F%96%E5%AE%9E%E7%8E%B0/     https://www.cnblogs.com/chenbjin/p/5638904.html
5、TextRank自动提取文档的关键词
6、TextCNN （注意还有多通道（最多2通道）的TextCNN存在【未完成！】，这里的多通道就是指一个与训练的embedding和一个随机的embedding，使用stack【axis=-1】）用于英文垃圾邮件任务（用于情感分析的项目还没看https://github.com/norybaby/sentiment_analysis_textcnn）
7、TextRNN 用于知乎用户的评论的分类  项目内容完整庞大
8、TextRCNN  对文本进行分类操作
9、Attention Model   它在很多任务上都有十分出色的表现，比如阅读理解 、文本继承 、自动文本摘要
10、HAN （未完成） 相较于TextCNN，HAN最大的进步在于完全保留了文章的结构信息，并且特别难能可贵的是，基于attention结构有很强的解释性。
（到这里可能注意到了我们的网络的深度都不是很深，只有2~3层，但是何凯明的网络告诉我们网络的层数是很重要的，由此引出了DPCNN）http://www.52nlp.cn/tag/han
11、机器人对话(seq2seq) 训练数据的清洗（正则表达式），句子编码解码训练（模型的保存），问答对数据的分组（模型的保存）   模型训练出现问题，其余部分完好，有待解决……
11、seq2seq(机器翻译)  机器翻译的评价指标（https://blog.csdn.net/joshuaxx316/article/details/58696552）
12、transformer self-attention，单词之间两两比较，每个单词的出现都考虑其他的单词，单词之间互相比较，但是为了解决句子之间的顺序的问题，我们需要加入一个position-encode
13



- epoch的设置可以根据acc-epoch图像来判断设置多大，在哪个epoch的的时候开始几乎不再提高
- 当网络过于简单，把模型复杂化一点点，但是还是要合理的，但是有时候甚至还会更加糟糕
- 可以使用word2vec作为作为初始值赋值使用embedding_lookup方法进行更好的调整

<<<=====================================================>>>
模型的探索
<<<=====================================================>>>
探索：各模型对哪类文本任务具有更好的表现力？
参数的设置问题，各个参数之间的关联




<<<=====================================================>>>
还要了解
<<<=====================================================>>>
分词的时候要注意 ：专业词汇、领域词汇、近义词、为什么有时候分词需要在文段开始或者结束时加标识符?
还要了解：朴素贝叶斯做分类、迁移学习、长文本、短文本、LDA、表征学习是个什么东东、潜在语义分析LSA、插值法、回退法、长距离信息的使用、熵模型、
未知词、nlp算法和传统机器学习的搭配使用、N-gram中的各种平滑算法思想（https://blog.csdn.net/baimafujinji/article/details/51297802）、

熟悉Keras！！！

激活函数的技巧

dropout随机失活，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。

各种的激活函数

Barch-Norm https://blog.csdn.net/marsjhao/article/details/72876460

Doc2Vec

模型优化方法 + 模型蒸馏 + 模型压缩

还有不同的优化器

损失函数的优缺点

ckpt 文件的是个文件分别存储的什么数据 meta保存的网络结构， index保存的key（tensor名）-value（BundleEntryProto，BundleEntryProto），data保存的是模型所有的变量值，batch_norm的mean，std不会保存在trainable_variable里面，但是会出现在global_variables里面：https://blog.csdn.net/u012871045/article/details/80784286

歧义性

停用词 steming

编辑距离    https://cuiqingcai.com/6101.html

backward train / forword train

语言模型一大堆……

tf_hub 加载训练好的ELMO模型，做迁移学习

多模型的ensemble 、 Fine-tuning 、 re-rank 、 back-translation

attention 和 self-attention


<<<=====================================================>>>
题外话
<<<=====================================================>>>
题外话：Fine ture 用途？
DAN


<<<=====================================================>>>
大纲
<<<=====================================================>>>
1.1 NLP算法复杂度
二、 除了一直的，还要必备知识：
2.1 Master's theorem
2.2 P和NP问题
2.3 简单的动态规划问题解释
三、搭建简单的问答系统
3.1 文本分析流程（pipeline）
3.2 简答问答系统架构
3.3 拼写错误以及编辑距离
3.4 停用词过滤，Stemming
3.5 文本表示技术（tf-idf，word2vec)
3.6 句子相似度计算
3.7 倒排表
3.8 基于检索的问答系统
依存句法分析
上下文法分析
CKY算法
MLE,MAP, 贝叶斯估计
Conjugate Prior
主题模型
....... http://greedyai.com/course/47/tasks



<<<=====================================================>>>
其他的原理
<<<=====================================================>>>


tensorflow进行自然语言处理经常用的就那些东西~ 都掌握就好了！加油！！！



关键词 + 实体 ==>  结构化数据 智能音箱