{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data1 = pd.read_csv('./data/TRAINSET_NEWS.csv')\n",
    "Data2 = pd.read_csv('./data/TRAINSET_STOCK.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26299</th>\n",
       "      <td>20190326_12</td>\n",
       "      <td>20190326</td>\n",
       "      <td>李克强主持召开国务院常务会议</td>\n",
       "      <td>国务院总理李克强3月26日主持召开国务院常务会议，落实降低社会保险费率部署，明确具体配套措施...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26300</th>\n",
       "      <td>20190326_13</td>\n",
       "      <td>20190326</td>\n",
       "      <td>习近平同法国总统会谈</td>\n",
       "      <td>欢迎仪式后，在威武整齐的摩托车队护卫下，习近平乘车前往爱丽舍宫，沿途100多名法兰西共和国卫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26301</th>\n",
       "      <td>20190326_14</td>\n",
       "      <td>20190326</td>\n",
       "      <td>习近平出席法国总统举行的欢迎仪式</td>\n",
       "      <td>在中华人民共和国和法兰西共和国建交55周年之际，中国国家主席习近平时隔五年之后再次对法国进行...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26302</th>\n",
       "      <td>20190327_00</td>\n",
       "      <td>20190327</td>\n",
       "      <td>联播快讯</td>\n",
       "      <td>全国开展危险化学品安全隐患排查国务院安委办、应急管理部今天（27日）召开视频会议，要求在全国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26303</th>\n",
       "      <td>20190327_01</td>\n",
       "      <td>20190327</td>\n",
       "      <td>国新办发表《伟大的跨越：西藏民主改革60年》白皮书</td>\n",
       "      <td>国务院新闻办公室今天（27日）发表《伟大的跨越：西藏民主改革60年》白皮书。白皮书全文约2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26304</th>\n",
       "      <td>20190327_02</td>\n",
       "      <td>20190327</td>\n",
       "      <td>新华社综述：谱写新时代中国梦的雪域篇章——以习近平同志为核心的党中央治边稳藏富民新实践</td>\n",
       "      <td>新华社今天（27日）播发综述《谱写新时代中国梦的雪域篇章——以习近平同志为核心的党中央治边稳...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26305</th>\n",
       "      <td>20190327_03</td>\n",
       "      <td>20190327</td>\n",
       "      <td>中共中央办公厅印发《公务员职务与职级并行规定》</td>\n",
       "      <td>近日，中共中央办公厅印发了《公务员职务与职级并行规定》，并发出通知，要求各地区各部门认真遵照...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26306</th>\n",
       "      <td>20190327_04</td>\n",
       "      <td>20190327</td>\n",
       "      <td>韩正在国家医疗保障局调研并主持召开座谈会</td>\n",
       "      <td>中共中央政治局常委、国务院副总理韩正26日到国家医疗保障局调研。韩正观看了打击欺诈骗取医保基...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26307</th>\n",
       "      <td>20190327_05</td>\n",
       "      <td>20190327</td>\n",
       "      <td>李克强会见圣多美和普林西比总理</td>\n",
       "      <td>国务院总理李克强今天（27日）在海南会见来华出席博鳌亚洲论坛2019年年会的圣多美和普林西比...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26308</th>\n",
       "      <td>20190327_06</td>\n",
       "      <td>20190327</td>\n",
       "      <td>李克强会见博鳌亚洲论坛理事会成员</td>\n",
       "      <td>国务院总理李克强今天（27日）在海南博鳌会见博鳌亚洲论坛理事长潘基文和理事会部分成员。李克强...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26309</th>\n",
       "      <td>20190327_07</td>\n",
       "      <td>20190327</td>\n",
       "      <td>李克强在海南考察时强调 更大激发市场主体活力 着力破解发展和民生难题</td>\n",
       "      <td>27日，在出席博鳌亚洲论坛期间，中共中央政治局常委、国务院总理李克强在海南海口考察。他强调，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26310</th>\n",
       "      <td>20190327_08</td>\n",
       "      <td>20190327</td>\n",
       "      <td>新华社长篇通讯：共绘美美与共的人类文明画卷——写在习近平主席在联合国教科文组织总部演讲五周年之际</td>\n",
       "      <td>新华社今天（27日）播发长篇通讯《共绘美美与共的人类文明画卷——写在习近平主席在联合国教科文...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26311</th>\n",
       "      <td>20190327_09</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平同出席中法全球治理论坛闭幕式的欧洲领导人举行会晤</td>\n",
       "      <td>国家主席习近平当地时间26日在巴黎同出席中法全球治理论坛闭幕式的法国总统马克龙、德国总理默克...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26312</th>\n",
       "      <td>20190327_10</td>\n",
       "      <td>20190327</td>\n",
       "      <td>又踏层峰望眼开——习近平主席访问意大利 摩纳哥 法国成果丰硕</td>\n",
       "      <td>2019年3月21日至26日，国家主席习近平应邀对意大利、摩纳哥、法国进行国事访问，引领中国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26313</th>\n",
       "      <td>20190327_11</td>\n",
       "      <td>20190327</td>\n",
       "      <td>文明交流互鉴推动合作共赢和平发展</td>\n",
       "      <td>五年前的今天，习近平主席在联合国教科文组织进行历史性访问并发表重要演讲，全面深刻地阐述了文明...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26314</th>\n",
       "      <td>20190327_12</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平会见德国总理</td>\n",
       "      <td>国家主席习近平当地时间26日在巴黎会见专程前来出席中法全球治理论坛闭幕式的德国总理默克尔。习...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26315</th>\n",
       "      <td>20190327_13</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平会见法国总理</td>\n",
       "      <td>国家主席习近平当地时间26日在巴黎总理府会见法国总理菲利普。习近平抵达总理府时，菲利普在停车...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26316</th>\n",
       "      <td>20190327_14</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平和法国总统共同出席中法全球治理论坛闭幕式</td>\n",
       "      <td>国家主席习近平当地时间26日在巴黎同法国总统马克龙一道出席中法全球治理论坛闭幕式。德国总理默...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26317</th>\n",
       "      <td>20190327_15</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平会见法国参议长</td>\n",
       "      <td>国家主席习近平当地时间26日在巴黎会见法国参议长拉尔歇。习近平抵达参议院时，拉尔歇议长在停车...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26318</th>\n",
       "      <td>20190327_16</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平和彭丽媛出席法国总统举行的隆重欢送仪式</td>\n",
       "      <td>国家主席习近平当地时间26日结束对法国的国事访问。离开巴黎之前，习近平和夫人彭丽媛出席法国总...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26319</th>\n",
       "      <td>20190327_17</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平结束对意大利共和国 摩纳哥公国 法兰西共和国国事访问回到北京</td>\n",
       "      <td>3月27日，在结束对意大利共和国、摩纳哥公国、法兰西共和国国事访问后，国家主席习近平回到北京...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26320</th>\n",
       "      <td>20190327_18</td>\n",
       "      <td>20190327</td>\n",
       "      <td>习近平会见法国国民议会议长</td>\n",
       "      <td>国家主席习近平当地时间26日在巴黎会见法国国民议会议长费朗。习近平抵达国民议会时，费朗议长在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26321</th>\n",
       "      <td>20190328_00</td>\n",
       "      <td>20190328</td>\n",
       "      <td>人民日报社论：铭记伟大变革 激扬奋进力量——纪念西藏民主改革六十周年</td>\n",
       "      <td>今天出版的人民日报发表社论《铭记伟大变革激扬奋进力量——纪念西藏民主改革六十周年》。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26322</th>\n",
       "      <td>20190328_01</td>\n",
       "      <td>20190328</td>\n",
       "      <td>美承认以色列对戈兰高地主权遭反对</td>\n",
       "      <td>27日，联合国安理会紧急开会讨论戈兰高地问题。叙利亚常驻联合国代表贾法里谴责美国单方面承认以...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26323</th>\n",
       "      <td>20190328_02</td>\n",
       "      <td>20190328</td>\n",
       "      <td>各界积极评价习近平主席访问欧洲三国成果</td>\n",
       "      <td>日前，中国国家主席习近平应邀对意大利、摩纳哥、法国进行国事访问。三国各界人士称赞习主席此次访...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26324</th>\n",
       "      <td>20190328_03</td>\n",
       "      <td>20190328</td>\n",
       "      <td>国内联播快讯</td>\n",
       "      <td>最高法发布优化营商环境司法解释最高人民法院今天对外发布优化营商环境司法解释，要求依法保护产权...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26325</th>\n",
       "      <td>20190328_04</td>\n",
       "      <td>20190328</td>\n",
       "      <td>第七届全国道德模范评选表彰活动启动</td>\n",
       "      <td>中央宣传部、中央文明办、全国总工会、共青团中央、全国妇联、中央军委政治工作部今天在京召开电视...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26326</th>\n",
       "      <td>20190328_05</td>\n",
       "      <td>20190328</td>\n",
       "      <td>【央视短评】致敬让荒漠变绿洲的奋斗者</td>\n",
       "      <td>三代愚公志，黄沙变绿颜。38年来，“六老汉”三代人薪火相传、久久为功，在与恶劣环境的不懈斗争...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26327</th>\n",
       "      <td>20190328_06</td>\n",
       "      <td>20190328</td>\n",
       "      <td>国际联播快讯</td>\n",
       "      <td>英国议会下院正式确认推迟“脱欧”英国议会下院27日投票表决，正式确认推迟原定于本月29日的“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26328</th>\n",
       "      <td>20190328_07</td>\n",
       "      <td>20190328</td>\n",
       "      <td>“六老汉”三代人 守得沙漠变绿洲</td>\n",
       "      <td>甘肃古浪县地处腾格里沙漠南缘，上个世纪80年代初，饱受风沙之苦的当地六位老汉为保卫家园，毅然...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26369</th>\n",
       "      <td>20190331_00</td>\n",
       "      <td>20190331</td>\n",
       "      <td>江苏南京：千里绿道 串起美景惠民生</td>\n",
       "      <td>江苏南京，通过绿道串联起自然山水和人文景观，为市民打造低碳绿色生活，为发展注入新活力。南京江...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26370</th>\n",
       "      <td>20190331_01</td>\n",
       "      <td>20190331</td>\n",
       "      <td>国际联播快讯</td>\n",
       "      <td>巴基斯坦新瓜达尔国际机场奠基29日，中巴经济走廊框架下的重点项目巴基斯坦新瓜达尔国际机场项目...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26371</th>\n",
       "      <td>20190331_02</td>\n",
       "      <td>20190331</td>\n",
       "      <td>古特雷斯：利比亚冲突方有望达成一致</td>\n",
       "      <td>联合国秘书长古特雷斯30日表示，利比亚两个主要冲突方有望就军队控制权这一关键问题达成一致。据...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26372</th>\n",
       "      <td>20190331_03</td>\n",
       "      <td>20190331</td>\n",
       "      <td>国内联播快讯</td>\n",
       "      <td>中国民航今起执行夏秋航季航班计划中国民航今天零时起执行2019年夏秋航季航班计划。新航季持续...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26373</th>\n",
       "      <td>20190331_04</td>\n",
       "      <td>20190331</td>\n",
       "      <td>高楼映春色 美景入城中</td>\n",
       "      <td>春和景明，柳绿桃红。持续不断的生态文明建设，不仅让希望的田野百花齐放，也让高楼林立的都市充满...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26374</th>\n",
       "      <td>20190331_05</td>\n",
       "      <td>20190331</td>\n",
       "      <td>吉林：聚焦实体经济 加强创新引领</td>\n",
       "      <td>吉林省把发展实体经济、稳定工业增长放在突出位置，出台一系列政策举措，实施创新驱动战略，加快产...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26375</th>\n",
       "      <td>20190331_06</td>\n",
       "      <td>20190331</td>\n",
       "      <td>巴勒斯坦人大规模示威</td>\n",
       "      <td>30日是巴勒斯坦第43个“土地日”。巴勒斯坦人当天在加沙地带边境地区举行大规模示威，并与部署...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26376</th>\n",
       "      <td>20190331_07</td>\n",
       "      <td>20190331</td>\n",
       "      <td>湖南湘西：聚焦精准 决战脱贫攻坚</td>\n",
       "      <td>湖南湘西州属于武陵山集中连片特困地区，脱贫任务重。近几年，当地大力推广十八洞村精准脱贫经验，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26377</th>\n",
       "      <td>20190331_08</td>\n",
       "      <td>20190331</td>\n",
       "      <td>中办 国办印发《关于以2022年北京冬奥会为契机 大力发展冰雪运动的意见》</td>\n",
       "      <td>中共中央办公厅、国务院办公厅近日印发《关于以2022年北京冬奥会为契机大力发展冰雪运动的意见...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26378</th>\n",
       "      <td>20190331_09</td>\n",
       "      <td>20190331</td>\n",
       "      <td>【在习近平新时代中国特色社会主义思想指引下——新时代 新作为 新篇章】北京立足“四个中心”提...</td>\n",
       "      <td>党的十八大以来，习近平总书记四次到北京考察慰问，五次对北京发表重要讲话，要求北京立足“四个中...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26379</th>\n",
       "      <td>20190331_10</td>\n",
       "      <td>20190331</td>\n",
       "      <td>习近平向第30届阿拉伯国家联盟首脑理事会会议致贺电</td>\n",
       "      <td>国家主席习近平31日致电阿拉伯国家联盟首脑理事会会议轮值主席突尼斯总统埃塞卜西，祝贺第30届...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26380</th>\n",
       "      <td>20190331_11</td>\n",
       "      <td>20190331</td>\n",
       "      <td>《求是》杂志发表习近平总书记重要文章《关于坚持和发展中国特色社会主义的几个问题》</td>\n",
       "      <td>4月1日出版的第7期《求是》杂志将发表中共中央总书记、国家主席、中央军委主席习近平的重要文章...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26381</th>\n",
       "      <td>20190331_12</td>\n",
       "      <td>20190331</td>\n",
       "      <td>制造业采购经理指数重回扩张区间</td>\n",
       "      <td>中国物流与采购联合会、国家统计局今天发布：3月份中国制造业采购经理指数又重回50%以上的扩张...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26382</th>\n",
       "      <td>20190401_00</td>\n",
       "      <td>20190401</td>\n",
       "      <td>中国轮胎企业在欧洲建首家工厂</td>\n",
       "      <td>中国轮胎企业的首个欧洲工厂项目近日在塞尔维亚兹雷尼亚宁市启动。塞尔维亚总统武契奇出席了启动仪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26383</th>\n",
       "      <td>20190401_01</td>\n",
       "      <td>20190401</td>\n",
       "      <td>国际联播快讯</td>\n",
       "      <td>乌克兰总统选举将进行第二轮投票乌克兰中央选举委员会当地时间4月1日上午发布消息称，3月31日...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26384</th>\n",
       "      <td>20190401_02</td>\n",
       "      <td>20190401</td>\n",
       "      <td>第六批在韩中国人民志愿军烈士遗骸装殓仪式举行</td>\n",
       "      <td>4月1日，第六批在韩中国人民志愿军烈士遗骸及遗物，在韩国仁川举行装殓仪式。根据安排，4月3日...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26385</th>\n",
       "      <td>20190401_03</td>\n",
       "      <td>20190401</td>\n",
       "      <td>阿盟峰会聚焦戈兰高地问题</td>\n",
       "      <td>第30届阿拉伯国家联盟首脑会议3月31日在突尼斯举行。会议发表声明，坚决反对美国承认戈兰高地...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26386</th>\n",
       "      <td>20190401_04</td>\n",
       "      <td>20190401</td>\n",
       "      <td>国内联播快讯</td>\n",
       "      <td>全国已登记器官捐献志愿者116万余人清明节前夕，中国红十字会总会等部门在多地举办缅怀器官捐献...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26387</th>\n",
       "      <td>20190401_05</td>\n",
       "      <td>20190401</td>\n",
       "      <td>四川凉山木里县发生森林火灾</td>\n",
       "      <td>3月30日18时许，四川省凉山州木里县境内发生森林火灾，当地地形复杂、坡陡谷深，交通、通讯不...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26388</th>\n",
       "      <td>20190401_06</td>\n",
       "      <td>20190401</td>\n",
       "      <td>国新办：我国对芬太尼类物质实施整类列管</td>\n",
       "      <td>今天上午，在国务院新闻办公室举行的新闻发布会上，公安部、国家卫生健康委、国家药监局联合发布公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26389</th>\n",
       "      <td>20190401_07</td>\n",
       "      <td>20190401</td>\n",
       "      <td>我国成功发射“天链二号01星”</td>\n",
       "      <td>昨天23时51分，我国在西昌卫星发射中心用“长征三号乙”运载火箭，将“天链二号01星”送入太...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26390</th>\n",
       "      <td>20190401_08</td>\n",
       "      <td>20190401</td>\n",
       "      <td>深化增值税改革系列措施今起实施</td>\n",
       "      <td>今天是第28个全国税收宣传月的首日，今年的主题是“落实减税降费，促进经济高质量发展”。同样从...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26391</th>\n",
       "      <td>20190401_09</td>\n",
       "      <td>20190401</td>\n",
       "      <td>王岐山会见蒙古国外长</td>\n",
       "      <td>国家副主席王岐山今天在中南海会见蒙古国外长朝格特巴特尔。王岐山表示，蒙古国是最早同新中国建交...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26392</th>\n",
       "      <td>20190401_10</td>\n",
       "      <td>20190401</td>\n",
       "      <td>汪洋会见澳区省级政协委员联谊会访京团</td>\n",
       "      <td>中共中央政治局常委、全国政协主席汪洋1日在京会见了由会长马志毅率领的澳区省级政协委员联谊会访...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26393</th>\n",
       "      <td>20190401_11</td>\n",
       "      <td>20190401</td>\n",
       "      <td>汪洋在脱贫攻坚民主监督工作座谈会上强调 聚焦脱贫攻坚突出问题 不断提高民主监督工作实效</td>\n",
       "      <td>各民主党派中央脱贫攻坚民主监督工作座谈会4月1日在京召开。中共中央政治局常委、全国政协主席汪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26394</th>\n",
       "      <td>20190401_12</td>\n",
       "      <td>20190401</td>\n",
       "      <td>李克强同新西兰总理举行会谈</td>\n",
       "      <td>欢迎仪式后，李克强同阿德恩举行会谈。李克强再次就新西兰前不久发生严重枪击事件造成重大人员伤亡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26395</th>\n",
       "      <td>20190401_13</td>\n",
       "      <td>20190401</td>\n",
       "      <td>李克强举行仪式欢迎新西兰总理访华</td>\n",
       "      <td>1日上午，国务院总理李克强在北京人民大会堂举行仪式，欢迎新西兰总理阿德恩对我国进行正式访问。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26396</th>\n",
       "      <td>20190401_14</td>\n",
       "      <td>20190401</td>\n",
       "      <td>习近平向2019“中国—太平洋岛国旅游年”开幕式致贺词</td>\n",
       "      <td>2019“中国—太平洋岛国旅游年”4月1日在萨摩亚首都阿皮亚开幕，国家主席习近平向开幕式致贺...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26397</th>\n",
       "      <td>20190401_15</td>\n",
       "      <td>20190401</td>\n",
       "      <td>习近平会见“元老会”代表团</td>\n",
       "      <td>国家主席习近平1日在人民大会堂会见“元老会”代表团。习近平指出，世界正处于百年未有之大变局。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26398</th>\n",
       "      <td>20190401_16</td>\n",
       "      <td>20190401</td>\n",
       "      <td>习近平会见新西兰总理</td>\n",
       "      <td>国家主席习近平1日在人民大会堂会见新西兰总理阿德恩。习近平就新西兰不久前发生严重枪击事件再次...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id      date  \\\n",
       "26299  20190326_12  20190326   \n",
       "26300  20190326_13  20190326   \n",
       "26301  20190326_14  20190326   \n",
       "26302  20190327_00  20190327   \n",
       "26303  20190327_01  20190327   \n",
       "26304  20190327_02  20190327   \n",
       "26305  20190327_03  20190327   \n",
       "26306  20190327_04  20190327   \n",
       "26307  20190327_05  20190327   \n",
       "26308  20190327_06  20190327   \n",
       "26309  20190327_07  20190327   \n",
       "26310  20190327_08  20190327   \n",
       "26311  20190327_09  20190327   \n",
       "26312  20190327_10  20190327   \n",
       "26313  20190327_11  20190327   \n",
       "26314  20190327_12  20190327   \n",
       "26315  20190327_13  20190327   \n",
       "26316  20190327_14  20190327   \n",
       "26317  20190327_15  20190327   \n",
       "26318  20190327_16  20190327   \n",
       "26319  20190327_17  20190327   \n",
       "26320  20190327_18  20190327   \n",
       "26321  20190328_00  20190328   \n",
       "26322  20190328_01  20190328   \n",
       "26323  20190328_02  20190328   \n",
       "26324  20190328_03  20190328   \n",
       "26325  20190328_04  20190328   \n",
       "26326  20190328_05  20190328   \n",
       "26327  20190328_06  20190328   \n",
       "26328  20190328_07  20190328   \n",
       "...            ...       ...   \n",
       "26369  20190331_00  20190331   \n",
       "26370  20190331_01  20190331   \n",
       "26371  20190331_02  20190331   \n",
       "26372  20190331_03  20190331   \n",
       "26373  20190331_04  20190331   \n",
       "26374  20190331_05  20190331   \n",
       "26375  20190331_06  20190331   \n",
       "26376  20190331_07  20190331   \n",
       "26377  20190331_08  20190331   \n",
       "26378  20190331_09  20190331   \n",
       "26379  20190331_10  20190331   \n",
       "26380  20190331_11  20190331   \n",
       "26381  20190331_12  20190331   \n",
       "26382  20190401_00  20190401   \n",
       "26383  20190401_01  20190401   \n",
       "26384  20190401_02  20190401   \n",
       "26385  20190401_03  20190401   \n",
       "26386  20190401_04  20190401   \n",
       "26387  20190401_05  20190401   \n",
       "26388  20190401_06  20190401   \n",
       "26389  20190401_07  20190401   \n",
       "26390  20190401_08  20190401   \n",
       "26391  20190401_09  20190401   \n",
       "26392  20190401_10  20190401   \n",
       "26393  20190401_11  20190401   \n",
       "26394  20190401_12  20190401   \n",
       "26395  20190401_13  20190401   \n",
       "26396  20190401_14  20190401   \n",
       "26397  20190401_15  20190401   \n",
       "26398  20190401_16  20190401   \n",
       "\n",
       "                                                   title  \\\n",
       "26299                                     李克强主持召开国务院常务会议   \n",
       "26300                                         习近平同法国总统会谈   \n",
       "26301                                   习近平出席法国总统举行的欢迎仪式   \n",
       "26302                                               联播快讯   \n",
       "26303                          国新办发表《伟大的跨越：西藏民主改革60年》白皮书   \n",
       "26304        新华社综述：谱写新时代中国梦的雪域篇章——以习近平同志为核心的党中央治边稳藏富民新实践   \n",
       "26305                            中共中央办公厅印发《公务员职务与职级并行规定》   \n",
       "26306                               韩正在国家医疗保障局调研并主持召开座谈会   \n",
       "26307                                    李克强会见圣多美和普林西比总理   \n",
       "26308                                   李克强会见博鳌亚洲论坛理事会成员   \n",
       "26309                 李克强在海南考察时强调 更大激发市场主体活力 着力破解发展和民生难题   \n",
       "26310   新华社长篇通讯：共绘美美与共的人类文明画卷——写在习近平主席在联合国教科文组织总部演讲五周年之际   \n",
       "26311                        习近平同出席中法全球治理论坛闭幕式的欧洲领导人举行会晤   \n",
       "26312                     又踏层峰望眼开——习近平主席访问意大利 摩纳哥 法国成果丰硕   \n",
       "26313                                   文明交流互鉴推动合作共赢和平发展   \n",
       "26314                                          习近平会见德国总理   \n",
       "26315                                          习近平会见法国总理   \n",
       "26316                            习近平和法国总统共同出席中法全球治理论坛闭幕式   \n",
       "26317                                         习近平会见法国参议长   \n",
       "26318                             习近平和彭丽媛出席法国总统举行的隆重欢送仪式   \n",
       "26319                  习近平结束对意大利共和国 摩纳哥公国 法兰西共和国国事访问回到北京   \n",
       "26320                                      习近平会见法国国民议会议长   \n",
       "26321                 人民日报社论：铭记伟大变革 激扬奋进力量——纪念西藏民主改革六十周年   \n",
       "26322                                   美承认以色列对戈兰高地主权遭反对   \n",
       "26323                                各界积极评价习近平主席访问欧洲三国成果   \n",
       "26324                                             国内联播快讯   \n",
       "26325                                  第七届全国道德模范评选表彰活动启动   \n",
       "26326                                 【央视短评】致敬让荒漠变绿洲的奋斗者   \n",
       "26327                                             国际联播快讯   \n",
       "26328                                   “六老汉”三代人 守得沙漠变绿洲   \n",
       "...                                                  ...   \n",
       "26369                                  江苏南京：千里绿道 串起美景惠民生   \n",
       "26370                                             国际联播快讯   \n",
       "26371                                  古特雷斯：利比亚冲突方有望达成一致   \n",
       "26372                                             国内联播快讯   \n",
       "26373                                        高楼映春色 美景入城中   \n",
       "26374                                   吉林：聚焦实体经济 加强创新引领   \n",
       "26375                                         巴勒斯坦人大规模示威   \n",
       "26376                                   湖南湘西：聚焦精准 决战脱贫攻坚   \n",
       "26377              中办 国办印发《关于以2022年北京冬奥会为契机 大力发展冰雪运动的意见》   \n",
       "26378  【在习近平新时代中国特色社会主义思想指引下——新时代 新作为 新篇章】北京立足“四个中心”提...   \n",
       "26379                          习近平向第30届阿拉伯国家联盟首脑理事会会议致贺电   \n",
       "26380           《求是》杂志发表习近平总书记重要文章《关于坚持和发展中国特色社会主义的几个问题》   \n",
       "26381                                    制造业采购经理指数重回扩张区间   \n",
       "26382                                     中国轮胎企业在欧洲建首家工厂   \n",
       "26383                                             国际联播快讯   \n",
       "26384                             第六批在韩中国人民志愿军烈士遗骸装殓仪式举行   \n",
       "26385                                       阿盟峰会聚焦戈兰高地问题   \n",
       "26386                                             国内联播快讯   \n",
       "26387                                      四川凉山木里县发生森林火灾   \n",
       "26388                                国新办：我国对芬太尼类物质实施整类列管   \n",
       "26389                                    我国成功发射“天链二号01星”   \n",
       "26390                                    深化增值税改革系列措施今起实施   \n",
       "26391                                         王岐山会见蒙古国外长   \n",
       "26392                                 汪洋会见澳区省级政协委员联谊会访京团   \n",
       "26393        汪洋在脱贫攻坚民主监督工作座谈会上强调 聚焦脱贫攻坚突出问题 不断提高民主监督工作实效   \n",
       "26394                                      李克强同新西兰总理举行会谈   \n",
       "26395                                   李克强举行仪式欢迎新西兰总理访华   \n",
       "26396                        习近平向2019“中国—太平洋岛国旅游年”开幕式致贺词   \n",
       "26397                                      习近平会见“元老会”代表团   \n",
       "26398                                         习近平会见新西兰总理   \n",
       "\n",
       "                                                 content  \n",
       "26299  国务院总理李克强3月26日主持召开国务院常务会议，落实降低社会保险费率部署，明确具体配套措施...  \n",
       "26300  欢迎仪式后，在威武整齐的摩托车队护卫下，习近平乘车前往爱丽舍宫，沿途100多名法兰西共和国卫...  \n",
       "26301  在中华人民共和国和法兰西共和国建交55周年之际，中国国家主席习近平时隔五年之后再次对法国进行...  \n",
       "26302  全国开展危险化学品安全隐患排查国务院安委办、应急管理部今天（27日）召开视频会议，要求在全国...  \n",
       "26303  国务院新闻办公室今天（27日）发表《伟大的跨越：西藏民主改革60年》白皮书。白皮书全文约2....  \n",
       "26304  新华社今天（27日）播发综述《谱写新时代中国梦的雪域篇章——以习近平同志为核心的党中央治边稳...  \n",
       "26305  近日，中共中央办公厅印发了《公务员职务与职级并行规定》，并发出通知，要求各地区各部门认真遵照...  \n",
       "26306  中共中央政治局常委、国务院副总理韩正26日到国家医疗保障局调研。韩正观看了打击欺诈骗取医保基...  \n",
       "26307  国务院总理李克强今天（27日）在海南会见来华出席博鳌亚洲论坛2019年年会的圣多美和普林西比...  \n",
       "26308  国务院总理李克强今天（27日）在海南博鳌会见博鳌亚洲论坛理事长潘基文和理事会部分成员。李克强...  \n",
       "26309  27日，在出席博鳌亚洲论坛期间，中共中央政治局常委、国务院总理李克强在海南海口考察。他强调，...  \n",
       "26310  新华社今天（27日）播发长篇通讯《共绘美美与共的人类文明画卷——写在习近平主席在联合国教科文...  \n",
       "26311  国家主席习近平当地时间26日在巴黎同出席中法全球治理论坛闭幕式的法国总统马克龙、德国总理默克...  \n",
       "26312  2019年3月21日至26日，国家主席习近平应邀对意大利、摩纳哥、法国进行国事访问，引领中国...  \n",
       "26313  五年前的今天，习近平主席在联合国教科文组织进行历史性访问并发表重要演讲，全面深刻地阐述了文明...  \n",
       "26314  国家主席习近平当地时间26日在巴黎会见专程前来出席中法全球治理论坛闭幕式的德国总理默克尔。习...  \n",
       "26315  国家主席习近平当地时间26日在巴黎总理府会见法国总理菲利普。习近平抵达总理府时，菲利普在停车...  \n",
       "26316  国家主席习近平当地时间26日在巴黎同法国总统马克龙一道出席中法全球治理论坛闭幕式。德国总理默...  \n",
       "26317  国家主席习近平当地时间26日在巴黎会见法国参议长拉尔歇。习近平抵达参议院时，拉尔歇议长在停车...  \n",
       "26318  国家主席习近平当地时间26日结束对法国的国事访问。离开巴黎之前，习近平和夫人彭丽媛出席法国总...  \n",
       "26319  3月27日，在结束对意大利共和国、摩纳哥公国、法兰西共和国国事访问后，国家主席习近平回到北京...  \n",
       "26320  国家主席习近平当地时间26日在巴黎会见法国国民议会议长费朗。习近平抵达国民议会时，费朗议长在...  \n",
       "26321         今天出版的人民日报发表社论《铭记伟大变革激扬奋进力量——纪念西藏民主改革六十周年》。  \n",
       "26322  27日，联合国安理会紧急开会讨论戈兰高地问题。叙利亚常驻联合国代表贾法里谴责美国单方面承认以...  \n",
       "26323  日前，中国国家主席习近平应邀对意大利、摩纳哥、法国进行国事访问。三国各界人士称赞习主席此次访...  \n",
       "26324  最高法发布优化营商环境司法解释最高人民法院今天对外发布优化营商环境司法解释，要求依法保护产权...  \n",
       "26325  中央宣传部、中央文明办、全国总工会、共青团中央、全国妇联、中央军委政治工作部今天在京召开电视...  \n",
       "26326  三代愚公志，黄沙变绿颜。38年来，“六老汉”三代人薪火相传、久久为功，在与恶劣环境的不懈斗争...  \n",
       "26327  英国议会下院正式确认推迟“脱欧”英国议会下院27日投票表决，正式确认推迟原定于本月29日的“...  \n",
       "26328  甘肃古浪县地处腾格里沙漠南缘，上个世纪80年代初，饱受风沙之苦的当地六位老汉为保卫家园，毅然...  \n",
       "...                                                  ...  \n",
       "26369  江苏南京，通过绿道串联起自然山水和人文景观，为市民打造低碳绿色生活，为发展注入新活力。南京江...  \n",
       "26370  巴基斯坦新瓜达尔国际机场奠基29日，中巴经济走廊框架下的重点项目巴基斯坦新瓜达尔国际机场项目...  \n",
       "26371  联合国秘书长古特雷斯30日表示，利比亚两个主要冲突方有望就军队控制权这一关键问题达成一致。据...  \n",
       "26372  中国民航今起执行夏秋航季航班计划中国民航今天零时起执行2019年夏秋航季航班计划。新航季持续...  \n",
       "26373  春和景明，柳绿桃红。持续不断的生态文明建设，不仅让希望的田野百花齐放，也让高楼林立的都市充满...  \n",
       "26374  吉林省把发展实体经济、稳定工业增长放在突出位置，出台一系列政策举措，实施创新驱动战略，加快产...  \n",
       "26375  30日是巴勒斯坦第43个“土地日”。巴勒斯坦人当天在加沙地带边境地区举行大规模示威，并与部署...  \n",
       "26376  湖南湘西州属于武陵山集中连片特困地区，脱贫任务重。近几年，当地大力推广十八洞村精准脱贫经验，...  \n",
       "26377  中共中央办公厅、国务院办公厅近日印发《关于以2022年北京冬奥会为契机大力发展冰雪运动的意见...  \n",
       "26378  党的十八大以来，习近平总书记四次到北京考察慰问，五次对北京发表重要讲话，要求北京立足“四个中...  \n",
       "26379  国家主席习近平31日致电阿拉伯国家联盟首脑理事会会议轮值主席突尼斯总统埃塞卜西，祝贺第30届...  \n",
       "26380  4月1日出版的第7期《求是》杂志将发表中共中央总书记、国家主席、中央军委主席习近平的重要文章...  \n",
       "26381  中国物流与采购联合会、国家统计局今天发布：3月份中国制造业采购经理指数又重回50%以上的扩张...  \n",
       "26382  中国轮胎企业的首个欧洲工厂项目近日在塞尔维亚兹雷尼亚宁市启动。塞尔维亚总统武契奇出席了启动仪...  \n",
       "26383  乌克兰总统选举将进行第二轮投票乌克兰中央选举委员会当地时间4月1日上午发布消息称，3月31日...  \n",
       "26384  4月1日，第六批在韩中国人民志愿军烈士遗骸及遗物，在韩国仁川举行装殓仪式。根据安排，4月3日...  \n",
       "26385  第30届阿拉伯国家联盟首脑会议3月31日在突尼斯举行。会议发表声明，坚决反对美国承认戈兰高地...  \n",
       "26386  全国已登记器官捐献志愿者116万余人清明节前夕，中国红十字会总会等部门在多地举办缅怀器官捐献...  \n",
       "26387  3月30日18时许，四川省凉山州木里县境内发生森林火灾，当地地形复杂、坡陡谷深，交通、通讯不...  \n",
       "26388  今天上午，在国务院新闻办公室举行的新闻发布会上，公安部、国家卫生健康委、国家药监局联合发布公...  \n",
       "26389  昨天23时51分，我国在西昌卫星发射中心用“长征三号乙”运载火箭，将“天链二号01星”送入太...  \n",
       "26390  今天是第28个全国税收宣传月的首日，今年的主题是“落实减税降费，促进经济高质量发展”。同样从...  \n",
       "26391  国家副主席王岐山今天在中南海会见蒙古国外长朝格特巴特尔。王岐山表示，蒙古国是最早同新中国建交...  \n",
       "26392  中共中央政治局常委、全国政协主席汪洋1日在京会见了由会长马志毅率领的澳区省级政协委员联谊会访...  \n",
       "26393  各民主党派中央脱贫攻坚民主监督工作座谈会4月1日在京召开。中共中央政治局常委、全国政协主席汪...  \n",
       "26394  欢迎仪式后，李克强同阿德恩举行会谈。李克强再次就新西兰前不久发生严重枪击事件造成重大人员伤亡...  \n",
       "26395  1日上午，国务院总理李克强在北京人民大会堂举行仪式，欢迎新西兰总理阿德恩对我国进行正式访问。...  \n",
       "26396  2019“中国—太平洋岛国旅游年”4月1日在萨摩亚首都阿皮亚开幕，国家主席习近平向开幕式致贺...  \n",
       "26397  国家主席习近平1日在人民大会堂会见“元老会”代表团。习近平指出，世界正处于百年未有之大变局。...  \n",
       "26398  国家主席习近平1日在人民大会堂会见新西兰总理阿德恩。习近平就新西兰不久前发生严重枪击事件再次...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data1.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>name</th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>close</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "      <th>pe</th>\n",
       "      <th>pb</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140401</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1668.75</td>\n",
       "      <td>1668.54</td>\n",
       "      <td>1689.12</td>\n",
       "      <td>1689.07</td>\n",
       "      <td>22.13</td>\n",
       "      <td>1.33</td>\n",
       "      <td>34914.0</td>\n",
       "      <td>291113.0</td>\n",
       "      <td>41.51</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140402</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1688.72</td>\n",
       "      <td>1684.53</td>\n",
       "      <td>1693.41</td>\n",
       "      <td>1692.24</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>36300.0</td>\n",
       "      <td>289020.0</td>\n",
       "      <td>41.63</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140403</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1693.05</td>\n",
       "      <td>1679.85</td>\n",
       "      <td>1697.73</td>\n",
       "      <td>1685.71</td>\n",
       "      <td>-6.53</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>31403.0</td>\n",
       "      <td>259464.0</td>\n",
       "      <td>41.38</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140404</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1681.92</td>\n",
       "      <td>1680.34</td>\n",
       "      <td>1698.44</td>\n",
       "      <td>1698.25</td>\n",
       "      <td>12.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>28648.0</td>\n",
       "      <td>240940.0</td>\n",
       "      <td>41.76</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140408</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1693.24</td>\n",
       "      <td>1692.22</td>\n",
       "      <td>1706.84</td>\n",
       "      <td>1706.84</td>\n",
       "      <td>8.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>35012.0</td>\n",
       "      <td>312423.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140409</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1708.13</td>\n",
       "      <td>1707.49</td>\n",
       "      <td>1726.01</td>\n",
       "      <td>1725.55</td>\n",
       "      <td>18.71</td>\n",
       "      <td>1.10</td>\n",
       "      <td>43114.0</td>\n",
       "      <td>378611.0</td>\n",
       "      <td>41.68</td>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140410</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1723.57</td>\n",
       "      <td>1721.18</td>\n",
       "      <td>1730.89</td>\n",
       "      <td>1725.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>42089.0</td>\n",
       "      <td>386253.0</td>\n",
       "      <td>41.68</td>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140411</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1721.84</td>\n",
       "      <td>1713.55</td>\n",
       "      <td>1724.08</td>\n",
       "      <td>1719.15</td>\n",
       "      <td>-6.52</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>33973.0</td>\n",
       "      <td>302877.0</td>\n",
       "      <td>41.38</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140414</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1718.35</td>\n",
       "      <td>1717.57</td>\n",
       "      <td>1751.81</td>\n",
       "      <td>1749.88</td>\n",
       "      <td>30.73</td>\n",
       "      <td>1.79</td>\n",
       "      <td>45683.0</td>\n",
       "      <td>440762.0</td>\n",
       "      <td>42.11</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140415</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1747.73</td>\n",
       "      <td>1743.36</td>\n",
       "      <td>1758.92</td>\n",
       "      <td>1748.24</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>52524.0</td>\n",
       "      <td>482483.0</td>\n",
       "      <td>41.83</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140416</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1743.14</td>\n",
       "      <td>1743.07</td>\n",
       "      <td>1761.87</td>\n",
       "      <td>1759.88</td>\n",
       "      <td>11.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>47387.0</td>\n",
       "      <td>415858.0</td>\n",
       "      <td>41.33</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140417</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1765.11</td>\n",
       "      <td>1751.34</td>\n",
       "      <td>1768.27</td>\n",
       "      <td>1751.95</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>39258.0</td>\n",
       "      <td>364281.0</td>\n",
       "      <td>41.19</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140418</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1747.78</td>\n",
       "      <td>1740.37</td>\n",
       "      <td>1749.27</td>\n",
       "      <td>1748.07</td>\n",
       "      <td>-3.88</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>27658.0</td>\n",
       "      <td>250801.0</td>\n",
       "      <td>41.05</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140421</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1738.50</td>\n",
       "      <td>1721.93</td>\n",
       "      <td>1753.82</td>\n",
       "      <td>1723.44</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>34708.0</td>\n",
       "      <td>317704.0</td>\n",
       "      <td>40.55</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140422</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1719.55</td>\n",
       "      <td>1689.66</td>\n",
       "      <td>1726.53</td>\n",
       "      <td>1712.55</td>\n",
       "      <td>-10.89</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>37810.0</td>\n",
       "      <td>350380.0</td>\n",
       "      <td>40.55</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140423</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1713.95</td>\n",
       "      <td>1704.45</td>\n",
       "      <td>1723.92</td>\n",
       "      <td>1708.59</td>\n",
       "      <td>-3.96</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>29933.0</td>\n",
       "      <td>281126.0</td>\n",
       "      <td>40.72</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140424</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1705.93</td>\n",
       "      <td>1695.73</td>\n",
       "      <td>1711.51</td>\n",
       "      <td>1696.33</td>\n",
       "      <td>-12.26</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>28072.0</td>\n",
       "      <td>263656.0</td>\n",
       "      <td>40.65</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140425</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1697.29</td>\n",
       "      <td>1664.62</td>\n",
       "      <td>1698.52</td>\n",
       "      <td>1665.01</td>\n",
       "      <td>-31.32</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>36588.0</td>\n",
       "      <td>345842.0</td>\n",
       "      <td>39.63</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140428</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1659.78</td>\n",
       "      <td>1606.24</td>\n",
       "      <td>1660.02</td>\n",
       "      <td>1608.36</td>\n",
       "      <td>-56.65</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>34912.0</td>\n",
       "      <td>308748.0</td>\n",
       "      <td>38.54</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140429</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1606.90</td>\n",
       "      <td>1606.45</td>\n",
       "      <td>1630.99</td>\n",
       "      <td>1630.56</td>\n",
       "      <td>22.20</td>\n",
       "      <td>1.38</td>\n",
       "      <td>24681.0</td>\n",
       "      <td>217555.0</td>\n",
       "      <td>40.81</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140430</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1627.63</td>\n",
       "      <td>1620.66</td>\n",
       "      <td>1635.84</td>\n",
       "      <td>1634.53</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.24</td>\n",
       "      <td>22190.0</td>\n",
       "      <td>200379.0</td>\n",
       "      <td>41.30</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140505</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1631.20</td>\n",
       "      <td>1627.64</td>\n",
       "      <td>1669.82</td>\n",
       "      <td>1669.82</td>\n",
       "      <td>35.29</td>\n",
       "      <td>2.16</td>\n",
       "      <td>30320.0</td>\n",
       "      <td>309628.0</td>\n",
       "      <td>43.57</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140506</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1666.06</td>\n",
       "      <td>1665.70</td>\n",
       "      <td>1692.11</td>\n",
       "      <td>1687.83</td>\n",
       "      <td>18.01</td>\n",
       "      <td>1.08</td>\n",
       "      <td>33004.0</td>\n",
       "      <td>352131.0</td>\n",
       "      <td>44.04</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140507</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1683.20</td>\n",
       "      <td>1663.34</td>\n",
       "      <td>1685.64</td>\n",
       "      <td>1664.49</td>\n",
       "      <td>-23.34</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>29082.0</td>\n",
       "      <td>296894.0</td>\n",
       "      <td>43.43</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140508</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1662.49</td>\n",
       "      <td>1658.42</td>\n",
       "      <td>1683.17</td>\n",
       "      <td>1662.89</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>23884.0</td>\n",
       "      <td>235397.0</td>\n",
       "      <td>43.39</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140509</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1660.53</td>\n",
       "      <td>1633.00</td>\n",
       "      <td>1662.81</td>\n",
       "      <td>1647.72</td>\n",
       "      <td>-15.17</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>22227.0</td>\n",
       "      <td>217123.0</td>\n",
       "      <td>42.99</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140512</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1657.54</td>\n",
       "      <td>1651.31</td>\n",
       "      <td>1682.60</td>\n",
       "      <td>1682.57</td>\n",
       "      <td>34.85</td>\n",
       "      <td>2.12</td>\n",
       "      <td>31357.0</td>\n",
       "      <td>280848.0</td>\n",
       "      <td>44.03</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140513</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1683.43</td>\n",
       "      <td>1675.81</td>\n",
       "      <td>1690.56</td>\n",
       "      <td>1679.71</td>\n",
       "      <td>-2.86</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>27356.0</td>\n",
       "      <td>261752.0</td>\n",
       "      <td>43.96</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140514</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1680.04</td>\n",
       "      <td>1678.35</td>\n",
       "      <td>1689.54</td>\n",
       "      <td>1687.06</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.44</td>\n",
       "      <td>28557.0</td>\n",
       "      <td>274429.0</td>\n",
       "      <td>44.15</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>801010</td>\n",
       "      <td>20140515</td>\n",
       "      <td>农林牧渔</td>\n",
       "      <td>1690.36</td>\n",
       "      <td>1667.12</td>\n",
       "      <td>1702.37</td>\n",
       "      <td>1668.01</td>\n",
       "      <td>-19.05</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>34382.0</td>\n",
       "      <td>345523.0</td>\n",
       "      <td>43.66</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39138</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190219</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3579.18</td>\n",
       "      <td>3523.59</td>\n",
       "      <td>3592.14</td>\n",
       "      <td>3571.10</td>\n",
       "      <td>7.13</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3830034.0</td>\n",
       "      <td>33069322.0</td>\n",
       "      <td>22.52</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39139</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190220</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3565.80</td>\n",
       "      <td>3530.05</td>\n",
       "      <td>3576.58</td>\n",
       "      <td>3573.88</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3228623.0</td>\n",
       "      <td>27097884.0</td>\n",
       "      <td>22.60</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39140</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190221</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3570.03</td>\n",
       "      <td>3558.00</td>\n",
       "      <td>3647.76</td>\n",
       "      <td>3571.58</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>3883750.0</td>\n",
       "      <td>34698806.0</td>\n",
       "      <td>22.58</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39141</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190222</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3564.22</td>\n",
       "      <td>3562.49</td>\n",
       "      <td>3667.00</td>\n",
       "      <td>3667.00</td>\n",
       "      <td>95.42</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3641490.0</td>\n",
       "      <td>33151818.0</td>\n",
       "      <td>23.18</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39142</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190225</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3732.26</td>\n",
       "      <td>3731.09</td>\n",
       "      <td>3866.40</td>\n",
       "      <td>3865.52</td>\n",
       "      <td>198.52</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5568265.0</td>\n",
       "      <td>51762223.0</td>\n",
       "      <td>24.43</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39143</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190226</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3878.98</td>\n",
       "      <td>3823.22</td>\n",
       "      <td>3935.83</td>\n",
       "      <td>3847.28</td>\n",
       "      <td>-18.24</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>5892699.0</td>\n",
       "      <td>54473918.0</td>\n",
       "      <td>24.33</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39144</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190227</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3834.98</td>\n",
       "      <td>3774.68</td>\n",
       "      <td>3880.18</td>\n",
       "      <td>3817.56</td>\n",
       "      <td>-29.72</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>4787456.0</td>\n",
       "      <td>43811779.0</td>\n",
       "      <td>24.12</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39145</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190228</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3819.39</td>\n",
       "      <td>3802.56</td>\n",
       "      <td>3853.09</td>\n",
       "      <td>3826.23</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3731296.0</td>\n",
       "      <td>32883739.0</td>\n",
       "      <td>24.17</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39146</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190301</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3846.13</td>\n",
       "      <td>3795.75</td>\n",
       "      <td>3857.13</td>\n",
       "      <td>3857.13</td>\n",
       "      <td>30.90</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3507033.0</td>\n",
       "      <td>31617194.0</td>\n",
       "      <td>24.37</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39147</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190304</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3900.97</td>\n",
       "      <td>3895.05</td>\n",
       "      <td>4001.73</td>\n",
       "      <td>3930.32</td>\n",
       "      <td>73.19</td>\n",
       "      <td>1.90</td>\n",
       "      <td>5295754.0</td>\n",
       "      <td>50696989.0</td>\n",
       "      <td>24.81</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39148</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190305</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>3921.66</td>\n",
       "      <td>3907.79</td>\n",
       "      <td>4044.30</td>\n",
       "      <td>4044.30</td>\n",
       "      <td>113.98</td>\n",
       "      <td>2.90</td>\n",
       "      <td>5071454.0</td>\n",
       "      <td>48564822.0</td>\n",
       "      <td>25.51</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39149</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190306</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4073.73</td>\n",
       "      <td>4018.09</td>\n",
       "      <td>4110.34</td>\n",
       "      <td>4110.04</td>\n",
       "      <td>65.73</td>\n",
       "      <td>1.63</td>\n",
       "      <td>6102440.0</td>\n",
       "      <td>58035380.0</td>\n",
       "      <td>25.90</td>\n",
       "      <td>2.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39150</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190307</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4103.92</td>\n",
       "      <td>4073.72</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>4146.26</td>\n",
       "      <td>36.22</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5969744.0</td>\n",
       "      <td>58999917.0</td>\n",
       "      <td>26.09</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39151</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190308</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4049.63</td>\n",
       "      <td>4000.45</td>\n",
       "      <td>4180.26</td>\n",
       "      <td>4000.45</td>\n",
       "      <td>-145.81</td>\n",
       "      <td>-3.52</td>\n",
       "      <td>6397305.0</td>\n",
       "      <td>63981639.0</td>\n",
       "      <td>25.19</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39152</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190311</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4038.47</td>\n",
       "      <td>4022.38</td>\n",
       "      <td>4167.05</td>\n",
       "      <td>4167.05</td>\n",
       "      <td>166.60</td>\n",
       "      <td>4.16</td>\n",
       "      <td>5248882.0</td>\n",
       "      <td>53541545.0</td>\n",
       "      <td>26.24</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39153</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190312</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4206.96</td>\n",
       "      <td>4185.74</td>\n",
       "      <td>4285.56</td>\n",
       "      <td>4259.90</td>\n",
       "      <td>92.85</td>\n",
       "      <td>2.23</td>\n",
       "      <td>6373095.0</td>\n",
       "      <td>65592259.0</td>\n",
       "      <td>26.84</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39154</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190313</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4257.71</td>\n",
       "      <td>4104.35</td>\n",
       "      <td>4257.71</td>\n",
       "      <td>4131.95</td>\n",
       "      <td>-127.95</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>5788780.0</td>\n",
       "      <td>60004033.0</td>\n",
       "      <td>26.03</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39155</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190314</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4095.09</td>\n",
       "      <td>3978.84</td>\n",
       "      <td>4122.73</td>\n",
       "      <td>4024.24</td>\n",
       "      <td>-107.71</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>4363434.0</td>\n",
       "      <td>43033723.0</td>\n",
       "      <td>25.37</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39156</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190315</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4047.72</td>\n",
       "      <td>4021.14</td>\n",
       "      <td>4102.95</td>\n",
       "      <td>4062.88</td>\n",
       "      <td>38.64</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3734033.0</td>\n",
       "      <td>37227564.0</td>\n",
       "      <td>25.63</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39157</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190318</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4077.09</td>\n",
       "      <td>4035.28</td>\n",
       "      <td>4152.46</td>\n",
       "      <td>4152.46</td>\n",
       "      <td>89.58</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3854941.0</td>\n",
       "      <td>38517998.0</td>\n",
       "      <td>26.20</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39158</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190319</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4154.04</td>\n",
       "      <td>4146.98</td>\n",
       "      <td>4206.74</td>\n",
       "      <td>4175.87</td>\n",
       "      <td>23.41</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3873893.0</td>\n",
       "      <td>39344316.0</td>\n",
       "      <td>26.35</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39159</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190320</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4174.58</td>\n",
       "      <td>4091.01</td>\n",
       "      <td>4181.05</td>\n",
       "      <td>4161.25</td>\n",
       "      <td>-14.62</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>3899754.0</td>\n",
       "      <td>38579665.0</td>\n",
       "      <td>26.23</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39160</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190321</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4166.62</td>\n",
       "      <td>4158.46</td>\n",
       "      <td>4241.77</td>\n",
       "      <td>4209.20</td>\n",
       "      <td>47.95</td>\n",
       "      <td>1.15</td>\n",
       "      <td>4519727.0</td>\n",
       "      <td>44639328.0</td>\n",
       "      <td>26.53</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39161</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190322</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4212.52</td>\n",
       "      <td>4139.92</td>\n",
       "      <td>4223.43</td>\n",
       "      <td>4217.71</td>\n",
       "      <td>8.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4111832.0</td>\n",
       "      <td>41501342.0</td>\n",
       "      <td>26.60</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39162</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190325</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4146.60</td>\n",
       "      <td>4134.48</td>\n",
       "      <td>4230.93</td>\n",
       "      <td>4178.99</td>\n",
       "      <td>-38.72</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>4151879.0</td>\n",
       "      <td>42732362.0</td>\n",
       "      <td>26.34</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39163</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190326</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4195.85</td>\n",
       "      <td>4054.34</td>\n",
       "      <td>4208.28</td>\n",
       "      <td>4062.73</td>\n",
       "      <td>-116.26</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>3998022.0</td>\n",
       "      <td>40950501.0</td>\n",
       "      <td>25.61</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39164</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190327</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4090.42</td>\n",
       "      <td>4002.09</td>\n",
       "      <td>4100.63</td>\n",
       "      <td>4072.08</td>\n",
       "      <td>9.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3231654.0</td>\n",
       "      <td>33449356.0</td>\n",
       "      <td>25.71</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39165</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190328</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4058.84</td>\n",
       "      <td>4028.51</td>\n",
       "      <td>4116.58</td>\n",
       "      <td>4029.95</td>\n",
       "      <td>-42.13</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>3175370.0</td>\n",
       "      <td>34319281.0</td>\n",
       "      <td>25.46</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39166</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190329</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4030.42</td>\n",
       "      <td>3995.79</td>\n",
       "      <td>4145.43</td>\n",
       "      <td>4145.43</td>\n",
       "      <td>115.48</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3671961.0</td>\n",
       "      <td>39397777.0</td>\n",
       "      <td>26.20</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39167</th>\n",
       "      <td>802600</td>\n",
       "      <td>20190401</td>\n",
       "      <td>交银装备</td>\n",
       "      <td>4182.31</td>\n",
       "      <td>4182.25</td>\n",
       "      <td>4306.69</td>\n",
       "      <td>4306.27</td>\n",
       "      <td>160.84</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4620338.0</td>\n",
       "      <td>51023180.0</td>\n",
       "      <td>27.22</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39168 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ts_code  trade_date  name     open      low     high    close  change  \\\n",
       "0       801010    20140401  农林牧渔  1668.75  1668.54  1689.12  1689.07   22.13   \n",
       "1       801010    20140402  农林牧渔  1688.72  1684.53  1693.41  1692.24    3.17   \n",
       "2       801010    20140403  农林牧渔  1693.05  1679.85  1697.73  1685.71   -6.53   \n",
       "3       801010    20140404  农林牧渔  1681.92  1680.34  1698.44  1698.25   12.54   \n",
       "4       801010    20140408  农林牧渔  1693.24  1692.22  1706.84  1706.84    8.59   \n",
       "5       801010    20140409  农林牧渔  1708.13  1707.49  1726.01  1725.55   18.71   \n",
       "6       801010    20140410  农林牧渔  1723.57  1721.18  1730.89  1725.67    0.12   \n",
       "7       801010    20140411  农林牧渔  1721.84  1713.55  1724.08  1719.15   -6.52   \n",
       "8       801010    20140414  农林牧渔  1718.35  1717.57  1751.81  1749.88   30.73   \n",
       "9       801010    20140415  农林牧渔  1747.73  1743.36  1758.92  1748.24   -1.64   \n",
       "10      801010    20140416  农林牧渔  1743.14  1743.07  1761.87  1759.88   11.64   \n",
       "11      801010    20140417  农林牧渔  1765.11  1751.34  1768.27  1751.95   -7.93   \n",
       "12      801010    20140418  农林牧渔  1747.78  1740.37  1749.27  1748.07   -3.88   \n",
       "13      801010    20140421  农林牧渔  1738.50  1721.93  1753.82  1723.44  -24.63   \n",
       "14      801010    20140422  农林牧渔  1719.55  1689.66  1726.53  1712.55  -10.89   \n",
       "15      801010    20140423  农林牧渔  1713.95  1704.45  1723.92  1708.59   -3.96   \n",
       "16      801010    20140424  农林牧渔  1705.93  1695.73  1711.51  1696.33  -12.26   \n",
       "17      801010    20140425  农林牧渔  1697.29  1664.62  1698.52  1665.01  -31.32   \n",
       "18      801010    20140428  农林牧渔  1659.78  1606.24  1660.02  1608.36  -56.65   \n",
       "19      801010    20140429  农林牧渔  1606.90  1606.45  1630.99  1630.56   22.20   \n",
       "20      801010    20140430  农林牧渔  1627.63  1620.66  1635.84  1634.53    3.97   \n",
       "21      801010    20140505  农林牧渔  1631.20  1627.64  1669.82  1669.82   35.29   \n",
       "22      801010    20140506  农林牧渔  1666.06  1665.70  1692.11  1687.83   18.01   \n",
       "23      801010    20140507  农林牧渔  1683.20  1663.34  1685.64  1664.49  -23.34   \n",
       "24      801010    20140508  农林牧渔  1662.49  1658.42  1683.17  1662.89   -1.60   \n",
       "25      801010    20140509  农林牧渔  1660.53  1633.00  1662.81  1647.72  -15.17   \n",
       "26      801010    20140512  农林牧渔  1657.54  1651.31  1682.60  1682.57   34.85   \n",
       "27      801010    20140513  农林牧渔  1683.43  1675.81  1690.56  1679.71   -2.86   \n",
       "28      801010    20140514  农林牧渔  1680.04  1678.35  1689.54  1687.06    7.35   \n",
       "29      801010    20140515  农林牧渔  1690.36  1667.12  1702.37  1668.01  -19.05   \n",
       "...        ...         ...   ...      ...      ...      ...      ...     ...   \n",
       "39138   802600    20190219  交银装备  3579.18  3523.59  3592.14  3571.10    7.13   \n",
       "39139   802600    20190220  交银装备  3565.80  3530.05  3576.58  3573.88    2.78   \n",
       "39140   802600    20190221  交银装备  3570.03  3558.00  3647.76  3571.58   -2.30   \n",
       "39141   802600    20190222  交银装备  3564.22  3562.49  3667.00  3667.00   95.42   \n",
       "39142   802600    20190225  交银装备  3732.26  3731.09  3866.40  3865.52  198.52   \n",
       "39143   802600    20190226  交银装备  3878.98  3823.22  3935.83  3847.28  -18.24   \n",
       "39144   802600    20190227  交银装备  3834.98  3774.68  3880.18  3817.56  -29.72   \n",
       "39145   802600    20190228  交银装备  3819.39  3802.56  3853.09  3826.23    8.67   \n",
       "39146   802600    20190301  交银装备  3846.13  3795.75  3857.13  3857.13   30.90   \n",
       "39147   802600    20190304  交银装备  3900.97  3895.05  4001.73  3930.32   73.19   \n",
       "39148   802600    20190305  交银装备  3921.66  3907.79  4044.30  4044.30  113.98   \n",
       "39149   802600    20190306  交银装备  4073.73  4018.09  4110.34  4110.04   65.73   \n",
       "39150   802600    20190307  交银装备  4103.92  4073.72  4184.69  4146.26   36.22   \n",
       "39151   802600    20190308  交银装备  4049.63  4000.45  4180.26  4000.45 -145.81   \n",
       "39152   802600    20190311  交银装备  4038.47  4022.38  4167.05  4167.05  166.60   \n",
       "39153   802600    20190312  交银装备  4206.96  4185.74  4285.56  4259.90   92.85   \n",
       "39154   802600    20190313  交银装备  4257.71  4104.35  4257.71  4131.95 -127.95   \n",
       "39155   802600    20190314  交银装备  4095.09  3978.84  4122.73  4024.24 -107.71   \n",
       "39156   802600    20190315  交银装备  4047.72  4021.14  4102.95  4062.88   38.64   \n",
       "39157   802600    20190318  交银装备  4077.09  4035.28  4152.46  4152.46   89.58   \n",
       "39158   802600    20190319  交银装备  4154.04  4146.98  4206.74  4175.87   23.41   \n",
       "39159   802600    20190320  交银装备  4174.58  4091.01  4181.05  4161.25  -14.62   \n",
       "39160   802600    20190321  交银装备  4166.62  4158.46  4241.77  4209.20   47.95   \n",
       "39161   802600    20190322  交银装备  4212.52  4139.92  4223.43  4217.71    8.51   \n",
       "39162   802600    20190325  交银装备  4146.60  4134.48  4230.93  4178.99  -38.72   \n",
       "39163   802600    20190326  交银装备  4195.85  4054.34  4208.28  4062.73 -116.26   \n",
       "39164   802600    20190327  交银装备  4090.42  4002.09  4100.63  4072.08    9.35   \n",
       "39165   802600    20190328  交银装备  4058.84  4028.51  4116.58  4029.95  -42.13   \n",
       "39166   802600    20190329  交银装备  4030.42  3995.79  4145.43  4145.43  115.48   \n",
       "39167   802600    20190401  交银装备  4182.31  4182.25  4306.69  4306.27  160.84   \n",
       "\n",
       "       pct_change        vol      amount     pe    pb  y  \n",
       "0            1.33    34914.0    291113.0  41.51  2.77  1  \n",
       "1            0.19    36300.0    289020.0  41.63  2.79  1  \n",
       "2           -0.39    31403.0    259464.0  41.38  2.78  0  \n",
       "3            0.74    28648.0    240940.0  41.76  2.80  1  \n",
       "4            0.51    35012.0    312423.0  42.00  2.79  1  \n",
       "5            1.10    43114.0    378611.0  41.68  2.81  1  \n",
       "6            0.01    42089.0    386253.0  41.68  2.81  1  \n",
       "7           -0.38    33973.0    302877.0  41.38  2.80  0  \n",
       "8            1.79    45683.0    440762.0  42.11  2.86  1  \n",
       "9           -0.09    52524.0    482483.0  41.83  2.89  0  \n",
       "10           0.67    47387.0    415858.0  41.33  2.91  1  \n",
       "11          -0.45    39258.0    364281.0  41.19  2.89  0  \n",
       "12          -0.22    27658.0    250801.0  41.05  2.87  0  \n",
       "13          -1.41    34708.0    317704.0  40.55  2.84  0  \n",
       "14          -0.63    37810.0    350380.0  40.55  2.81  0  \n",
       "15          -0.23    29933.0    281126.0  40.72  2.80  0  \n",
       "16          -0.72    28072.0    263656.0  40.65  2.78  0  \n",
       "17          -1.85    36588.0    345842.0  39.63  2.73  0  \n",
       "18          -3.40    34912.0    308748.0  38.54  2.63  0  \n",
       "19           1.38    24681.0    217555.0  40.81  2.65  1  \n",
       "20           0.24    22190.0    200379.0  41.30  2.61  1  \n",
       "21           2.16    30320.0    309628.0  43.57  2.67  1  \n",
       "22           1.08    33004.0    352131.0  44.04  2.70  1  \n",
       "23          -1.38    29082.0    296894.0  43.43  2.66  0  \n",
       "24          -0.10    23884.0    235397.0  43.39  2.66  0  \n",
       "25          -0.91    22227.0    217123.0  42.99  2.63  0  \n",
       "26           2.12    31357.0    280848.0  44.03  2.68  1  \n",
       "27          -0.17    27356.0    261752.0  43.96  2.68  0  \n",
       "28           0.44    28557.0    274429.0  44.15  2.69  1  \n",
       "29          -1.13    34382.0    345523.0  43.66  2.66  0  \n",
       "...           ...        ...         ...    ...   ... ..  \n",
       "39138        0.20  3830034.0  33069322.0  22.52  2.07  1  \n",
       "39139        0.08  3228623.0  27097884.0  22.60  2.07  1  \n",
       "39140       -0.06  3883750.0  34698806.0  22.58  2.07  0  \n",
       "39141        2.67  3641490.0  33151818.0  23.18  2.13  1  \n",
       "39142        5.41  5568265.0  51762223.0  24.43  2.24  1  \n",
       "39143       -0.47  5892699.0  54473918.0  24.33  2.23  0  \n",
       "39144       -0.77  4787456.0  43811779.0  24.12  2.22  0  \n",
       "39145        0.23  3731296.0  32883739.0  24.17  2.22  1  \n",
       "39146        0.81  3507033.0  31617194.0  24.37  2.24  1  \n",
       "39147        1.90  5295754.0  50696989.0  24.81  2.28  1  \n",
       "39148        2.90  5071454.0  48564822.0  25.51  2.35  1  \n",
       "39149        1.63  6102440.0  58035380.0  25.90  2.39  1  \n",
       "39150        0.88  5969744.0  58999917.0  26.09  2.41  1  \n",
       "39151       -3.52  6397305.0  63981639.0  25.19  2.32  0  \n",
       "39152        4.16  5248882.0  53541545.0  26.24  2.42  1  \n",
       "39153        2.23  6373095.0  65592259.0  26.84  2.47  1  \n",
       "39154       -3.00  5788780.0  60004033.0  26.03  2.40  0  \n",
       "39155       -2.61  4363434.0  43033723.0  25.37  2.33  0  \n",
       "39156        0.96  3734033.0  37227564.0  25.63  2.36  1  \n",
       "39157        2.20  3854941.0  38517998.0  26.20  2.41  1  \n",
       "39158        0.56  3873893.0  39344316.0  26.35  2.42  1  \n",
       "39159       -0.35  3899754.0  38579665.0  26.23  2.41  0  \n",
       "39160        1.15  4519727.0  44639328.0  26.53  2.44  1  \n",
       "39161        0.20  4111832.0  41501342.0  26.60  2.44  1  \n",
       "39162       -0.92  4151879.0  42732362.0  26.34  2.42  0  \n",
       "39163       -2.78  3998022.0  40950501.0  25.61  2.35  0  \n",
       "39164        0.23  3231654.0  33449356.0  25.71  2.36  1  \n",
       "39165       -1.03  3175370.0  34319281.0  25.46  2.33  0  \n",
       "39166        2.87  3671961.0  39397777.0  26.20  2.40  1  \n",
       "39167        3.88  4620338.0  51023180.0  27.22  2.49  1  \n",
       "\n",
       "[39168 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、文本处理阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    '''获取停用词'''\n",
    "    stop_words = []\n",
    "    for file in os.listdir('./stopwords'):\n",
    "        with open('./stopwords/{}'.format(file),encoding='utf8') as f:\n",
    "            stop_words.extend(f.read().splitlines())\n",
    "    return set(stop_words)\n",
    "\n",
    "stop_words = get_stopwords()\n",
    "\n",
    "def clear_data(data,is_nan=0):\n",
    "    # 注意这里有nan数据\n",
    "    '''数据的清洗'''\n",
    "    if data is np.nan:\n",
    "        data = ''\n",
    "        is_nan += 1\n",
    "        print('\\rHere are {} empty titles...'.format(is_nan),end='')\n",
    "    data = re.sub(r'[1-9]+月[0-9]+日至[0-9]+日','',data)\n",
    "    data = re.sub(r'(当地时间)*[0-9]+日','',data)\n",
    "    data = re.sub(r'[1-9]+月[0-9]+日([上|下]午)*','',data)\n",
    "    data = re.sub(r'[0-9]+日([上|下]午)*','',data)\n",
    "    data = re.sub(r'[0-9]+时[0-9]+分(许)*','',data)\n",
    "    data = re.sub(r'([0-9]+日)*([上|下]午)*[0-9]+时[0-9]+分(许)*','',data)\n",
    "    data = re.sub(r'[a-zA-Z0-9]+','',data)\n",
    "    data = re.sub(r'[,.，。、!:\"\\'：《》【】’%‘“”)(（·）—]','',data)\n",
    "    data = re.sub(r'\\s+',' ',data)\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    '''结巴分词'''\n",
    "    data = jieba.cut(data)\n",
    "    # todo token的时候进行停用词的去除\n",
    "    data = [word for word in data if word not in stop_words ]\n",
    "    \n",
    "#     data =  ' '.join(data) + '\\n'\n",
    "    return data\n",
    "\n",
    "def data_process(head,data,save_path=''):\n",
    "    '''数据处理第一大部分'''\n",
    "    cleared = data[head].apply(clear_data)\n",
    "    splited = cleared.apply(split_data)\n",
    "    \n",
    "    return splited\n",
    "\n",
    "#     with open('./{}'.format(save_path),'w',encoding='utf8') as f:\n",
    "#         f.writelines(splited_title)\n",
    "#         print('\\nTo {} processed.'.format(save_path))\n",
    "\n",
    "# 统计空title\n",
    "if not os.path.exists('./my_data'):\n",
    "    os.makedirs('./my_data')\n",
    "    print('创建文件夹({})成功'.format('/my_data'))\n",
    "        \n",
    "    \n",
    "# data_process('title',save_path='./my_data/titles_split.txt')\n",
    "# data_process('content',save_path='./my_data/contents_split.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 word2idx && idx2word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['在', '发展', '对华关系', '方面', '多次', '领跑', '感到', '自豪', '我', '赞同', '习', '主席', '对', '两国关系', '的', '评价', '期待', '通过', '此访', '加强', '新中', '全面', '战略伙伴', '关系', '深化', '两', '国', '经贸合作', '和', '人文', '交流', '我愿', '重申', '新西兰', '坚持', '一个', '中国', '政策', '新西兰', '奉行', '独立自主', '的', '外交政策', '坚定', '支持', '多边', '主义', '支持', '自由贸易', '很', '早就', '支持', '一带', '一路', '倡议', '参加', '亚洲', '基础设施', '投资银行', '新西兰', '将', '派', '高级别', '代表团', '来华', '出席', '即将', '举行', '的', '第二届', '一带', '一路', '国际', '合作', '高峰论坛', '深化', '共建', '一带', '一路', '合作', '新方愿', '就', '应对', '气候变化', '等', '重大', '国际', '问题', '密切', '同', '中方', '的', '协调', '配合', '杨洁篪', '王毅', '何立峰', '等', '参加', '会见']\n"
     ]
    }
   ],
   "source": [
    "# 合并所有的数据\n",
    "def concat_splited_data():\n",
    "    '''整合词表以便统计单词'''\n",
    "    words = []\n",
    "    with open('./my_data/titles_split.txt',encoding='utf8') as f:\n",
    "        for sent in f.readlines():\n",
    "            words.extend(sent.strip().split()) \n",
    "\n",
    "    with open('./my_data/contents_split.txt',encoding='utf8') as f:\n",
    "        for sent in f.readlines():\n",
    "            words.extend(sent.strip().split())\n",
    "        \n",
    "    return words\n",
    "\n",
    "data = concat_splited_data()\n",
    "print(data[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5914844\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('芙蓉', 7),\n",
       " ('一首首', 7),\n",
       " ('矿坑', 7),\n",
       " ('胶囊', 7),\n",
       " ('毛驴', 7),\n",
       " ('黄孟复', 7),\n",
       " ('净化器', 7),\n",
       " ('服装品牌', 7),\n",
       " ('喀拉', 7),\n",
       " ('长河', 7),\n",
       " ('方案设计', 7),\n",
       " ('中国经贸', 7),\n",
       " ('发源', 7),\n",
       " ('青河县', 7),\n",
       " ('结核病', 7),\n",
       " ('全球战略', 7),\n",
       " ('葫芦岛市', 7),\n",
       " ('别具特色', 7),\n",
       " ('新时尚', 7),\n",
       " ('出岛', 7),\n",
       " ('精度高', 7),\n",
       " ('蔬果', 7),\n",
       " ('红外线', 7),\n",
       " ('跟得上', 7),\n",
       " ('唐古拉山', 7),\n",
       " ('推高', 7),\n",
       " ('应用领域', 7),\n",
       " ('棕榈', 7),\n",
       " ('结构复杂', 7),\n",
       " ('军品', 7),\n",
       " ('吴桥', 7),\n",
       " ('看电视', 7),\n",
       " ('地面站', 7),\n",
       " ('西溪', 7),\n",
       " ('戴尔', 7),\n",
       " ('海藻', 7),\n",
       " ('何炳南', 7),\n",
       " ('半天', 7),\n",
       " ('廖雪玲', 7),\n",
       " ('雁翅楼', 7),\n",
       " ('原则同意', 7),\n",
       " ('小孙', 7),\n",
       " ('西欧', 7),\n",
       " ('一个点', 7),\n",
       " ('美兰', 7),\n",
       " ('西安卫星测控中心', 7),\n",
       " ('皮埃尔', 7),\n",
       " ('北京航空航天大学', 7),\n",
       " ('首单', 7),\n",
       " ('宝贵意见', 7),\n",
       " ('青贮', 7),\n",
       " ('童话', 7),\n",
       " ('均等', 7),\n",
       " ('中国海关', 7),\n",
       " ('弯路', 7),\n",
       " ('韩俊', 7),\n",
       " ('洪小勇', 7),\n",
       " ('好帮手', 7),\n",
       " ('亲密无间', 7),\n",
       " ('显示器', 7),\n",
       " ('永年', 7),\n",
       " ('徐秀娟', 7),\n",
       " ('穆虹', 7),\n",
       " ('盖房子', 7),\n",
       " ('北京市政府', 7),\n",
       " ('金融管理', 7),\n",
       " ('通过考核', 7),\n",
       " ('央广', 7),\n",
       " ('不厌其烦', 7),\n",
       " ('莫兰', 7),\n",
       " ('三文鱼', 7),\n",
       " ('靠拢', 7),\n",
       " ('眼疾', 7),\n",
       " ('陈雷', 7),\n",
       " ('互动式', 7),\n",
       " ('人口众多', 7),\n",
       " ('效能型', 7),\n",
       " ('首钢', 7),\n",
       " ('复明', 7),\n",
       " ('却成', 7),\n",
       " ('姚檀栋', 7),\n",
       " ('穿山甲', 7),\n",
       " ('山花', 7),\n",
       " ('诺市', 7),\n",
       " ('莫中', 7),\n",
       " ('几万', 7),\n",
       " ('泰兴市', 7),\n",
       " ('布托', 7),\n",
       " ('尼方愿', 7),\n",
       " ('停驶', 7),\n",
       " ('阿济', 7),\n",
       " ('肺部', 7),\n",
       " ('弱冷空气', 7),\n",
       " ('保育', 7),\n",
       " ('走后门', 7),\n",
       " ('比达', 7),\n",
       " ('前十', 7),\n",
       " ('刘振民', 7),\n",
       " ('差率', 7),\n",
       " ('光环', 7),\n",
       " ('选优', 7),\n",
       " ('萨卡', 7),\n",
       " ('墓葬', 7),\n",
       " ('竹简', 7),\n",
       " ('互联网安全', 7),\n",
       " ('京台', 7),\n",
       " ('乱放', 7),\n",
       " ('澳门半岛', 7),\n",
       " ('闯过', 7),\n",
       " ('拉什', 7),\n",
       " ('大中专', 7),\n",
       " ('监督网', 7),\n",
       " ('遵从', 7),\n",
       " ('军是', 7),\n",
       " ('一年四季', 7),\n",
       " ('冰下', 7),\n",
       " ('保持一致', 7),\n",
       " ('敏锐性', 7),\n",
       " ('选段', 7),\n",
       " ('旁遮普邦', 7),\n",
       " ('苗家', 7),\n",
       " ('妈', 7),\n",
       " ('马扎里沙里夫', 7),\n",
       " ('时度效', 7),\n",
       " ('无中生有', 7),\n",
       " ('华大基因', 7),\n",
       " ('登峰', 7),\n",
       " ('冬闲', 7),\n",
       " ('逢节', 7),\n",
       " ('乌山', 7),\n",
       " ('麻怀村', 7),\n",
       " ('猛村', 7),\n",
       " ('镇安县', 7),\n",
       " ('尹本', 7),\n",
       " ('一开', 7),\n",
       " ('乞讨', 7),\n",
       " ('石泉县', 7),\n",
       " ('问效', 7),\n",
       " ('菊', 7),\n",
       " ('开拓精神', 7),\n",
       " ('边缘化', 7),\n",
       " ('心手', 7),\n",
       " ('冰期', 7),\n",
       " ('多颗', 7),\n",
       " ('贾尼', 7),\n",
       " ('宁陕县', 7),\n",
       " ('鲁冠球', 7),\n",
       " ('兽药', 7),\n",
       " ('新貌', 7),\n",
       " ('川陕', 7),\n",
       " ('年糕', 7),\n",
       " ('马化腾', 7),\n",
       " ('汽车销量', 7),\n",
       " ('茅坪', 7),\n",
       " ('号子', 7),\n",
       " ('至点', 7),\n",
       " ('桐城', 7),\n",
       " ('感受一下', 7),\n",
       " ('吉他', 7),\n",
       " ('开关', 7),\n",
       " ('十四个', 7),\n",
       " ('异乡', 7),\n",
       " ('完年', 7),\n",
       " ('爱因斯坦', 7),\n",
       " ('赏景', 7),\n",
       " ('锣鼓声', 7),\n",
       " ('讨价还价', 7),\n",
       " ('雪橇', 7),\n",
       " ('赵一立', 7),\n",
       " ('春来早', 7),\n",
       " ('眼瞅', 7),\n",
       " ('不具', 7),\n",
       " ('迎接挑战', 7),\n",
       " ('离婚', 7),\n",
       " ('两微', 7),\n",
       " ('拆旧', 7),\n",
       " ('西卢安', 7),\n",
       " ('多方位', 7),\n",
       " ('第二类', 7),\n",
       " ('抽样', 7),\n",
       " ('秦真岭', 7),\n",
       " ('禁渔', 7),\n",
       " ('格桑卓嘎', 7),\n",
       " ('庆丰', 7),\n",
       " ('比比', 7),\n",
       " ('黄羊', 7),\n",
       " ('修文', 7),\n",
       " ('陈先生', 7),\n",
       " ('刘士余', 7),\n",
       " ('斗殴', 7),\n",
       " ('亨利', 7),\n",
       " ('上海大学', 7),\n",
       " ('马蒂', 7),\n",
       " ('比尼', 7),\n",
       " ('拉姆齐', 7),\n",
       " ('服药', 7),\n",
       " ('入盟', 7),\n",
       " ('宜游', 7),\n",
       " ('客服', 7),\n",
       " ('小看', 7),\n",
       " ('走街串巷', 7),\n",
       " ('气象日', 7),\n",
       " ('民用机场', 7),\n",
       " ('每场', 7),\n",
       " ('百只', 7),\n",
       " ('托老所', 7),\n",
       " ('摊', 7),\n",
       " ('私宅', 7),\n",
       " ('道口', 7),\n",
       " ('冯德', 7),\n",
       " ('一法', 7),\n",
       " ('库什', 7),\n",
       " ('英国国防部', 7),\n",
       " ('王平', 7),\n",
       " ('挥锹', 7),\n",
       " ('巴库', 7),\n",
       " ('陨石', 7),\n",
       " ('充气式', 7),\n",
       " ('严守纪律', 7),\n",
       " ('投食', 7),\n",
       " ('屏山县', 7),\n",
       " ('黑木耳', 7),\n",
       " ('火山地震', 7),\n",
       " ('巡逻机', 7),\n",
       " ('灵敏度', 7),\n",
       " ('贩子', 7),\n",
       " ('揭榜', 7),\n",
       " ('政教', 7),\n",
       " ('工商业者', 7),\n",
       " ('百倍', 7),\n",
       " ('脚力', 7),\n",
       " ('笔力', 7),\n",
       " ('三宝', 7),\n",
       " ('惧怕', 7),\n",
       " ('员们', 7),\n",
       " ('慕名', 7),\n",
       " ('大上', 7),\n",
       " ('超冷', 7),\n",
       " ('个人收入', 7),\n",
       " ('试金石', 7),\n",
       " ('自治县', 7),\n",
       " ('实用性', 7),\n",
       " ('油管', 7),\n",
       " ('董绪伦', 7),\n",
       " ('刘贺', 7),\n",
       " ('于伟国', 7),\n",
       " ('公差', 7),\n",
       " ('伊斯坎德尔', 7),\n",
       " ('真信', 7),\n",
       " ('海之滨', 7),\n",
       " ('矿点', 7),\n",
       " ('二哥', 7),\n",
       " ('贞丰县', 7),\n",
       " ('指手画脚', 7),\n",
       " ('计算能力', 7),\n",
       " ('体育部', 7),\n",
       " ('闽粤', 7),\n",
       " ('云盘', 7),\n",
       " ('达维', 7),\n",
       " ('旧村', 7),\n",
       " ('坚冰', 7),\n",
       " ('冲日', 7),\n",
       " ('南京航空航天大学', 7),\n",
       " ('等离子体', 7),\n",
       " ('一小', 7),\n",
       " ('节日快乐', 7),\n",
       " ('王建军', 7),\n",
       " ('地学', 7),\n",
       " ('库琴斯', 7),\n",
       " ('水势', 7),\n",
       " ('欣然', 7),\n",
       " ('尼山', 7),\n",
       " ('北非国家', 7),\n",
       " ('长白山天池', 7),\n",
       " ('赛马节', 7),\n",
       " ('势不可挡', 7),\n",
       " ('同尼', 7),\n",
       " ('熔化', 7),\n",
       " ('泄露机密', 7),\n",
       " ('只身', 7),\n",
       " ('政策方针', 7),\n",
       " ('弹道', 7),\n",
       " ('阜宁县', 7),\n",
       " ('阿塔图尔克', 7),\n",
       " ('刘波', 7),\n",
       " ('暗夜', 7),\n",
       " ('肾', 7),\n",
       " ('自行设计', 7),\n",
       " ('对空', 7),\n",
       " ('古丈县', 7),\n",
       " ('奥运健儿', 7),\n",
       " ('联合国贸易和发展会议', 7),\n",
       " ('正说', 7),\n",
       " ('个人卫生', 7),\n",
       " ('前头', 7),\n",
       " ('唐山人', 7),\n",
       " ('亚当斯', 7),\n",
       " ('下到', 7),\n",
       " ('十余', 7),\n",
       " ('中国女篮', 7),\n",
       " ('集体承包', 7),\n",
       " ('律师协会', 7),\n",
       " ('华中科技大学', 7),\n",
       " ('共枚', 7),\n",
       " ('董栋', 7),\n",
       " ('刘洋', 7),\n",
       " ('俄伊', 7),\n",
       " ('任茜', 7),\n",
       " ('作伪证', 7),\n",
       " ('加塞', 7),\n",
       " ('同质', 7),\n",
       " ('霓虹', 7),\n",
       " ('中影', 7),\n",
       " ('值勤', 7),\n",
       " ('网红', 7),\n",
       " ('佩德罗', 7),\n",
       " ('巴杰', 7),\n",
       " ('万盏', 7),\n",
       " ('织造', 7),\n",
       " ('钣', 7),\n",
       " ('索尔兹伯里', 7),\n",
       " ('重大胜利', 7),\n",
       " ('一二', 7),\n",
       " ('周炳耀', 7),\n",
       " ('中华全国归国华侨联合会', 7),\n",
       " ('写票', 7),\n",
       " ('毕世华', 7),\n",
       " ('轨道舱', 7),\n",
       " ('新建村', 7),\n",
       " ('海法', 7),\n",
       " ('穹顶', 7),\n",
       " ('车灯', 7),\n",
       " ('以船', 7),\n",
       " ('冬笋', 7),\n",
       " ('媚俗', 7),\n",
       " ('清产核资', 7),\n",
       " ('这成', 7),\n",
       " ('烟草专卖局', 7),\n",
       " ('主动脉', 7),\n",
       " ('药箱', 7),\n",
       " ('东渡', 7),\n",
       " ('劳有所得', 7),\n",
       " ('阿姆里', 7),\n",
       " ('火险', 7),\n",
       " ('腊八节', 7),\n",
       " ('校准', 7),\n",
       " ('高田', 7),\n",
       " ('韦恩', 7),\n",
       " ('波拉', 7),\n",
       " ('硬脱', 7),\n",
       " ('电煤', 7),\n",
       " ('绥化', 7),\n",
       " ('一年之计在于春', 7),\n",
       " ('选情', 7),\n",
       " ('曲玉权', 7),\n",
       " ('耶茨', 7),\n",
       " ('德沃斯', 7),\n",
       " ('王帆', 7),\n",
       " ('长富', 7),\n",
       " ('开航', 7),\n",
       " ('罗维尔', 7),\n",
       " ('孤立主义', 7),\n",
       " ('衡阳县', 7),\n",
       " ('扶梯', 7),\n",
       " ('麦克马斯特', 7),\n",
       " ('密集区', 7),\n",
       " ('孟玲芬', 7),\n",
       " ('反舰', 7),\n",
       " ('初春', 7),\n",
       " ('天鲲', 7),\n",
       " ('李林', 7),\n",
       " ('刘昆作', 7),\n",
       " ('埃特纳', 7),\n",
       " ('藕', 7),\n",
       " ('泰典', 7),\n",
       " ('金世富', 7),\n",
       " ('还贷', 7),\n",
       " ('刘赐贵', 7),\n",
       " ('中程导弹', 7),\n",
       " ('颇丰', 7),\n",
       " ('金平', 7),\n",
       " ('武雯', 7),\n",
       " ('之母', 7),\n",
       " ('彼得森', 7),\n",
       " ('卡留', 7),\n",
       " ('安哲秀', 7),\n",
       " ('车俊', 7),\n",
       " ('丝胶', 7),\n",
       " ('官厅水库', 7),\n",
       " ('残渣', 7),\n",
       " ('爱国运动', 7),\n",
       " ('企联', 7),\n",
       " ('国之志', 7),\n",
       " ('朱雨玲', 7),\n",
       " ('皇马', 7),\n",
       " ('过海', 7),\n",
       " ('委要', 7),\n",
       " ('呈递', 7),\n",
       " ('孙志刚', 7),\n",
       " ('包钢', 7),\n",
       " ('提审', 7),\n",
       " ('彩云', 7),\n",
       " ('葵花籽', 7),\n",
       " ('九寨', 7),\n",
       " ('杨道', 7),\n",
       " ('李鲁帅', 7),\n",
       " ('采沉区', 7),\n",
       " ('叶卡捷琳堡', 7),\n",
       " ('靓靓', 7),\n",
       " ('留抵', 7),\n",
       " ('斯托克', 7),\n",
       " ('第二架', 7),\n",
       " ('张彦', 7),\n",
       " ('洋山', 7),\n",
       " ('曹先建', 7),\n",
       " ('傅政华', 7),\n",
       " ('二十大', 7),\n",
       " ('阿罗约', 7),\n",
       " ('曲巴', 7),\n",
       " ('萨阿德', 7),\n",
       " ('武文赏', 7),\n",
       " ('宋迪', 7),\n",
       " ('阿根廷海军', 7),\n",
       " ('艇员', 7),\n",
       " ('图布辛', 7),\n",
       " ('中导', 7),\n",
       " ('投标法', 7),\n",
       " ('监察权', 7),\n",
       " ('不骛于', 7),\n",
       " ('虚声', 7),\n",
       " ('陈果', 7),\n",
       " ('维阿', 7),\n",
       " ('蓝花', 7),\n",
       " ('上朝', 7),\n",
       " ('辛庄村', 7),\n",
       " ('正常率', 7),\n",
       " ('自然段', 7),\n",
       " ('溶', 7),\n",
       " ('王银香', 7),\n",
       " ('选举票', 7),\n",
       " ('几比', 7),\n",
       " ('达达埃', 7),\n",
       " ('拔高', 7),\n",
       " ('马斯', 7),\n",
       " ('黄花岗', 7),\n",
       " ('同盟会', 7),\n",
       " ('刘鹤何', 7),\n",
       " ('因伊核', 7),\n",
       " ('秋分', 7),\n",
       " ('兰特', 7),\n",
       " ('扇贝', 7),\n",
       " ('共兴', 7),\n",
       " ('同萨方', 7),\n",
       " ('郭宗俊', 7),\n",
       " ('艾沙', 7),\n",
       " ('博索', 7),\n",
       " ('渔歌', 7),\n",
       " ('白格', 7),\n",
       " ('戈恩', 7),\n",
       " ('埃斯库', 7),\n",
       " ('德罗', 7),\n",
       " ('西娅', 7),\n",
       " ('鲍尔', 7),\n",
       " ('邱娥国', 7),\n",
       " ('钟竹筠', 7),\n",
       " ('邮车', 7),\n",
       " ('哨点', 7),\n",
       " ('北极熊', 7),\n",
       " ('杨忠岐', 7),\n",
       " ('狼窝', 7),\n",
       " ('贾永青', 6),\n",
       " ('新关角', 6),\n",
       " ('四百多', 6),\n",
       " ('优先股', 6),\n",
       " ('国家能源委员会', 6),\n",
       " ('祭品', 6),\n",
       " ('黑金', 6),\n",
       " ('生聚', 6),\n",
       " ('黎介寿', 6),\n",
       " ('唱主角', 6),\n",
       " ('做点', 6),\n",
       " ('田阳', 6),\n",
       " ('反贪', 6),\n",
       " ('造谣', 6),\n",
       " ('唐述', 6),\n",
       " ('滚动式', 6),\n",
       " ('京外', 6),\n",
       " ('延坪岛', 6),\n",
       " ('神农', 6),\n",
       " ('兴疆', 6),\n",
       " ('招远', 6),\n",
       " ('礼泉', 6),\n",
       " ('蒙阴', 6),\n",
       " ('冠', 6),\n",
       " ('明晨', 6),\n",
       " ('开球', 6),\n",
       " ('阿旺', 6),\n",
       " ('年度预算', 6),\n",
       " ('郝龙斌', 6),\n",
       " ('毕世祥', 6),\n",
       " ('杨瑞辉', 6),\n",
       " ('纳米材料', 6),\n",
       " ('正反', 6),\n",
       " ('玷污', 6),\n",
       " ('红柳', 6),\n",
       " ('针线包', 6),\n",
       " ('绝杀', 6),\n",
       " ('澎湖', 6),\n",
       " ('舍己救人', 6),\n",
       " ('好学', 6),\n",
       " ('澳门中华总商会', 6),\n",
       " ('蔡秀梅', 6),\n",
       " ('金凤', 6),\n",
       " ('深得人心', 6),\n",
       " ('随礼', 6),\n",
       " ('哲学家', 6),\n",
       " ('同饮', 6),\n",
       " ('医联', 6),\n",
       " ('两袖清风', 6),\n",
       " ('江红', 6),\n",
       " ('陈榕', 6),\n",
       " ('韩媒', 6),\n",
       " ('葛', 6),\n",
       " ('同享', 6),\n",
       " ('机关化', 6),\n",
       " ('始终不变', 6),\n",
       " ('尉犁县', 6),\n",
       " ('鸣警', 6),\n",
       " ('新民党', 6),\n",
       " ('获枚', 6),\n",
       " ('勤政为民', 6),\n",
       " ('误', 6),\n",
       " ('工联', 6),\n",
       " ('湾仔', 6),\n",
       " ('纠纷案件', 6),\n",
       " ('辈出', 6),\n",
       " ('无穷的', 6),\n",
       " ('汉风', 6),\n",
       " ('生化武器', 6),\n",
       " ('影视界', 6),\n",
       " ('摘要', 6),\n",
       " ('秋色', 6),\n",
       " ('俄木', 6),\n",
       " ('根本大法', 6),\n",
       " ('年本', 6),\n",
       " ('金埃', 6),\n",
       " ('槟榔', 6),\n",
       " ('柳青', 6),\n",
       " ('煤检站', 6),\n",
       " ('前前后后', 6),\n",
       " ('小官', 6),\n",
       " ('免予', 6),\n",
       " ('为强', 6),\n",
       " ('周四', 6),\n",
       " ('十五届', 6),\n",
       " ('共融', 6),\n",
       " ('朝鲜人', 6),\n",
       " ('十二月', 6),\n",
       " ('论为', 6),\n",
       " ('长青', 6),\n",
       " ('变通途', 6),\n",
       " ('犬瘟热', 6),\n",
       " ('陈锡联', 6),\n",
       " ('淡泊', 6),\n",
       " ('贡山独龙族怒族自治县', 6),\n",
       " ('正举', 6),\n",
       " ('抗埃', 6),\n",
       " ('坠河', 6),\n",
       " ('雅尔塔', 6),\n",
       " ('独臂', 6),\n",
       " ('仍有', 6),\n",
       " ('惩', 6),\n",
       " ('毕加索', 6),\n",
       " ('聂', 6),\n",
       " ('第四十二', 6),\n",
       " ('热议习', 6),\n",
       " ('如期而至', 6),\n",
       " ('禽肉', 6),\n",
       " ('盗窃案', 6),\n",
       " ('真枪', 6),\n",
       " ('电击', 6),\n",
       " ('朝霞', 6),\n",
       " ('李影超', 6),\n",
       " ('重装', 6),\n",
       " ('吕榕麟', 6),\n",
       " ('陪审员', 6),\n",
       " ('减肥', 6),\n",
       " ('美国黑人', 6),\n",
       " ('保水', 6),\n",
       " ('允展', 6),\n",
       " ('臂膀', 6),\n",
       " ('谨防', 6),\n",
       " ('鲁先平', 6),\n",
       " ('赖远明', 6),\n",
       " ('海防部队', 6),\n",
       " ('外向型', 6),\n",
       " ('崇明', 6),\n",
       " ('冷用斌', 6),\n",
       " ('肃宁县', 6),\n",
       " ('用爱', 6),\n",
       " ('红艳', 6),\n",
       " ('细毛羊', 6),\n",
       " ('增来', 6),\n",
       " ('惠建林', 6),\n",
       " ('文献片', 6),\n",
       " ('罗丹', 6),\n",
       " ('南培', 6),\n",
       " ('铁面', 6),\n",
       " ('深切关怀', 6),\n",
       " ('勇救', 6),\n",
       " ('反对声', 6),\n",
       " ('车轮下', 6),\n",
       " ('幼童', 6),\n",
       " ('终', 6),\n",
       " ('入企', 6),\n",
       " ('对令', 6),\n",
       " ('赴藏', 6),\n",
       " ('二十六', 6),\n",
       " ('薄雾', 6),\n",
       " ('盗', 6),\n",
       " ('存亡', 6),\n",
       " ('一齐', 6),\n",
       " ('获年', 6),\n",
       " ('读万卷书', 6),\n",
       " ('狭路相逢', 6),\n",
       " ('试吃', 6),\n",
       " ('扬长补短', 6),\n",
       " ('国际品牌', 6),\n",
       " ('金质', 6),\n",
       " ('永存', 6),\n",
       " ('王书平', 6),\n",
       " ('人革', 6),\n",
       " ('二营', 6),\n",
       " ('世界大赛', 6),\n",
       " ('区域系统', 6),\n",
       " ('租赁业', 6),\n",
       " ('共推', 6),\n",
       " ('联大会议', 6),\n",
       " ('匾', 6),\n",
       " ('主教', 6),\n",
       " ('美国微软公司', 6),\n",
       " ('授奖仪式', 6),\n",
       " ('江家', 6),\n",
       " ('促进会', 6),\n",
       " ('抒情', 6),\n",
       " ('致癌物', 6),\n",
       " ('十七次', 6),\n",
       " ('祖孙', 6),\n",
       " ('巨幕', 6),\n",
       " ('冲锋号', 6),\n",
       " ('土司', 6),\n",
       " ('开普敦', 6),\n",
       " ('心贴心', 6),\n",
       " ('永久中立', 6),\n",
       " ('一滴', 6),\n",
       " ('平邑', 6),\n",
       " ('吕敬', 6),\n",
       " ('权威人士', 6),\n",
       " ('开发式', 6),\n",
       " ('紧固件', 6),\n",
       " ('以营', 6),\n",
       " ('捡漏', 6),\n",
       " ('立心', 6),\n",
       " ('探险', 6),\n",
       " ('北医三院', 6),\n",
       " ('思乡', 6),\n",
       " ('创效', 6),\n",
       " ('方毅', 6),\n",
       " ('法立', 6),\n",
       " ('干货', 6),\n",
       " ('坦诚相见', 6),\n",
       " ('一团', 6),\n",
       " ('春花', 6),\n",
       " ('抗议声', 6),\n",
       " ('踏春', 6),\n",
       " ('家暴', 6),\n",
       " ('芦', 6),\n",
       " ('人人有责', 6),\n",
       " ('返回式', 6),\n",
       " ('远山', 6),\n",
       " ('杀菌剂', 6),\n",
       " ('云岭', 6),\n",
       " ('刘沙', 6),\n",
       " ('水法', 6),\n",
       " ('迪士尼', 6),\n",
       " ('王阳', 6),\n",
       " ('减上', 6),\n",
       " ('补下', 6),\n",
       " ('杨金龙', 6),\n",
       " ('改玉', 6),\n",
       " ('先声', 6),\n",
       " ('中共中央台办', 6),\n",
       " ('祝词', 6),\n",
       " ('中国芯', 6),\n",
       " ('必问', 6),\n",
       " ('目', 6),\n",
       " ('颗粒归仓', 6),\n",
       " ('美一', 6),\n",
       " ('润', 6),\n",
       " ('星星之火', 6),\n",
       " ('公司律师', 6),\n",
       " ('宜居城市', 6),\n",
       " ('访塞', 6),\n",
       " ('首获', 6),\n",
       " ('势均力敌', 6),\n",
       " ('拉纳', 6),\n",
       " ('费米子', 6),\n",
       " ('安格', 6),\n",
       " ('商运', 6),\n",
       " ('傅企平', 6),\n",
       " ('永立', 6),\n",
       " ('志在', 6),\n",
       " ('倾泻', 6),\n",
       " ('长七', 6),\n",
       " ('搅局', 6),\n",
       " ('百科全书', 6),\n",
       " ('灌顶', 6),\n",
       " ('多起', 6),\n",
       " ('百花园', 6),\n",
       " ('马兰', 6),\n",
       " ('毒气弹', 6),\n",
       " ('赛时', 6),\n",
       " ('陈人海', 6),\n",
       " ('超万人', 6),\n",
       " ('银两', 6),\n",
       " ('补钙', 6),\n",
       " ('金三银', 6),\n",
       " ('苏州高新区', 6),\n",
       " ('礼服', 6),\n",
       " ('挪穷', 6),\n",
       " ('美誉度', 6),\n",
       " ('公学', 6),\n",
       " ('世民', 6),\n",
       " ('国际标准化组织', 6),\n",
       " ('相思', 6),\n",
       " ('民族魂', 6),\n",
       " ('林阿不列', 6),\n",
       " ('诺贝尔物理学奖', 6),\n",
       " ('高峰会', 6),\n",
       " ('第七十九', 6),\n",
       " ('语', 6),\n",
       " ('冷少农', 6),\n",
       " ('认真负责', 6),\n",
       " ('第八十', 6),\n",
       " ('释法', 6),\n",
       " ('天涯若比邻', 6),\n",
       " ('寸步难行', 6),\n",
       " ('一身正气', 6),\n",
       " ('战地', 6),\n",
       " ('忘我', 6),\n",
       " ('潮州', 6),\n",
       " ('拳拳', 6),\n",
       " ('苏知斌', 6),\n",
       " ('肖文智', 6),\n",
       " ('捷报频传', 6),\n",
       " ('治税', 6),\n",
       " ('拐卖妇女', 6),\n",
       " ('新航', 6),\n",
       " ('破难', 6),\n",
       " ('奔忙', 6),\n",
       " ('纯属', 6),\n",
       " ('仪封', 6),\n",
       " ('溜', 6),\n",
       " ('骡马', 6),\n",
       " ('恭贺', 6),\n",
       " ('边防战士', 6),\n",
       " ('冰点', 6),\n",
       " ('多伤', 6),\n",
       " ('治穷', 6),\n",
       " ('电波', 6),\n",
       " ('采油厂', 6),\n",
       " ('德军', 6),\n",
       " ('容缺', 6),\n",
       " ('国安', 6),\n",
       " ('称以', 6),\n",
       " ('侨心', 6),\n",
       " ('第五十四', 6),\n",
       " ('外汇局', 6),\n",
       " ('涉腐', 6),\n",
       " ('突显', 6),\n",
       " ('带回去', 6),\n",
       " ('领保', 6),\n",
       " ('第九十', 6),\n",
       " ('三假', 6),\n",
       " ('查核', 6),\n",
       " ('安全线', 6),\n",
       " ('水清岸', 6),\n",
       " ('城市更新', 6),\n",
       " ('书海', 6),\n",
       " ('六十八', 6),\n",
       " ('这行', 6),\n",
       " ('圣多美', 6),\n",
       " ('树人德法', 6),\n",
       " ('之门', 6),\n",
       " ('六场', 6),\n",
       " ('倾情', 6),\n",
       " ('菲政府', 6),\n",
       " ('闽台', 6),\n",
       " ('冷板凳', 6),\n",
       " ('村小', 6),\n",
       " ('油溪桥', 6),\n",
       " ('立新', 6),\n",
       " ('省籍', 6),\n",
       " ('新磨村', 6),\n",
       " ('第六十九', 6),\n",
       " ('浓烈', 6),\n",
       " ('朔', 6),\n",
       " ('以向', 6),\n",
       " ('徐嘉余', 6),\n",
       " ('第六十二', 6),\n",
       " ('建绿', 6),\n",
       " ('七论', 6),\n",
       " ('颜值', 6),\n",
       " ('破坏者', 6),\n",
       " ('鸡西', 6),\n",
       " ('展示会', 6),\n",
       " ('丹心', 6),\n",
       " ('击水', 6),\n",
       " ('谦虚谨慎', 6),\n",
       " ('臭氧层', 6),\n",
       " ('先扶志', 6),\n",
       " ('之四', 6),\n",
       " ('再发', 6),\n",
       " ('项俊波', 6),\n",
       " ('受控', 6),\n",
       " ('聘任制', 6),\n",
       " ('逐梦新', 6),\n",
       " ('八十', 6),\n",
       " ('之火', 6),\n",
       " ('胡福明', 6),\n",
       " ('恰是', 6),\n",
       " ('喘', 6),\n",
       " ('玛霍索', 6),\n",
       " ('乾坤', 6),\n",
       " ('米林县', 6),\n",
       " ('苍穹', 6),\n",
       " ('少数派', 6),\n",
       " ('宇宙线', 6),\n",
       " ('叶笃初', 6),\n",
       " ('北段', 6),\n",
       " ('首绘', 6),\n",
       " ('策划者', 6),\n",
       " ('法德英', 6),\n",
       " ('以色列国', 6),\n",
       " ('新课程', 6),\n",
       " ('油船', 6),\n",
       " ('苏皖', 6),\n",
       " ('快步', 6),\n",
       " ('窦店', 6),\n",
       " ('首堆', 6),\n",
       " ('科右', 6),\n",
       " ('亏', 6),\n",
       " ('晴空', 6),\n",
       " ('通俄', 6),\n",
       " ('万紫千红', 6),\n",
       " ('阿婆', 6),\n",
       " ('撤职处分', 6),\n",
       " ('多得', 6),\n",
       " ('温敏', 6),\n",
       " ('三颗', 6),\n",
       " ('机群', 6),\n",
       " ('固防', 6),\n",
       " ('以身许国', 6),\n",
       " ('法学院', 6),\n",
       " ('小村', 6),\n",
       " ('拥枪', 6),\n",
       " ('施洋', 6),\n",
       " ('五卅运动', 6),\n",
       " ('丰田', 6),\n",
       " ('站立起来', 6),\n",
       " ('实证', 6),\n",
       " ('李培东', 6),\n",
       " ('潮涌', 6),\n",
       " ('文泰', 6),\n",
       " ('经略', 6),\n",
       " ('龙州', 6),\n",
       " ('仲夏', 6),\n",
       " ('月央企', 6),\n",
       " ('牛犇', 6),\n",
       " ('张人亚', 6),\n",
       " ('十六强', 6),\n",
       " ('奇葩', 6),\n",
       " ('遮', 6),\n",
       " ('尼雷尔', 6),\n",
       " ('单边制裁', 6),\n",
       " ('侵扰', 6),\n",
       " ('之名', 6),\n",
       " ('蓝标河', 6),\n",
       " ('部约', 6),\n",
       " ('热射病', 6),\n",
       " ('风生水', 6),\n",
       " ('美新', 6),\n",
       " ('百枚', 6),\n",
       " ('熊雄', 6),\n",
       " ('蒋先云', 6),\n",
       " ('以变', 6),\n",
       " ('雷霆', 6),\n",
       " ('锦春', 6),\n",
       " ('五分', 6),\n",
       " ('中卫市', 6),\n",
       " ('隋维钧', 6),\n",
       " ('杜梅', 6),\n",
       " ('大管家', 6),\n",
       " ('勇当', 6),\n",
       " ('乳源', 6),\n",
       " ('混', 6),\n",
       " ('柳传志', 6),\n",
       " ('海洋卫星', 6),\n",
       " ('塔里木', 6),\n",
       " ('冯平', 6),\n",
       " ('不怕死', 6),\n",
       " ('灯火', 6),\n",
       " ('探馆', 6),\n",
       " ('要敢', 6),\n",
       " ('招商银行', 6),\n",
       " ('俞昌准', 6),\n",
       " ('陇', 6),\n",
       " ('企业顾问', 6),\n",
       " ('中俄印', 6),\n",
       " ('病防治', 6),\n",
       " ('万水千山', 6),\n",
       " ('中华魂', 6),\n",
       " ('生态化', 6),\n",
       " ('辞别', 6),\n",
       " ('吴光浩', 6),\n",
       " ('神木', 6),\n",
       " ('媒', 6),\n",
       " ('卡塔尔国', 6),\n",
       " ('夜行', 6),\n",
       " ('地名', 6),\n",
       " ('陈大香', 6),\n",
       " ('九牧', 6),\n",
       " ('理论修养', 6),\n",
       " ('修炼', 6),\n",
       " ('第二十六', 6),\n",
       " ('像素', 6),\n",
       " ('鲁班', 6),\n",
       " ('打兵', 6),\n",
       " ('戈尔', 6),\n",
       " ('婚姻家庭', 6),\n",
       " ('举证', 6),\n",
       " ('中学校长', 6),\n",
       " ('追根溯源', 6),\n",
       " ('纪工委', 6),\n",
       " ('赶回来', 6),\n",
       " ('最佳时机', 6),\n",
       " ('近况', 6),\n",
       " ('近条', 6),\n",
       " ('真诚地', 6),\n",
       " ('晴好', 6),\n",
       " ('医务室', 6),\n",
       " ('出工', 6),\n",
       " ('西格', 6),\n",
       " ('期求', 6),\n",
       " ('行车时间', 6),\n",
       " ('颜料', 6),\n",
       " ('彩塑', 6),\n",
       " ('爱情故事', 6),\n",
       " ('拟制', 6),\n",
       " ('网吧', 6),\n",
       " ('小册子', 6),\n",
       " ('深学', 6),\n",
       " ('手机卡', 6),\n",
       " ('万向', 6),\n",
       " ('挎包', 6),\n",
       " ('城南', 6),\n",
       " ('终点线', 6),\n",
       " ('拒绝执行', 6),\n",
       " ('华语', 6),\n",
       " ('捏造事实', 6),\n",
       " ('后能', 6),\n",
       " ('脑部', 6),\n",
       " ('住院病人', 6),\n",
       " ('重新启动', 6),\n",
       " ('一男一女', 6),\n",
       " ('弃船', 6),\n",
       " ('乳山', 6),\n",
       " ('分管领导', 6),\n",
       " ('外省', 6),\n",
       " ('民事纠纷', 6),\n",
       " ('每期', 6),\n",
       " ('工商局', 6),\n",
       " ('接待费', 6),\n",
       " ('数为', 6),\n",
       " ('财政拨款', 6),\n",
       " ('涉密', 6),\n",
       " ('中国农业银行', 6),\n",
       " ('特长生', 6),\n",
       " ('愧疚', 6),\n",
       " ('死后', 6),\n",
       " ('风筝会', 6),\n",
       " ('广汉', 6),\n",
       " ('飞行速度', 6),\n",
       " ('放慢', 6),\n",
       " ('惦记', 6),\n",
       " ('伤疤', 6),\n",
       " ('情侣', 6),\n",
       " ('右眼', 6),\n",
       " ('出站口', 6),\n",
       " ('一亿人次', 6),\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def get_stopwords():\n",
    "    stop_words = []\n",
    "    for file in os.listdir('./stopwords'):\n",
    "        with open('./stopwords/{}'.format(file),encoding='utf8') as f:\n",
    "            stop_words.extend(f.read().splitlines())\n",
    "    return set(stop_words)\n",
    "\n",
    "stop_words = get_stopwords()\n",
    "data = [word for word in data if word not in stop_words]\n",
    "\n",
    "counter = Counter(data)\n",
    "print(len(counter))\n",
    "counter.most_common()[-90000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''增加 <PAD> 和 <UNK> 标记'''\n",
    "words = [word[0] for word in counter.most_common(24630)]\n",
    "words.insert(0,'<PAD>')\n",
    "words.insert(1,'<UNK>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# word2idx saved.\n",
      "# idx2word saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "'''保存词表'''\n",
    "idx2word = dict(enumerate(words))\n",
    "word2idx = {word:idx for idx,word in enumerate(words)}\n",
    "\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "with open('./models/word2idx.pkl','wb') as f:\n",
    "    pickle.dump(word2idx,f)\n",
    "    print('# word2idx saved.')\n",
    "with open('./models/idx2word.pkl','wb') as f:\n",
    "    pickle.dump(idx2word,f)\n",
    "    print('# idx2word saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 token the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\liao\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.739 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 1 empty titles..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 确定seq_len\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calc_length(x):\n",
    "    '''观察长度的分布图像确定最佳的seq_max'''\n",
    "    if isinstance(x,str):\n",
    "        return len(x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.distplot(Data1.title.apply(clear_data).apply(lambda x : jieba.cut(x)).apply(lambda x:len(list(x))).sort_values(ascending=False),fit=norm)\n",
    "plt.xlabel('len(title)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.distplot(Data1.content.apply(clear_data).apply(lambda x : jieba.cut(x)).apply(lambda x:len(list(x))).sort_values(ascending=False),fit=norm)\n",
    "plt.xlabel('len(content)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 由上图可知，把contnet长度确定在800处 、title的长度在30处   比较合理的,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 1 empty titles..."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def padding(titles,contents,word2idx,seq_len_content=2000,seq_len_title=40):\n",
    "    '''对数据进行token及padding操作'''\n",
    "    input_x_titles = []\n",
    "    input_x_contents = []\n",
    "    \n",
    "    for title in titles:\n",
    "        tmp = [word2idx.get(word,word2idx['<UNK>']) for word in title]\n",
    "        sent_len = len(tmp)\n",
    "        if sent_len > seq_len_title:\n",
    "            tmp = tmp[:40]\n",
    "            \n",
    "        else:\n",
    "            tmp.extend([word2idx['<PAD>'] for _ in range(seq_len_title - sent_len)])\n",
    "        input_x_titles.append(tmp)\n",
    "        \n",
    "    for content in contents:\n",
    "        tmp = [word2idx.get(word,word2idx['<UNK>']) for word in content]\n",
    "        sent_len = len(tmp)\n",
    "        if sent_len > seq_len_content:\n",
    "            tmp = tmp[:seq_len_content]\n",
    "            \n",
    "        else:\n",
    "            tmp.extend([word2idx['<PAD>'] for _ in range(seq_len_content - sent_len)])\n",
    "        input_x_contents.append(tmp)\n",
    "\n",
    "    return np.concatenate([input_x_titles,input_x_contents],axis=-1)\n",
    "\n",
    "def token(titles,contents,word2idx):\n",
    "    '''数据的处理及padding'''\n",
    "    data_titles = titles.apply(clear_data).apply(lambda x : jieba.cut(x))\n",
    "    data_contents = contents.apply(clear_data).apply(lambda x: jieba.cut(x))\n",
    "    X = padding(data_titles,data_contents,word2idx)\n",
    "    \n",
    "    return X\n",
    "\n",
    "X = token(Data1['title'],Data1['content'],word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles...\r",
      "Here are 1 empty titles..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\liao\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.765 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 1 empty titles..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish build word_vec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_word2vec(data,n_dims=200,w2v_path='./models/w2v_model.pkl'):\n",
    "    '''进行word2vec模型的获取'''\n",
    "    if not os.path.exists(w2v_path):\n",
    "        print('\\n# Train the word2vec ')\n",
    "        core_count = cpu_count()\n",
    "        w2v = Word2Vec(size=n_dims,min_count=5,workers=core_count)\n",
    "        w2v.build_vocab(data)\n",
    "        w2v.train(data,total_examples=w2v.corpus_count,epochs=128)\n",
    "        w2v.save(w2v_path)\n",
    "        print('# Finish train the word2vec')\n",
    "    else:\n",
    "        w2v = Word2Vec.load(w2v_path)\n",
    "        \n",
    "    \n",
    "    return w2v\n",
    "\n",
    "def get_tf_idf(data,model_path='./models/tf_idf_model.pkl'):\n",
    "    '''进行tfidf模型的获取'''\n",
    "    if not os.path.exists(model_path):\n",
    "        tf_model = TfidfVectorizer(ngram_range=(1,2),token_pattern=r'(?u)\\b\\w+\\b\"')\n",
    "        tf_model.fit(data)\n",
    "        pickle.dump(tf_model,open(model_path,'wb'))\n",
    "        print('# Finish train the Tf-idf')\n",
    "    else:\n",
    "        tf_model = pickle.load(open(model_path,'rb'))\n",
    "    \n",
    "    return tf_model\n",
    "\n",
    "def build_tfidf_vec(sent,tfidf):\n",
    "    '''获取tfidf向量'''\n",
    "    return tfidf.transform([sent]).todense()[0]\n",
    "\n",
    "\n",
    "def build_word_vec(sent,w2v):\n",
    "    '''获取word2vec向量'''\n",
    "    vec = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec.append(w2v[word])\n",
    "        except:\n",
    "            vec.append(np.zeros(200))\n",
    "    if len(vec) != 0:\n",
    "        vec = np.mean(vec,axis=0)\n",
    "    else:\n",
    "        vec = np.zeros(200)\n",
    "    \n",
    "    return vec\n",
    "\n",
    "\n",
    "def convert2vec(data):    \n",
    "    '''把处理后的文本转换成向量'''\n",
    "    '''注意：再使用word2vec的时候split_data和使用tfidf的时候方法里面需要进行就该，split_data函数使用tfidf时加入代码\" \".join(data) '''\n",
    "    ## word2vec  \n",
    "    tmp1 = data_process('title',data).apply(list)\n",
    "    tmp2 = data_process('content',data).apply(list)\n",
    "    data.drop(['id','title','content'],axis=1,inplace=True)\n",
    "    w2v = get_word2vec(tmp1 + tmp2)\n",
    "\n",
    "    ## tf-idf   \n",
    "#     tmp1 = data_process('title',data)\n",
    "#     tmp2 = data_process('content',data)\n",
    "#     tfidf = get_tf_idf(tmp1 + tmp1)\n",
    "    \n",
    "    tmp_size = 200\n",
    "    for i in range(int(np.ceil(len(tmp1 + tmp2) / tmp_size))):\n",
    "        tmp1[i * tmp_size: i * tmp_size + tmp_size] = tmp1[i * tmp_size: i * tmp_size + tmp_size].apply(build_word_vec,args=(w2v,))\n",
    "        tmp2[i * tmp_size: i * tmp_size + tmp_size] = tmp2[i * tmp_size :i * tmp_size + tmp_size].apply(build_word_vec,args=(w2v,))\n",
    "        \n",
    "#         tmp1[i * tmp_size: i * tmp_size + tmp_size] = tmp1[i * tmp_size: i * tmp_size + tmp_size].apply(build_tfidf_vec,args=(tfidf,))\n",
    "#         tmp2[i * tmp_size: i * tmp_size + tmp_size] = tmp2[i * tmp_size :i * tmp_size + tmp_size].apply(build_tfidf_vec,args=(tfidf,))\n",
    "    \n",
    "#     data['allVec'] = list(np.concatenate([list(tmp1),list(tmp2)],axis=1))  # 这里转换成list再存进df\n",
    "    data['allVec'] = list(np.sum([list(tmp1),list(tmp2)],axis=0))  # 这里转换成list再存进df\n",
    "\n",
    "    sum_vec = data.groupby(['date']).sum().reset_index()\n",
    "    count_vec = data.groupby(['date']).count().reset_index()['allVec']\n",
    "    \n",
    "    sum_vec['allVec'] = sum_vec['allVec'] / count_vec\n",
    "    \n",
    "    print('# Finish build word_vec')\n",
    "    \n",
    "    return sum_vec\n",
    "\n",
    "new_data = convert2vec(Data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 合并一天对应一段时间10天内的数据(必要去除train_set开头与test_set的末尾一部分数据再做组合验证数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>allVec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-14</th>\n",
       "      <td>2014-04-14</td>\n",
       "      <td>[0.06317358290155729, 0.43262030531962714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-15</th>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>[-0.1878579319221899, 0.06973273736657576, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-16</th>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>[-0.07485291386644045, 0.07098943205481326, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-17</th>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>[-0.18737137507414445, -0.0876698462292552, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-18</th>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>[0.052207767883724075, -0.6337272375423639, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-19</th>\n",
       "      <td>2014-04-19</td>\n",
       "      <td>[0.3185145132361896, -0.4371863577574019, -0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-20</th>\n",
       "      <td>2014-04-20</td>\n",
       "      <td>[-0.1160324103475272, -0.49018933321093466, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-21</th>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>[0.21492725891225478, -0.2824603892424527, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-22</th>\n",
       "      <td>2014-04-22</td>\n",
       "      <td>[0.11055358188730993, -0.01651803932729223, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-23</th>\n",
       "      <td>2014-04-23</td>\n",
       "      <td>[0.2789467494668705, -0.20326825696974993, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-24</th>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>[0.2319015884869977, 0.40526491674153425, -0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-25</th>\n",
       "      <td>2014-04-25</td>\n",
       "      <td>[-0.21469083944698683, -0.22129782234385076, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-26</th>\n",
       "      <td>2014-04-26</td>\n",
       "      <td>[0.245105941840891, -0.47111576946363565, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-27</th>\n",
       "      <td>2014-04-27</td>\n",
       "      <td>[0.12406243034638464, -0.25139506079722196, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-28</th>\n",
       "      <td>2014-04-28</td>\n",
       "      <td>[0.08236550566028146, -0.3728565445900256, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-29</th>\n",
       "      <td>2014-04-29</td>\n",
       "      <td>[-0.20064894533833535, -0.3332128195772095, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>2014-04-30</td>\n",
       "      <td>[0.010440887036648664, 0.007233017547564073, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-01</th>\n",
       "      <td>2014-05-01</td>\n",
       "      <td>[-0.08968028926523403, -0.36635966882109644, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-02</th>\n",
       "      <td>2014-05-02</td>\n",
       "      <td>[0.16242664886845484, -0.6231746909519037, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-03</th>\n",
       "      <td>2014-05-03</td>\n",
       "      <td>[-0.10046919034077571, -0.5014296925793855, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-04</th>\n",
       "      <td>2014-05-04</td>\n",
       "      <td>[0.46871524144496235, -0.25603435511168626, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-05</th>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>[0.03161852844793068, -0.16087545233704742, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-06</th>\n",
       "      <td>2014-05-06</td>\n",
       "      <td>[-0.18779879225486962, 0.40200529468280294, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-07</th>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>[-0.0033269312349148094, 0.26320724195490286, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-08</th>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>[-0.0116067118629688, 0.07190808885488188, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-09</th>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>[0.3222431540489197, -0.18174753757193685, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-10</th>\n",
       "      <td>2014-05-10</td>\n",
       "      <td>[-0.4086331130388905, 0.03841691363383742, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-11</th>\n",
       "      <td>2014-05-11</td>\n",
       "      <td>[-0.22461932771524093, -0.017055863164421314, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-12</th>\n",
       "      <td>2014-05-12</td>\n",
       "      <td>[-0.009486113353209065, -0.08427692524750124, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-13</th>\n",
       "      <td>2014-05-13</td>\n",
       "      <td>[0.08071086666028171, 0.30901424428888824, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>2019-03-03</td>\n",
       "      <td>[-0.2020006664097309, -0.11363947745412588, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>[-0.03988091216275567, -0.11202058627417213, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>[-0.20344245977818018, 0.7372373402828262, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>[-0.2465901678078808, -0.09947523940354586, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>[-0.7913894275617268, 1.004884098139074, 1.398...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>[-0.47105524020598216, 0.3232247809715131, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>2019-03-09</td>\n",
       "      <td>[0.15557727054692805, 0.42383681843057275, 1.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>[0.13728358570693266, -0.12430853696746963, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>[0.34189128239328664, -0.17620940615112582, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>[-0.09236960698451314, 0.10829498880498466, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>[-0.06382871922865815, 0.05267816135773854, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>[-0.054967357660643756, -0.45509158493950963, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>[0.046129180578624494, 0.10269019399203506, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>2019-03-16</td>\n",
       "      <td>[0.031253376327179096, 0.11078477711589248, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>[0.17994708114561916, -0.3953890476516691, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>[-0.3490238786839387, -0.529447498238262, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>[0.011133174101511637, -0.003462909658749898, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>[-0.4041741090662339, -0.29064978702979927, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>[0.11162909699810873, -0.09217680143537345, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>[0.004055563100230168, -0.10558473274988286, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>2019-03-23</td>\n",
       "      <td>[-0.27142128306219265, 0.35459551073255996, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>[-0.28013465725458586, 0.45258940297823685, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>[-0.5653454885400253, 0.3382759407573475, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>[0.13403566082318624, 0.10535221497217814, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>[-0.1512569575325439, 0.48190362571755, -0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>[0.1946971543236739, 0.005427229735586379, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>[0.06450071040954855, -0.1656143127511891, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>[0.24856407248548099, -0.23959227601940533, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>[0.13668648449656293, -0.07084936810991702, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>[0.27238404506645286, 0.2541276776709226, -0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1814 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                             allVec\n",
       "date                                                                    \n",
       "2014-04-14 2014-04-14  [0.06317358290155729, 0.43262030531962714, -0....\n",
       "2014-04-15 2014-04-15  [-0.1878579319221899, 0.06973273736657576, -0....\n",
       "2014-04-16 2014-04-16  [-0.07485291386644045, 0.07098943205481326, -0...\n",
       "2014-04-17 2014-04-17  [-0.18737137507414445, -0.0876698462292552, -0...\n",
       "2014-04-18 2014-04-18  [0.052207767883724075, -0.6337272375423639, -0...\n",
       "2014-04-19 2014-04-19  [0.3185145132361896, -0.4371863577574019, -0.6...\n",
       "2014-04-20 2014-04-20  [-0.1160324103475272, -0.49018933321093466, -0...\n",
       "2014-04-21 2014-04-21  [0.21492725891225478, -0.2824603892424527, -0....\n",
       "2014-04-22 2014-04-22  [0.11055358188730993, -0.01651803932729223, -0...\n",
       "2014-04-23 2014-04-23  [0.2789467494668705, -0.20326825696974993, -0....\n",
       "2014-04-24 2014-04-24  [0.2319015884869977, 0.40526491674153425, -0.4...\n",
       "2014-04-25 2014-04-25  [-0.21469083944698683, -0.22129782234385076, -...\n",
       "2014-04-26 2014-04-26  [0.245105941840891, -0.47111576946363565, -0.0...\n",
       "2014-04-27 2014-04-27  [0.12406243034638464, -0.25139506079722196, -0...\n",
       "2014-04-28 2014-04-28  [0.08236550566028146, -0.3728565445900256, -0....\n",
       "2014-04-29 2014-04-29  [-0.20064894533833535, -0.3332128195772095, -0...\n",
       "2014-04-30 2014-04-30  [0.010440887036648664, 0.007233017547564073, -...\n",
       "2014-05-01 2014-05-01  [-0.08968028926523403, -0.36635966882109644, 0...\n",
       "2014-05-02 2014-05-02  [0.16242664886845484, -0.6231746909519037, -0....\n",
       "2014-05-03 2014-05-03  [-0.10046919034077571, -0.5014296925793855, -0...\n",
       "2014-05-04 2014-05-04  [0.46871524144496235, -0.25603435511168626, -0...\n",
       "2014-05-05 2014-05-05  [0.03161852844793068, -0.16087545233704742, -0...\n",
       "2014-05-06 2014-05-06  [-0.18779879225486962, 0.40200529468280294, -0...\n",
       "2014-05-07 2014-05-07  [-0.0033269312349148094, 0.26320724195490286, ...\n",
       "2014-05-08 2014-05-08  [-0.0116067118629688, 0.07190808885488188, -0....\n",
       "2014-05-09 2014-05-09  [0.3222431540489197, -0.18174753757193685, 0.0...\n",
       "2014-05-10 2014-05-10  [-0.4086331130388905, 0.03841691363383742, -0....\n",
       "2014-05-11 2014-05-11  [-0.22461932771524093, -0.017055863164421314, ...\n",
       "2014-05-12 2014-05-12  [-0.009486113353209065, -0.08427692524750124, ...\n",
       "2014-05-13 2014-05-13  [0.08071086666028171, 0.30901424428888824, -0....\n",
       "...               ...                                                ...\n",
       "2019-03-03 2019-03-03  [-0.2020006664097309, -0.11363947745412588, 0....\n",
       "2019-03-04 2019-03-04  [-0.03988091216275567, -0.11202058627417213, 0...\n",
       "2019-03-05 2019-03-05  [-0.20344245977818018, 0.7372373402828262, 0.6...\n",
       "2019-03-06 2019-03-06  [-0.2465901678078808, -0.09947523940354586, 0....\n",
       "2019-03-07 2019-03-07  [-0.7913894275617268, 1.004884098139074, 1.398...\n",
       "2019-03-08 2019-03-08  [-0.47105524020598216, 0.3232247809715131, 0.8...\n",
       "2019-03-09 2019-03-09  [0.15557727054692805, 0.42383681843057275, 1.2...\n",
       "2019-03-10 2019-03-10  [0.13728358570693266, -0.12430853696746963, 0....\n",
       "2019-03-11 2019-03-11  [0.34189128239328664, -0.17620940615112582, 0....\n",
       "2019-03-12 2019-03-12  [-0.09236960698451314, 0.10829498880498466, 0....\n",
       "2019-03-13 2019-03-13  [-0.06382871922865815, 0.05267816135773854, 0....\n",
       "2019-03-14 2019-03-14  [-0.054967357660643756, -0.45509158493950963, ...\n",
       "2019-03-15 2019-03-15  [0.046129180578624494, 0.10269019399203506, 0....\n",
       "2019-03-16 2019-03-16  [0.031253376327179096, 0.11078477711589248, 0....\n",
       "2019-03-17 2019-03-17  [0.17994708114561916, -0.3953890476516691, -0....\n",
       "2019-03-18 2019-03-18  [-0.3490238786839387, -0.529447498238262, 0.04...\n",
       "2019-03-19 2019-03-19  [0.011133174101511637, -0.003462909658749898, ...\n",
       "2019-03-20 2019-03-20  [-0.4041741090662339, -0.29064978702979927, 0....\n",
       "2019-03-21 2019-03-21  [0.11162909699810873, -0.09217680143537345, 0....\n",
       "2019-03-22 2019-03-22  [0.004055563100230168, -0.10558473274988286, -...\n",
       "2019-03-23 2019-03-23  [-0.27142128306219265, 0.35459551073255996, -0...\n",
       "2019-03-24 2019-03-24  [-0.28013465725458586, 0.45258940297823685, -0...\n",
       "2019-03-25 2019-03-25  [-0.5653454885400253, 0.3382759407573475, -0.1...\n",
       "2019-03-26 2019-03-26  [0.13403566082318624, 0.10535221497217814, -0....\n",
       "2019-03-27 2019-03-27  [-0.1512569575325439, 0.48190362571755, -0.033...\n",
       "2019-03-28 2019-03-28  [0.1946971543236739, 0.005427229735586379, -0....\n",
       "2019-03-29 2019-03-29  [0.06450071040954855, -0.1656143127511891, -0....\n",
       "2019-03-30 2019-03-30  [0.24856407248548099, -0.23959227601940533, -0...\n",
       "2019-03-31 2019-03-31  [0.13668648449656293, -0.07084936810991702, 0....\n",
       "2019-04-01 2019-04-01  [0.27238404506645286, 0.2541276776709226, -0.1...\n",
       "\n",
       "[1814 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Data2.y = Data2.y.astype(str)\n",
    "Data2.trade_date = Data2.trade_date.astype(str)\n",
    "\n",
    "Data = Data2.loc[:,['name','y','trade_date']].groupby('trade_date').sum()['y'].reset_index()\n",
    "Data.set_index(Data.trade_date,inplace=True)\n",
    "\n",
    "new_data['date'] = pd.to_datetime(new_data['date'],format='%Y%m%d')\n",
    "new_data.set_index(new_data['date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#  把对应的过去一段时间内10天的数据合并进第一个表\n",
    "Data['allVec'] = None\n",
    "\n",
    "for date in Data.index:\n",
    "    temp_data = new_data.truncate(after=date)[-20:-10]\n",
    "    Data.set_value(date,'allVec',temp_data.allVec.mean())\n",
    "\n",
    "# 去除不完整数据\n",
    "Data = Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>allVec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-14</th>\n",
       "      <td>2014-04-14</td>\n",
       "      <td>[0.06317358290155729, 0.43262030531962714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-15</th>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>[-0.1878579319221899, 0.06973273736657576, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-16</th>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>[-0.07485291386644045, 0.07098943205481326, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-17</th>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>[-0.18737137507414445, -0.0876698462292552, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-18</th>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>[0.052207767883724075, -0.6337272375423639, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-19</th>\n",
       "      <td>2014-04-19</td>\n",
       "      <td>[0.3185145132361896, -0.4371863577574019, -0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-20</th>\n",
       "      <td>2014-04-20</td>\n",
       "      <td>[-0.1160324103475272, -0.49018933321093466, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-21</th>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>[0.21492725891225478, -0.2824603892424527, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-22</th>\n",
       "      <td>2014-04-22</td>\n",
       "      <td>[0.11055358188730993, -0.01651803932729223, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-23</th>\n",
       "      <td>2014-04-23</td>\n",
       "      <td>[0.2789467494668705, -0.20326825696974993, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-24</th>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>[0.2319015884869977, 0.40526491674153425, -0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-25</th>\n",
       "      <td>2014-04-25</td>\n",
       "      <td>[-0.21469083944698683, -0.22129782234385076, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-26</th>\n",
       "      <td>2014-04-26</td>\n",
       "      <td>[0.245105941840891, -0.47111576946363565, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-27</th>\n",
       "      <td>2014-04-27</td>\n",
       "      <td>[0.12406243034638464, -0.25139506079722196, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-28</th>\n",
       "      <td>2014-04-28</td>\n",
       "      <td>[0.08236550566028146, -0.3728565445900256, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-29</th>\n",
       "      <td>2014-04-29</td>\n",
       "      <td>[-0.20064894533833535, -0.3332128195772095, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>2014-04-30</td>\n",
       "      <td>[0.010440887036648664, 0.007233017547564073, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-01</th>\n",
       "      <td>2014-05-01</td>\n",
       "      <td>[-0.08968028926523403, -0.36635966882109644, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-02</th>\n",
       "      <td>2014-05-02</td>\n",
       "      <td>[0.16242664886845484, -0.6231746909519037, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-03</th>\n",
       "      <td>2014-05-03</td>\n",
       "      <td>[-0.10046919034077571, -0.5014296925793855, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-04</th>\n",
       "      <td>2014-05-04</td>\n",
       "      <td>[0.46871524144496235, -0.25603435511168626, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-05</th>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>[0.03161852844793068, -0.16087545233704742, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-06</th>\n",
       "      <td>2014-05-06</td>\n",
       "      <td>[-0.18779879225486962, 0.40200529468280294, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-07</th>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>[-0.0033269312349148094, 0.26320724195490286, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-08</th>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>[-0.0116067118629688, 0.07190808885488188, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-09</th>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>[0.3222431540489197, -0.18174753757193685, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-10</th>\n",
       "      <td>2014-05-10</td>\n",
       "      <td>[-0.4086331130388905, 0.03841691363383742, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-11</th>\n",
       "      <td>2014-05-11</td>\n",
       "      <td>[-0.22461932771524093, -0.017055863164421314, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-12</th>\n",
       "      <td>2014-05-12</td>\n",
       "      <td>[-0.009486113353209065, -0.08427692524750124, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-13</th>\n",
       "      <td>2014-05-13</td>\n",
       "      <td>[0.08071086666028171, 0.30901424428888824, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>2019-03-03</td>\n",
       "      <td>[-0.2020006664097309, -0.11363947745412588, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>[-0.03988091216275567, -0.11202058627417213, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>[-0.20344245977818018, 0.7372373402828262, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>[-0.2465901678078808, -0.09947523940354586, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>[-0.7913894275617268, 1.004884098139074, 1.398...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>[-0.47105524020598216, 0.3232247809715131, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>2019-03-09</td>\n",
       "      <td>[0.15557727054692805, 0.42383681843057275, 1.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>[0.13728358570693266, -0.12430853696746963, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>[0.34189128239328664, -0.17620940615112582, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>[-0.09236960698451314, 0.10829498880498466, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>[-0.06382871922865815, 0.05267816135773854, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>2019-03-14</td>\n",
       "      <td>[-0.054967357660643756, -0.45509158493950963, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>2019-03-15</td>\n",
       "      <td>[0.046129180578624494, 0.10269019399203506, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>2019-03-16</td>\n",
       "      <td>[0.031253376327179096, 0.11078477711589248, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>2019-03-17</td>\n",
       "      <td>[0.17994708114561916, -0.3953890476516691, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>2019-03-18</td>\n",
       "      <td>[-0.3490238786839387, -0.529447498238262, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>2019-03-19</td>\n",
       "      <td>[0.011133174101511637, -0.003462909658749898, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>[-0.4041741090662339, -0.29064978702979927, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>[0.11162909699810873, -0.09217680143537345, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>[0.004055563100230168, -0.10558473274988286, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>2019-03-23</td>\n",
       "      <td>[-0.27142128306219265, 0.35459551073255996, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>[-0.28013465725458586, 0.45258940297823685, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>[-0.5653454885400253, 0.3382759407573475, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>[0.13403566082318624, 0.10535221497217814, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>[-0.1512569575325439, 0.48190362571755, -0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>[0.1946971543236739, 0.005427229735586379, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>[0.06450071040954855, -0.1656143127511891, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>[0.24856407248548099, -0.23959227601940533, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>[0.13668648449656293, -0.07084936810991702, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>[0.27238404506645286, 0.2541276776709226, -0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1814 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                             allVec\n",
       "date                                                                    \n",
       "2014-04-14 2014-04-14  [0.06317358290155729, 0.43262030531962714, -0....\n",
       "2014-04-15 2014-04-15  [-0.1878579319221899, 0.06973273736657576, -0....\n",
       "2014-04-16 2014-04-16  [-0.07485291386644045, 0.07098943205481326, -0...\n",
       "2014-04-17 2014-04-17  [-0.18737137507414445, -0.0876698462292552, -0...\n",
       "2014-04-18 2014-04-18  [0.052207767883724075, -0.6337272375423639, -0...\n",
       "2014-04-19 2014-04-19  [0.3185145132361896, -0.4371863577574019, -0.6...\n",
       "2014-04-20 2014-04-20  [-0.1160324103475272, -0.49018933321093466, -0...\n",
       "2014-04-21 2014-04-21  [0.21492725891225478, -0.2824603892424527, -0....\n",
       "2014-04-22 2014-04-22  [0.11055358188730993, -0.01651803932729223, -0...\n",
       "2014-04-23 2014-04-23  [0.2789467494668705, -0.20326825696974993, -0....\n",
       "2014-04-24 2014-04-24  [0.2319015884869977, 0.40526491674153425, -0.4...\n",
       "2014-04-25 2014-04-25  [-0.21469083944698683, -0.22129782234385076, -...\n",
       "2014-04-26 2014-04-26  [0.245105941840891, -0.47111576946363565, -0.0...\n",
       "2014-04-27 2014-04-27  [0.12406243034638464, -0.25139506079722196, -0...\n",
       "2014-04-28 2014-04-28  [0.08236550566028146, -0.3728565445900256, -0....\n",
       "2014-04-29 2014-04-29  [-0.20064894533833535, -0.3332128195772095, -0...\n",
       "2014-04-30 2014-04-30  [0.010440887036648664, 0.007233017547564073, -...\n",
       "2014-05-01 2014-05-01  [-0.08968028926523403, -0.36635966882109644, 0...\n",
       "2014-05-02 2014-05-02  [0.16242664886845484, -0.6231746909519037, -0....\n",
       "2014-05-03 2014-05-03  [-0.10046919034077571, -0.5014296925793855, -0...\n",
       "2014-05-04 2014-05-04  [0.46871524144496235, -0.25603435511168626, -0...\n",
       "2014-05-05 2014-05-05  [0.03161852844793068, -0.16087545233704742, -0...\n",
       "2014-05-06 2014-05-06  [-0.18779879225486962, 0.40200529468280294, -0...\n",
       "2014-05-07 2014-05-07  [-0.0033269312349148094, 0.26320724195490286, ...\n",
       "2014-05-08 2014-05-08  [-0.0116067118629688, 0.07190808885488188, -0....\n",
       "2014-05-09 2014-05-09  [0.3222431540489197, -0.18174753757193685, 0.0...\n",
       "2014-05-10 2014-05-10  [-0.4086331130388905, 0.03841691363383742, -0....\n",
       "2014-05-11 2014-05-11  [-0.22461932771524093, -0.017055863164421314, ...\n",
       "2014-05-12 2014-05-12  [-0.009486113353209065, -0.08427692524750124, ...\n",
       "2014-05-13 2014-05-13  [0.08071086666028171, 0.30901424428888824, -0....\n",
       "...               ...                                                ...\n",
       "2019-03-03 2019-03-03  [-0.2020006664097309, -0.11363947745412588, 0....\n",
       "2019-03-04 2019-03-04  [-0.03988091216275567, -0.11202058627417213, 0...\n",
       "2019-03-05 2019-03-05  [-0.20344245977818018, 0.7372373402828262, 0.6...\n",
       "2019-03-06 2019-03-06  [-0.2465901678078808, -0.09947523940354586, 0....\n",
       "2019-03-07 2019-03-07  [-0.7913894275617268, 1.004884098139074, 1.398...\n",
       "2019-03-08 2019-03-08  [-0.47105524020598216, 0.3232247809715131, 0.8...\n",
       "2019-03-09 2019-03-09  [0.15557727054692805, 0.42383681843057275, 1.2...\n",
       "2019-03-10 2019-03-10  [0.13728358570693266, -0.12430853696746963, 0....\n",
       "2019-03-11 2019-03-11  [0.34189128239328664, -0.17620940615112582, 0....\n",
       "2019-03-12 2019-03-12  [-0.09236960698451314, 0.10829498880498466, 0....\n",
       "2019-03-13 2019-03-13  [-0.06382871922865815, 0.05267816135773854, 0....\n",
       "2019-03-14 2019-03-14  [-0.054967357660643756, -0.45509158493950963, ...\n",
       "2019-03-15 2019-03-15  [0.046129180578624494, 0.10269019399203506, 0....\n",
       "2019-03-16 2019-03-16  [0.031253376327179096, 0.11078477711589248, 0....\n",
       "2019-03-17 2019-03-17  [0.17994708114561916, -0.3953890476516691, -0....\n",
       "2019-03-18 2019-03-18  [-0.3490238786839387, -0.529447498238262, 0.04...\n",
       "2019-03-19 2019-03-19  [0.011133174101511637, -0.003462909658749898, ...\n",
       "2019-03-20 2019-03-20  [-0.4041741090662339, -0.29064978702979927, 0....\n",
       "2019-03-21 2019-03-21  [0.11162909699810873, -0.09217680143537345, 0....\n",
       "2019-03-22 2019-03-22  [0.004055563100230168, -0.10558473274988286, -...\n",
       "2019-03-23 2019-03-23  [-0.27142128306219265, 0.35459551073255996, -0...\n",
       "2019-03-24 2019-03-24  [-0.28013465725458586, 0.45258940297823685, -0...\n",
       "2019-03-25 2019-03-25  [-0.5653454885400253, 0.3382759407573475, -0.1...\n",
       "2019-03-26 2019-03-26  [0.13403566082318624, 0.10535221497217814, -0....\n",
       "2019-03-27 2019-03-27  [-0.1512569575325439, 0.48190362571755, -0.033...\n",
       "2019-03-28 2019-03-28  [0.1946971543236739, 0.005427229735586379, -0....\n",
       "2019-03-29 2019-03-29  [0.06450071040954855, -0.1656143127511891, -0....\n",
       "2019-03-30 2019-03-30  [0.24856407248548099, -0.23959227601940533, -0...\n",
       "2019-03-31 2019-03-31  [0.13668648449656293, -0.07084936810991702, 0....\n",
       "2019-04-01 2019-04-01  [0.27238404506645286, 0.2541276776709226, -0.1...\n",
       "\n",
       "[1814 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_date</th>\n",
       "      <th>y</th>\n",
       "      <th>allVec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trade_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20140424</th>\n",
       "      <td>20140424</td>\n",
       "      <td>0100000000000100000000000000001000</td>\n",
       "      <td>[0.06317358290155729, 0.43262030531962714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140425</th>\n",
       "      <td>20140425</td>\n",
       "      <td>0000000000000000000000000000010000</td>\n",
       "      <td>[-0.06234217451031631, 0.2511765213431014, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140428</th>\n",
       "      <td>20140428</td>\n",
       "      <td>0000000000000000000000000000000000</td>\n",
       "      <td>[-0.0669401740154987, -0.029610921806120617, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140429</th>\n",
       "      <td>20140429</td>\n",
       "      <td>1111111111111011111111111111111111</td>\n",
       "      <td>[-0.0026977261402173123, -0.09754016113133417,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140430</th>\n",
       "      <td>20140430</td>\n",
       "      <td>1011010111111011111111111111100111</td>\n",
       "      <td>[-0.018888395312690154, -0.1536328999998485, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           trade_date                                   y  \\\n",
       "trade_date                                                  \n",
       "20140424     20140424  0100000000000100000000000000001000   \n",
       "20140425     20140425  0000000000000000000000000000010000   \n",
       "20140428     20140428  0000000000000000000000000000000000   \n",
       "20140429     20140429  1111111111111011111111111111111111   \n",
       "20140430     20140430  1011010111111011111111111111100111   \n",
       "\n",
       "                                                       allVec  \n",
       "trade_date                                                     \n",
       "20140424    [0.06317358290155729, 0.43262030531962714, -0....  \n",
       "20140425    [-0.06234217451031631, 0.2511765213431014, -0....  \n",
       "20140428    [-0.0669401740154987, -0.029610921806120617, -...  \n",
       "20140429    [-0.0026977261402173123, -0.09754016113133417,...  \n",
       "20140430    [-0.018888395312690154, -0.1536328999998485, -...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_y_1(y):\n",
    "    '''处理y值成格式1'''\n",
    "    def map_f(x):\n",
    "        if x == '1':\n",
    "            return [1,0]\n",
    "        else:\n",
    "            return [0,1]\n",
    "    return list(map(map_f,y))\n",
    "    \n",
    "def fix_y_2(y):\n",
    "    '''处理y值成格式2'''\n",
    "    labels = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1':\n",
    "            labels.append(i)\n",
    "    return labels\n",
    "\n",
    "Data['y_'] = Data['y'].apply(lambda x: list(map(int,x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对数据进行适当的排版\n",
    "2. word2vec向量化\n",
    "3. 过去5天的word2vec合并的到向量\n",
    "4. 使用到过去5天部分的stack数据\n",
    "5. 构建LSTM网络进行预测\n",
    "6. softmax进行多分类的概率输出，使用cross_entropy进行多分类的概率输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### val_data 数据集的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_date = ['20190402','20190403','20190404','20190408','20190409']\n",
    "def set_val_data(date_list,new_data):\n",
    "    val_data = pd.DataFrame(columns=['vec'],index=pre_date)\n",
    "\n",
    "    for date in date_list:\n",
    "        temp_data = new_data.truncate(before=20190402)[-20:-10]\n",
    "        val_data.set_value(date,'vec',temp_data.allVec.mean())\n",
    "    \n",
    "    return val_data\n",
    "\n",
    "val_data = []\n",
    "for i in [1,2,3,7,8]:\n",
    "    val_data.append(list(new_data.allVec[-20 + i: -10 + i].mean()))\n",
    "val_data = pd.DataFrame(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.075507</td>\n",
       "      <td>-0.320794</td>\n",
       "      <td>0.276342</td>\n",
       "      <td>-0.329421</td>\n",
       "      <td>0.424764</td>\n",
       "      <td>0.600995</td>\n",
       "      <td>0.306627</td>\n",
       "      <td>-0.165874</td>\n",
       "      <td>-0.399119</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290867</td>\n",
       "      <td>-0.295676</td>\n",
       "      <td>-0.305748</td>\n",
       "      <td>0.311399</td>\n",
       "      <td>-0.109803</td>\n",
       "      <td>0.298839</td>\n",
       "      <td>1.260910</td>\n",
       "      <td>-0.043404</td>\n",
       "      <td>0.059408</td>\n",
       "      <td>-0.907341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052452</td>\n",
       "      <td>-0.188885</td>\n",
       "      <td>0.153269</td>\n",
       "      <td>-0.363884</td>\n",
       "      <td>0.396031</td>\n",
       "      <td>0.535677</td>\n",
       "      <td>0.285108</td>\n",
       "      <td>-0.083379</td>\n",
       "      <td>-0.267617</td>\n",
       "      <td>-0.117902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274767</td>\n",
       "      <td>-0.290852</td>\n",
       "      <td>-0.217000</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>-0.041643</td>\n",
       "      <td>0.397983</td>\n",
       "      <td>1.349095</td>\n",
       "      <td>-0.145328</td>\n",
       "      <td>0.148178</td>\n",
       "      <td>-0.903176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.027414</td>\n",
       "      <td>-0.136584</td>\n",
       "      <td>0.085760</td>\n",
       "      <td>-0.407716</td>\n",
       "      <td>0.353459</td>\n",
       "      <td>0.536137</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>-0.065369</td>\n",
       "      <td>-0.175194</td>\n",
       "      <td>-0.097996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300956</td>\n",
       "      <td>-0.268377</td>\n",
       "      <td>-0.244627</td>\n",
       "      <td>0.246943</td>\n",
       "      <td>0.052092</td>\n",
       "      <td>0.460955</td>\n",
       "      <td>1.336976</td>\n",
       "      <td>-0.127951</td>\n",
       "      <td>0.195492</td>\n",
       "      <td>-0.928511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.054269</td>\n",
       "      <td>0.015327</td>\n",
       "      <td>-0.495508</td>\n",
       "      <td>0.221508</td>\n",
       "      <td>0.586908</td>\n",
       "      <td>0.343249</td>\n",
       "      <td>-0.035499</td>\n",
       "      <td>-0.073718</td>\n",
       "      <td>-0.135872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470499</td>\n",
       "      <td>-0.189278</td>\n",
       "      <td>-0.285506</td>\n",
       "      <td>0.289240</td>\n",
       "      <td>0.187289</td>\n",
       "      <td>0.329051</td>\n",
       "      <td>1.466203</td>\n",
       "      <td>-0.280283</td>\n",
       "      <td>0.302277</td>\n",
       "      <td>-0.988963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.101556</td>\n",
       "      <td>0.049912</td>\n",
       "      <td>-0.091650</td>\n",
       "      <td>-0.364659</td>\n",
       "      <td>0.118718</td>\n",
       "      <td>0.541568</td>\n",
       "      <td>0.252173</td>\n",
       "      <td>-0.083627</td>\n",
       "      <td>-0.055401</td>\n",
       "      <td>-0.137156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>-0.154338</td>\n",
       "      <td>-0.281979</td>\n",
       "      <td>0.243603</td>\n",
       "      <td>0.031188</td>\n",
       "      <td>0.216902</td>\n",
       "      <td>1.467225</td>\n",
       "      <td>-0.176486</td>\n",
       "      <td>0.336783</td>\n",
       "      <td>-0.936390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.075507 -0.320794  0.276342 -0.329421  0.424764  0.600995  0.306627   \n",
       "1  0.052452 -0.188885  0.153269 -0.363884  0.396031  0.535677  0.285108   \n",
       "2 -0.027414 -0.136584  0.085760 -0.407716  0.353459  0.536137  0.263683   \n",
       "3  0.018558  0.054269  0.015327 -0.495508  0.221508  0.586908  0.343249   \n",
       "4  0.101556  0.049912 -0.091650 -0.364659  0.118718  0.541568  0.252173   \n",
       "\n",
       "        7         8         9      ...          190       191       192  \\\n",
       "0 -0.165874 -0.399119  0.012200    ...     0.290867 -0.295676 -0.305748   \n",
       "1 -0.083379 -0.267617 -0.117902    ...     0.274767 -0.290852 -0.217000   \n",
       "2 -0.065369 -0.175194 -0.097996    ...     0.300956 -0.268377 -0.244627   \n",
       "3 -0.035499 -0.073718 -0.135872    ...     0.470499 -0.189278 -0.285506   \n",
       "4 -0.083627 -0.055401 -0.137156    ...     0.464896 -0.154338 -0.281979   \n",
       "\n",
       "        193       194       195       196       197       198       199  \n",
       "0  0.311399 -0.109803  0.298839  1.260910 -0.043404  0.059408 -0.907341  \n",
       "1  0.262200 -0.041643  0.397983  1.349095 -0.145328  0.148178 -0.903176  \n",
       "2  0.246943  0.052092  0.460955  1.336976 -0.127951  0.195492 -0.928511  \n",
       "3  0.289240  0.187289  0.329051  1.466203 -0.280283  0.302277 -0.988963  \n",
       "4  0.243603  0.031188  0.216902  1.467225 -0.176486  0.336783 -0.936390  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 机器学习模型尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = np.array(list(Data['allVec']))\n",
    "input_y = np.array(list(Data['y_']))[:,0]\n",
    "input_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV,LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_x,input_y,test_size=0.2,train_size=0.8,random_state=128)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=[0.001], class_weight=None, cv=10, dual=True,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=None, penalty='l2', random_state=None,\n",
       "           refit=True, scoring='roc_auc', solver='liblinear', tol=0.0001,\n",
       "           verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs = np.logspace(-7,-7,20)\n",
    "# Cs = np.linspace(0.0012,0.0008,10)\n",
    "Cs = [0.001]\n",
    "\n",
    "model_lr = LogisticRegressionCV(Cs,penalty='l2',scoring='roc_auc',dual=True,solver='liblinear',multi_class='ovr',cv=10)\n",
    "model_lr.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n0:0.0069\\n1：0.001\\n2:0.0001\\n3:0.0008\\n4:0.001\\n5:0.0004\\n6:0.01\\n7:1e-7\\n8:1e-6\\n9:0.001\\n10:1e-7\\n11:1e-7\\n12:1e-7\\n13:0.0001\\n14:0.0008\\n15:0.001\\n16:0.001\\n17:1e-7\\n18:1e-8\\n19:0.001\\n20:\\n21:1e-4\\n22:1e-8\\n23:1e-8\\n24:1e-8\\n25:0.001\\n26:0.001\\n27:0.001\\n28:0.001\\n29:183.298\\n30:\\n31:0.001\\n32:0.001\\n33:0.001\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len_cs = len(Cs)\n",
    "score = np.mean(model_lr.scores_[1],axis=0)\n",
    "\n",
    "plt.title('best C:{}'.format(model_lr.C_))\n",
    "plt.plot(Cs,score)\n",
    "plt.show()\n",
    " \n",
    "'''\n",
    "0:0.0069\n",
    "1：0.001\n",
    "2:0.0001\n",
    "3:0.0008\n",
    "4:0.001\n",
    "5:0.0004\n",
    "6:0.01\n",
    "7:1e-7\n",
    "8:1e-6\n",
    "9:0.001\n",
    "10:1e-7\n",
    "11:1e-7\n",
    "12:1e-7\n",
    "13:0.0001\n",
    "14:0.0008\n",
    "15:0.001\n",
    "16:0.001\n",
    "17:1e-7\n",
    "18:1e-8\n",
    "19:0.001\n",
    "20:\n",
    "21:1e-4\n",
    "22:1e-8\n",
    "23:1e-8\n",
    "24:1e-8\n",
    "25:0.001\n",
    "26:0.001\n",
    "27:0.001\n",
    "28:0.001\n",
    "29:183.298\n",
    "30:\n",
    "31:0.001\n",
    "32:0.001\n",
    "33:0.001\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (908, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-25bc3a1b4c7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# print('# accuracy score in train : {}'.format(accuracy_score(y_train_pred,y_train)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# print('# accuracy score in test : {}'.format(accuracy_score(y_test_pred,y_test)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'# auc score in train : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'# auc score in test : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    354\u001b[0m     return _average_binary_score(\n\u001b[0;32m    355\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\metrics\\base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[1;32m--> 328\u001b[1;33m                                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \"\"\"\n\u001b[0;32m    617\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 618\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (908, 2)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "\n",
    "y_train_pred = model_lr.predict_proba(x_train)\n",
    "y_test_pred = model_lr.predict_proba(x_test)\n",
    "\n",
    "# print('# accuracy score in train : {}'.format(accuracy_score(y_train_pred,y_train)))\n",
    "# print('# accuracy score in test : {}'.format(accuracy_score(y_test_pred,y_test)))\n",
    "print('# auc score in train : {}'.format(roc_auc_score(y_train,y_train_pred)))\n",
    "print('# auc score in test : {}'.format(roc_auc_score(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 发现使用词向量的训练得到的结果很不好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i ,ts in enumerate(Data2.ts_code.unique()):\n",
    "    input_x = np.array(list(Data['allVec']))\n",
    "    input_y = np.array(list(Data['y_']))[:,i]\n",
    "    ss = StandardScaler()\n",
    "    x_train = ss.fit_transform(x_train)\n",
    "    x_test = ss.transform(x_test)\n",
    "\n",
    "    model = LogisticRegression(C=0.006)\n",
    "    model.fit(input_x,input_y)\n",
    "    pred = model.predict_proba(val_data)\n",
    "\n",
    "    for _date,_pred in zip(pre_date,pred):\n",
    "        result.append([ts,_date,_pred[1]])\n",
    "result = pd.DataFrame(result,columns=['ts_code','trade_date','p'])\n",
    "result.to_csv('result2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for param in Data2.ts_code.unique():\n",
    "    model = pickle.load(open('./models/{}_1.pkl'.format(param),'rb'))\n",
    "    pre = model.predict_proba(val_data)\n",
    "    \n",
    "    for _ in zip(pre_date,pre):\n",
    "        result.append([param, _[0],_[1][0]])\n",
    "result = pd.DataFrame(result,columns=['ts_code','trade_date','p'])\n",
    "result.to_csv('result2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "result = []\n",
    "for i ,ts in enumerate(Data2.ts_code.unique()):\n",
    "    input_x = np.array(list(Data['allVec']))\n",
    "    input_y = np.array(list(Data['y_']))[:,i]\n",
    "    ss = StandardScaler()\n",
    "    x_train = ss.fit_transform(x_train)\n",
    "    x_test = ss.transform(x_test)\n",
    "\n",
    "    model = model_dtc = RandomForestClassifier(n_estimators=80)\n",
    "    model.fit(input_x,input_y)\n",
    "    pred = model.predict_proba(val_data)\n",
    "\n",
    "    for _date,_pred in zip(pre_date,pred):\n",
    "        result.append([ts,_date,_pred[1]])\n",
    "result = pd.DataFrame(result,columns=['ts_code','trade_date','p'])\n",
    "result.to_csv('result3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "result = []\n",
    "for i ,ts in enumerate(Data2.ts_code.unique()):\n",
    "    input_x = np.array(list(Data['allVec']))\n",
    "    input_y = np.array(list(Data['y_']))[:,i]\n",
    "    ss = StandardScaler()\n",
    "    x_train = ss.fit_transform(x_train)\n",
    "    x_test = ss.transform(x_test)\n",
    "\n",
    "    model = model_dtc = XGBClassifier(learning_rate=0.1,n_estimators=500,seed=64)\n",
    "    print('# fit...')\n",
    "    model.fit(input_x,input_y)\n",
    "    print('auc:',roc_auc_score(input_y,model.predict_proba(np.array(input_x))[:,1]))\n",
    "    pred = model.predict_proba(np.array(val_data))\n",
    "\n",
    "#     for _date,_pred in zip(pre_date,pred):\n",
    "#         result.append([ts,_date,_pred[1]])\n",
    "# result = pd.DataFrame(result,columns=['ts_code','trade_date','p'])\n",
    "# result.to_csv('result2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n",
      "# fit...\n",
      "auc: 1.0\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "result = []\n",
    "for i ,ts in enumerate(Data2.ts_code.unique()):\n",
    "    input_x = np.array(list(Data['allVec']))\n",
    "    input_y = np.array(list(Data['y_']))[:,i]\n",
    "    ss = StandardScaler()\n",
    "    x_train = ss.fit_transform(x_train)\n",
    "    x_test = ss.transform(x_test)\n",
    "\n",
    "    model = model_dtc = LGBMClassifier(learning_rate=0.1,n_estimators=300,seed=64)\n",
    "    print('# fit...')\n",
    "    model.fit(input_x,input_y)\n",
    "    print('auc:',roc_auc_score(input_y,model.predict_proba(np.array(input_x))[:,1]))\n",
    "    pred = model.predict_proba(np.array(val_data))\n",
    "\n",
    "#     for _date,_pred in zip(pre_date,pred):\n",
    "#         result.append([ts,_date,_pred[1]])\n",
    "# result = pd.DataFrame(result,columns=['ts_code','trade_date','p'])\n",
    "# result.to_csv('result_lgb.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fit 0...\n",
      "auc: 1.0\n",
      "# fit 1...\n",
      "auc: 1.0\n",
      "# fit 2...\n",
      "auc: 1.0\n",
      "# fit 3...\n",
      "auc: 1.0\n",
      "# fit 4...\n",
      "auc: 1.0\n",
      "# fit 5...\n",
      "auc: 1.0\n",
      "# fit 6...\n",
      "auc: 1.0\n",
      "# fit 7...\n",
      "auc: 1.0\n",
      "# fit 8...\n",
      "auc: 1.0\n",
      "# fit 9...\n",
      "auc: 1.0\n",
      "# fit 10...\n",
      "auc: 1.0\n",
      "# fit 11...\n",
      "auc: 1.0\n",
      "# fit 12...\n",
      "auc: 1.0\n",
      "# fit 13...\n",
      "auc: 1.0\n",
      "# fit 14...\n",
      "auc: 1.0\n",
      "# fit 15...\n",
      "auc: 1.0\n",
      "# fit 16...\n",
      "auc: 1.0\n",
      "# fit 17...\n",
      "auc: 1.0\n",
      "# fit 18...\n",
      "auc: 1.0\n",
      "# fit 19...\n",
      "auc: 1.0\n",
      "# fit 20...\n",
      "auc: 1.0\n",
      "# fit 21...\n",
      "auc: 1.0\n",
      "# fit 22...\n",
      "auc: 1.0\n",
      "# fit 23...\n",
      "auc: 1.0\n",
      "# fit 24...\n",
      "auc: 1.0\n",
      "# fit 25...\n",
      "auc: 1.0\n",
      "# fit 26...\n",
      "auc: 1.0\n",
      "# fit 27...\n",
      "auc: 1.0\n",
      "# fit 28...\n",
      "auc: 1.0\n",
      "# fit 29...\n",
      "auc: 1.0\n",
      "# fit 30...\n",
      "auc: 1.0\n",
      "# fit 31...\n",
      "auc: 1.0\n",
      "# fit 32...\n",
      "auc: 1.0\n",
      "# fit 33...\n",
      "auc: 1.0\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "params ={\n",
    "    'boosting_type':'gbdt',\n",
    "#     'objective':'regression',\n",
    "    'learning_rate':0.1,\n",
    "    'metric':'auc',\n",
    "    'max_depth':-1,\n",
    "    'seed':64,\n",
    "#     'num_leaves':50,\n",
    "#     'sample':0.8,\n",
    "#     'colsample_bytree':0.8,\n",
    "}\n",
    "\n",
    "def model_fit(params,model,x_train,y_train,early_stopping_rounds=80):\n",
    "    lgb_train = lgb.Dataset(x_train,y_train)\n",
    "    \n",
    "    print('cving ...')\n",
    "    cv_result = lgb.cv(params,lgb_train,num_boost_round=5000,nfold=10,stratified=False,\n",
    "                       shuffle=True,metrics='auc',early_stopping_rounds=early_stopping_rounds,seed=0)\n",
    "    \n",
    "    print('cv finished ')\n",
    "    import json\n",
    "    with open('cv_result.json','w')as file :\n",
    "        json.dump(cv_result,file)\n",
    "        print(cv_result)\n",
    "    n_estimators = len(cv_result['auc-mean'])\n",
    "    print(n_estimators)\n",
    "    model.set_params(n_estimators=n_estimators)\n",
    "    print('refitting ...')\n",
    "    model.fit(x_train,y_train)\n",
    "    print('over!')\n",
    "\n",
    "result = []\n",
    "for i ,ts in enumerate(Data2.ts_code.unique()):\n",
    "    input_x = np.array(list(Data['allVec']))\n",
    "    input_y = np.array(list(Data['y_']))[:,i]\n",
    "    ss = StandardScaler()\n",
    "    x_train = ss.fit_transform(x_train)\n",
    "    x_test = ss.transform(x_test)\n",
    "\n",
    "    model = model_dtc = LGBMClassifier(learn_rate=0.2,n_estimators=200)\n",
    "#     model_fit(params,model,input_x,input_y)\n",
    "    print('# fit {}...'.format(i))\n",
    "    model.fit(input_x,input_y)\n",
    "    print('auc:',roc_auc_score(input_y,model.predict_proba(np.array(input_x))[:,1]))\n",
    "    pred = model.predict_proba(np.array(val_data))\n",
    "\n",
    "#     for _date,_pred in zip(pre_date,pred):\n",
    "#         result.append([ts,_date,_pred[1]])\n",
    "# result = pd.DataFrame(result,columns=['ts_code','trade_date','p'])\n",
    "# result.to_csv('result_lgb2.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = pd.read_csv('result_lgb.csv')\n",
    "result2 = pd.read_csv('result_lgb2.csv')\n",
    "result3 = pd.read_csv('result2.csv')\n",
    "result1.p = result1.p * 3/8 + result2.p * 1/8 + result3.p * 1/2\n",
    "result1.to_csv('final_result_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 深度模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel:\n",
    "    def __init__(self,learning_rate,classes_num=2):\n",
    "        self.lr = learning_rate\n",
    "        self.classes_num = classes_num\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        # Placeholder \n",
    "        self.input_x = tf.placeholder(shape=[None,200],dtype=tf.float32,name='input_x')\n",
    "        self.input_y = tf.placeholder(shape=[None],dtype=tf.int32,name='input_y')\n",
    "        self.keep_dropout_rate = tf.placeholder(dtype=tf.float32,name='dropout_rate')\n",
    "        \n",
    "        \n",
    "        W_shape = {'w1':256,'w2':64,'w3':2}\n",
    "        b_shape = {'b1':256,'b2':64,'b3':2}\n",
    "        \n",
    "        \n",
    "        self.W1 = tf.Variable(tf.truncated_normal(shape=(200,W_shape['w1']),stddev=0.1))\n",
    "        self.W2 = tf.Variable(tf.truncated_normal(shape=(W_shape['w1'],W_shape['w2']),stddev=0.1))\n",
    "        self.W3 = tf.Variable(tf.truncated_normal(shape=(W_shape['w2'],W_shape['w3']),stddev=0.1))\n",
    "        \n",
    "        \n",
    "        self.b1 = tf.Variable(tf.constant(0.1,shape=[b_shape['b1']]))\n",
    "        self.b2 = tf.Variable(tf.constant(0.1,shape=[b_shape['b2']]))\n",
    "        self.b3 = tf.Variable(tf.constant(0.1,shape=[b_shape['b3']]))\n",
    "        self.inference()\n",
    "\n",
    "        \n",
    "    def inference(self):\n",
    "        output = tf.matmul(self.input_x,self.W1) + self.b1\n",
    "        output = tf.nn.relu(output)\n",
    "#         output = tf.nn.dropout(output, self.keep_dropout_rate)\n",
    "        \n",
    "        output = tf.matmul(output,self.W2) + self.b2\n",
    "        output = tf.nn.relu(output)\n",
    "#         output = tf.nn.dropout(output, self.keep_dropout_rate)\n",
    "        \n",
    "        output = tf.matmul(output,self.W3) + self.b3\n",
    "        output = tf.nn.relu(output)\n",
    "#         output = tf.nn.dropout(output,self.keep_dropout_rate)\n",
    "        \n",
    "        self.logits = tf.layers.dense(output,self.classes_num,activation=tf.nn.tanh,name='logits')\n",
    "        \n",
    "        \n",
    "        self.loss = cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits))\n",
    "        \n",
    "        op = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = op.minimize(cross_entropy,global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Initilize model ...\n",
      "Step:50 , train loss : 0.7148253917694092\n",
      "Step:50 , test loss : 0.698843240737915\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:100 , train loss : 0.6710616946220398\n",
      "Step:100 , test loss : 0.6913337707519531\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:150 , train loss : 0.663375735282898\n",
      "Step:150 , test loss : 0.6929583549499512\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:200 , train loss : 0.6986691355705261\n",
      "Step:200 , test loss : 0.6928972005844116\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:250 , train loss : 0.67051100730896\n",
      "Step:250 , test loss : 0.690761923789978\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:300 , train loss : 0.6602226495742798\n",
      "Step:300 , test loss : 0.6907006502151489\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:350 , train loss : 0.6635974645614624\n",
      "Step:350 , test loss : 0.6927167773246765\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:400 , train loss : 0.6607749462127686\n",
      "Step:400 , test loss : 0.6919770240783691\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:450 , train loss : 0.6808528304100037\n",
      "Step:450 , test loss : 0.6920340061187744\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:500 , train loss : 0.6846380233764648\n",
      "Step:500 , test loss : 0.6907791495323181\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:550 , train loss : 0.7005483508110046\n",
      "Step:550 , test loss : 0.7037014961242676\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:600 , train loss : 0.6757882237434387\n",
      "Step:600 , test loss : 0.6909709572792053\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:650 , train loss : 0.7014646530151367\n",
      "Step:650 , test loss : 0.6974716186523438\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:700 , train loss : 0.6763556599617004\n",
      "Step:700 , test loss : 0.6911476254463196\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:750 , train loss : 0.6951329708099365\n",
      "Step:750 , test loss : 0.6909424662590027\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:800 , train loss : 0.6876227855682373\n",
      "Step:800 , test loss : 0.6971882581710815\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:850 , train loss : 0.7039161920547485\n",
      "Step:850 , test loss : 0.6919684410095215\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:900 , train loss : 0.7009446620941162\n",
      "Step:900 , test loss : 0.691893994808197\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:950 , train loss : 0.6573110818862915\n",
      "Step:950 , test loss : 0.6937476992607117\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1000 , train loss : 0.7264371514320374\n",
      "Step:1000 , test loss : 0.6982586979866028\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1050 , train loss : 0.6971748471260071\n",
      "Step:1050 , test loss : 0.6909695863723755\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1100 , train loss : 0.6853175163269043\n",
      "Step:1100 , test loss : 0.6922128200531006\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1150 , train loss : 0.6786378622055054\n",
      "Step:1150 , test loss : 0.6915175318717957\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1200 , train loss : 0.7642621397972107\n",
      "Step:1200 , test loss : 0.6947953104972839\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1250 , train loss : 0.6879065036773682\n",
      "Step:1250 , test loss : 0.7004285454750061\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1300 , train loss : 0.6938730478286743\n",
      "Step:1300 , test loss : 0.692805826663971\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1350 , train loss : 0.7901360392570496\n",
      "Step:1350 , test loss : 0.6919906735420227\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1400 , train loss : 0.6907798051834106\n",
      "Step:1400 , test loss : 0.694570779800415\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1450 , train loss : 0.6771810054779053\n",
      "Step:1450 , test loss : 0.6927240490913391\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1500 , train loss : 0.6610941290855408\n",
      "Step:1500 , test loss : 0.6919365525245667\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1550 , train loss : 0.6735590696334839\n",
      "Step:1550 , test loss : 0.6924468874931335\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1600 , train loss : 0.6948826909065247\n",
      "Step:1600 , test loss : 0.6910713315010071\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1650 , train loss : 0.6597548723220825\n",
      "Step:1650 , test loss : 0.6923300623893738\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1700 , train loss : 0.6922153234481812\n",
      "Step:1700 , test loss : 0.6913871765136719\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1750 , train loss : 0.6854550838470459\n",
      "Step:1750 , test loss : 0.6914772987365723\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1800 , train loss : 0.726105272769928\n",
      "Step:1800 , test loss : 0.6925545334815979\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1850 , train loss : 0.7103371620178223\n",
      "Step:1850 , test loss : 0.692683756351471\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1900 , train loss : 0.6854357123374939\n",
      "Step:1900 , test loss : 0.6913701295852661\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:1950 , train loss : 0.6805811524391174\n",
      "Step:1950 , test loss : 0.6916823983192444\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2000 , train loss : 0.69511479139328\n",
      "Step:2000 , test loss : 0.6910895109176636\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2050 , train loss : 0.7099866271018982\n",
      "Step:2050 , test loss : 0.6915399432182312\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2100 , train loss : 0.6794732213020325\n",
      "Step:2100 , test loss : 0.6934803128242493\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2150 , train loss : 0.6723872423171997\n",
      "Step:2150 , test loss : 0.6932141780853271\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2200 , train loss : 0.712479829788208\n",
      "Step:2200 , test loss : 0.6919615268707275\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2250 , train loss : 0.6618216633796692\n",
      "Step:2250 , test loss : 0.6916688084602356\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2300 , train loss : 0.702947199344635\n",
      "Step:2300 , test loss : 0.6930202841758728\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2350 , train loss : 0.6817083358764648\n",
      "Step:2350 , test loss : 0.6916710138320923\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2400 , train loss : 0.7349674701690674\n",
      "Step:2400 , test loss : 0.6912448406219482\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2450 , train loss : 0.6959333419799805\n",
      "Step:2450 , test loss : 0.6947036981582642\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2500 , train loss : 0.6887463331222534\n",
      "Step:2500 , test loss : 0.6910291910171509\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2550 , train loss : 0.6639687418937683\n",
      "Step:2550 , test loss : 0.6914069652557373\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2600 , train loss : 0.6890115141868591\n",
      "Step:2600 , test loss : 0.6918329000473022\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2650 , train loss : 0.6587143540382385\n",
      "Step:2650 , test loss : 0.6913239359855652\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2700 , train loss : 0.6798027157783508\n",
      "Step:2700 , test loss : 0.6924048066139221\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2750 , train loss : 0.6640465259552002\n",
      "Step:2750 , test loss : 0.6928624510765076\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2800 , train loss : 0.6719460487365723\n",
      "Step:2800 , test loss : 0.6915379166603088\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2850 , train loss : 0.6820828914642334\n",
      "Step:2850 , test loss : 0.6909304261207581\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2900 , train loss : 0.6707773208618164\n",
      "Step:2900 , test loss : 0.6914901733398438\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:2950 , train loss : 0.6758143901824951\n",
      "Step:2950 , test loss : 0.6912071704864502\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3000 , train loss : 0.7359871864318848\n",
      "Step:3000 , test loss : 0.6946839690208435\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3050 , train loss : 0.6809228658676147\n",
      "Step:3050 , test loss : 0.6955692172050476\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3100 , train loss : 0.6856558322906494\n",
      "Step:3100 , test loss : 0.6938685178756714\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3150 , train loss : 0.679570198059082\n",
      "Step:3150 , test loss : 0.6931397318840027\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3200 , train loss : 0.6993333697319031\n",
      "Step:3200 , test loss : 0.6935206055641174\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3250 , train loss : 0.6823476552963257\n",
      "Step:3250 , test loss : 0.6911768913269043\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3300 , train loss : 0.6580531001091003\n",
      "Step:3300 , test loss : 0.6927911639213562\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3350 , train loss : 0.6732715368270874\n",
      "Step:3350 , test loss : 0.6923702359199524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3400 , train loss : 0.6659589409828186\n",
      "Step:3400 , test loss : 0.6919938325881958\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3450 , train loss : 0.7041915059089661\n",
      "Step:3450 , test loss : 0.6932259202003479\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3500 , train loss : 0.6761798858642578\n",
      "Step:3500 , test loss : 0.6942471265792847\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3550 , train loss : 0.6853406429290771\n",
      "Step:3550 , test loss : 0.6919216513633728\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3600 , train loss : 0.6797896027565002\n",
      "Step:3600 , test loss : 0.6927928328514099\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3650 , train loss : 0.6853252649307251\n",
      "Step:3650 , test loss : 0.691916823387146\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3700 , train loss : 0.6695630550384521\n",
      "Step:3700 , test loss : 0.6922535300254822\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3750 , train loss : 0.6963396668434143\n",
      "Step:3750 , test loss : 0.6907424926757812\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3800 , train loss : 0.6979526281356812\n",
      "Step:3800 , test loss : 0.6924476623535156\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3850 , train loss : 0.701552152633667\n",
      "Step:3850 , test loss : 0.6907117366790771\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3900 , train loss : 0.6794567108154297\n",
      "Step:3900 , test loss : 0.693193256855011\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:3950 , train loss : 0.6889176964759827\n",
      "Step:3950 , test loss : 0.6916359066963196\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4000 , train loss : 0.6660760641098022\n",
      "Step:4000 , test loss : 0.6923930048942566\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4050 , train loss : 0.7053942084312439\n",
      "Step:4050 , test loss : 0.6939030885696411\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4100 , train loss : 0.6893622875213623\n",
      "Step:4100 , test loss : 0.6921648383140564\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4150 , train loss : 0.709053099155426\n",
      "Step:4150 , test loss : 0.6920397281646729\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4200 , train loss : 0.6592267155647278\n",
      "Step:4200 , test loss : 0.6924858093261719\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4250 , train loss : 0.698708713054657\n",
      "Step:4250 , test loss : 0.6935228109359741\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4300 , train loss : 0.680203914642334\n",
      "Step:4300 , test loss : 0.6907582879066467\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4350 , train loss : 0.6555555462837219\n",
      "Step:4350 , test loss : 0.693569540977478\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4400 , train loss : 0.712928056716919\n",
      "Step:4400 , test loss : 0.6907649636268616\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4450 , train loss : 0.6668403744697571\n",
      "Step:4450 , test loss : 0.6908353567123413\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4500 , train loss : 0.6813502311706543\n",
      "Step:4500 , test loss : 0.6912878155708313\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4550 , train loss : 0.685334324836731\n",
      "Step:4550 , test loss : 0.6926780343055725\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4600 , train loss : 0.7045626044273376\n",
      "Step:4600 , test loss : 0.6910321712493896\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4650 , train loss : 0.6793316006660461\n",
      "Step:4650 , test loss : 0.6941667795181274\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4700 , train loss : 0.691004753112793\n",
      "Step:4700 , test loss : 0.6951801776885986\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4750 , train loss : 0.6702333688735962\n",
      "Step:4750 , test loss : 0.6920923590660095\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4800 , train loss : 0.7032926082611084\n",
      "Step:4800 , test loss : 0.6929692029953003\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4850 , train loss : 0.6921743154525757\n",
      "Step:4850 , test loss : 0.6911499500274658\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4900 , train loss : 0.6857928037643433\n",
      "Step:4900 , test loss : 0.6945599317550659\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:4950 , train loss : 0.6569979190826416\n",
      "Step:4950 , test loss : 0.6929518580436707\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5000 , train loss : 0.7016860246658325\n",
      "Step:5000 , test loss : 0.692457914352417\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5050 , train loss : 0.685354471206665\n",
      "Step:5050 , test loss : 0.6928020119667053\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5100 , train loss : 0.6807476878166199\n",
      "Step:5100 , test loss : 0.6917452216148376\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5150 , train loss : 0.6946141123771667\n",
      "Step:5150 , test loss : 0.6908978223800659\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5200 , train loss : 0.6973084211349487\n",
      "Step:5200 , test loss : 0.690887987613678\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5250 , train loss : 0.7022883296012878\n",
      "Step:5250 , test loss : 0.6927865743637085\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5300 , train loss : 0.6633063554763794\n",
      "Step:5300 , test loss : 0.6916963458061218\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5350 , train loss : 0.6864877939224243\n",
      "Step:5350 , test loss : 0.6951276063919067\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5400 , train loss : 0.7272209525108337\n",
      "Step:5400 , test loss : 0.69305819272995\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5450 , train loss : 0.6737197041511536\n",
      "Step:5450 , test loss : 0.6922448873519897\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5500 , train loss : 0.7032248973846436\n",
      "Step:5500 , test loss : 0.6916282176971436\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5550 , train loss : 0.6984090805053711\n",
      "Step:5550 , test loss : 0.6912018060684204\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5600 , train loss : 0.6994220018386841\n",
      "Step:5600 , test loss : 0.691413938999176\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5650 , train loss : 0.702587902545929\n",
      "Step:5650 , test loss : 0.6956461668014526\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5700 , train loss : 0.7345762848854065\n",
      "Step:5700 , test loss : 0.6912280917167664\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5750 , train loss : 0.7108583450317383\n",
      "Step:5750 , test loss : 0.694169819355011\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5800 , train loss : 0.6741862297058105\n",
      "Step:5800 , test loss : 0.6909791827201843\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5850 , train loss : 0.7238052487373352\n",
      "Step:5850 , test loss : 0.6925012469291687\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5900 , train loss : 0.6666861176490784\n",
      "Step:5900 , test loss : 0.6918226480484009\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:5950 , train loss : 0.6853199005126953\n",
      "Step:5950 , test loss : 0.6921394467353821\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6000 , train loss : 0.6118929982185364\n",
      "Step:6000 , test loss : 0.6932989358901978\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6050 , train loss : 0.6857662200927734\n",
      "Step:6050 , test loss : 0.6946669220924377\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6100 , train loss : 0.7013713121414185\n",
      "Step:6100 , test loss : 0.6910313963890076\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6150 , train loss : 0.6795465350151062\n",
      "Step:6150 , test loss : 0.6930497288703918\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6200 , train loss : 0.6663926243782043\n",
      "Step:6200 , test loss : 0.6946253180503845\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6250 , train loss : 0.6964150071144104\n",
      "Step:6250 , test loss : 0.6919108629226685\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6300 , train loss : 0.6793255805969238\n",
      "Step:6300 , test loss : 0.6933124661445618\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6350 , train loss : 0.6760509014129639\n",
      "Step:6350 , test loss : 0.6947728395462036\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6400 , train loss : 0.708637535572052\n",
      "Step:6400 , test loss : 0.6919620633125305\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6450 , train loss : 0.7445728182792664\n",
      "Step:6450 , test loss : 0.6923530101776123\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6500 , train loss : 0.6760149598121643\n",
      "Step:6500 , test loss : 0.6949015259742737\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6550 , train loss : 0.6779775619506836\n",
      "Step:6550 , test loss : 0.6919350624084473\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6600 , train loss : 0.720827043056488\n",
      "Step:6600 , test loss : 0.6918222308158875\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6650 , train loss : 0.6773602366447449\n",
      "Step:6650 , test loss : 0.6908516883850098\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6700 , train loss : 0.7066312432289124\n",
      "Step:6700 , test loss : 0.6929775476455688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6750 , train loss : 0.6581518650054932\n",
      "Step:6750 , test loss : 0.6927797794342041\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6800 , train loss : 0.6853718757629395\n",
      "Step:6800 , test loss : 0.6933302879333496\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6850 , train loss : 0.6973538398742676\n",
      "Step:6850 , test loss : 0.6959271430969238\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6900 , train loss : 0.7414500117301941\n",
      "Step:6900 , test loss : 0.6918449401855469\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:6950 , train loss : 0.6704189777374268\n",
      "Step:6950 , test loss : 0.6910985112190247\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7000 , train loss : 0.7134264707565308\n",
      "Step:7000 , test loss : 0.6923686861991882\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7050 , train loss : 0.7321934700012207\n",
      "Step:7050 , test loss : 0.6939541101455688\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7100 , train loss : 0.6853878498077393\n",
      "Step:7100 , test loss : 0.6916438341140747\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7150 , train loss : 0.6924376487731934\n",
      "Step:7150 , test loss : 0.6916269659996033\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7200 , train loss : 0.6808229088783264\n",
      "Step:7200 , test loss : 0.6914571523666382\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7250 , train loss : 0.7131799459457397\n",
      "Step:7250 , test loss : 0.6919969320297241\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7300 , train loss : 0.6895296573638916\n",
      "Step:7300 , test loss : 0.6922407746315002\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7350 , train loss : 0.698512852191925\n",
      "Step:7350 , test loss : 0.6911605000495911\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7400 , train loss : 0.6815011501312256\n",
      "Step:7400 , test loss : 0.6922018527984619\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7450 , train loss : 0.6813668012619019\n",
      "Step:7450 , test loss : 0.6968843936920166\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7500 , train loss : 0.7592275738716125\n",
      "Step:7500 , test loss : 0.693533718585968\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7550 , train loss : 0.6517747640609741\n",
      "Step:7550 , test loss : 0.6933433413505554\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7600 , train loss : 0.6949442625045776\n",
      "Step:7600 , test loss : 0.6936106085777283\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7650 , train loss : 0.7223982214927673\n",
      "Step:7650 , test loss : 0.6907049417495728\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7700 , train loss : 0.6809277534484863\n",
      "Step:7700 , test loss : 0.6936928033828735\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7750 , train loss : 0.6854350566864014\n",
      "Step:7750 , test loss : 0.6913957595825195\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7800 , train loss : 0.6613020300865173\n",
      "Step:7800 , test loss : 0.6920064091682434\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7850 , train loss : 0.6759449243545532\n",
      "Step:7850 , test loss : 0.6910737156867981\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7900 , train loss : 0.6958773136138916\n",
      "Step:7900 , test loss : 0.6914819478988647\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:7950 , train loss : 0.7281971573829651\n",
      "Step:7950 , test loss : 0.692535936832428\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8000 , train loss : 0.6887952089309692\n",
      "Step:8000 , test loss : 0.6913455724716187\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8050 , train loss : 0.6940346956253052\n",
      "Step:8050 , test loss : 0.6907604932785034\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8100 , train loss : 0.6805346012115479\n",
      "Step:8100 , test loss : 0.6919070482254028\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8150 , train loss : 0.7125394940376282\n",
      "Step:8150 , test loss : 0.6919592618942261\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8200 , train loss : 0.6823124289512634\n",
      "Step:8200 , test loss : 0.6912892460823059\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8250 , train loss : 0.6407030820846558\n",
      "Step:8250 , test loss : 0.6922478079795837\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8300 , train loss : 0.6814211010932922\n",
      "Step:8300 , test loss : 0.6921214461326599\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8350 , train loss : 0.7071698904037476\n",
      "Step:8350 , test loss : 0.69161456823349\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8400 , train loss : 0.7030391693115234\n",
      "Step:8400 , test loss : 0.6927918791770935\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8450 , train loss : 0.6809166669845581\n",
      "Step:8450 , test loss : 0.6939831376075745\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8500 , train loss : 0.6888262033462524\n",
      "Step:8500 , test loss : 0.691388726234436\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8550 , train loss : 0.6594757437705994\n",
      "Step:8550 , test loss : 0.6920329928398132\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8600 , train loss : 0.6811020374298096\n",
      "Step:8600 , test loss : 0.6930896639823914\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8650 , train loss : 0.6721218824386597\n",
      "Step:8650 , test loss : 0.6935931444168091\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8700 , train loss : 0.6810385584831238\n",
      "Step:8700 , test loss : 0.6914559006690979\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8750 , train loss : 0.6817885637283325\n",
      "Step:8750 , test loss : 0.6916112899780273\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8800 , train loss : 0.6655151844024658\n",
      "Step:8800 , test loss : 0.6924265623092651\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8850 , train loss : 0.6795978546142578\n",
      "Step:8850 , test loss : 0.6930152773857117\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8900 , train loss : 0.6825056076049805\n",
      "Step:8900 , test loss : 0.6906862258911133\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:8950 , train loss : 0.6632715463638306\n",
      "Step:8950 , test loss : 0.693415105342865\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9000 , train loss : 0.6449822187423706\n",
      "Step:9000 , test loss : 0.6916108131408691\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9050 , train loss : 0.7067019939422607\n",
      "Step:9050 , test loss : 0.6953176259994507\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9100 , train loss : 0.6895509362220764\n",
      "Step:9100 , test loss : 0.6927934885025024\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9150 , train loss : 0.6793356537818909\n",
      "Step:9150 , test loss : 0.6937553882598877\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9200 , train loss : 0.6812210083007812\n",
      "Step:9200 , test loss : 0.6927638053894043\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9250 , train loss : 0.6719241142272949\n",
      "Step:9250 , test loss : 0.6906912922859192\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9300 , train loss : 0.7171534895896912\n",
      "Step:9300 , test loss : 0.6912705302238464\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9350 , train loss : 0.6988970637321472\n",
      "Step:9350 , test loss : 0.6935402750968933\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9400 , train loss : 0.7073105573654175\n",
      "Step:9400 , test loss : 0.692980170249939\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9450 , train loss : 0.6802523732185364\n",
      "Step:9450 , test loss : 0.6919937133789062\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9500 , train loss : 0.7079737186431885\n",
      "Step:9500 , test loss : 0.6908450722694397\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9550 , train loss : 0.6814946532249451\n",
      "Step:9550 , test loss : 0.6920647621154785\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9600 , train loss : 0.7443706393241882\n",
      "Step:9600 , test loss : 0.6919742822647095\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9650 , train loss : 0.6813310384750366\n",
      "Step:9650 , test loss : 0.6924132108688354\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9700 , train loss : 0.7101140022277832\n",
      "Step:9700 , test loss : 0.6917515397071838\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9750 , train loss : 0.6804690361022949\n",
      "Step:9750 , test loss : 0.6918238401412964\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9800 , train loss : 0.6952965259552002\n",
      "Step:9800 , test loss : 0.6937370300292969\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9850 , train loss : 0.669661283493042\n",
      "Step:9850 , test loss : 0.6912116408348083\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9900 , train loss : 0.6236117482185364\n",
      "Step:9900 , test loss : 0.6919463276863098\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:9950 , train loss : 0.6665857434272766\n",
      "Step:9950 , test loss : 0.6945924162864685\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10000 , train loss : 0.6931619644165039\n",
      "Step:10000 , test loss : 0.6919746994972229\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10050 , train loss : 0.7221352458000183\n",
      "Step:10050 , test loss : 0.691978931427002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10100 , train loss : 0.7082303166389465\n",
      "Step:10100 , test loss : 0.6935698986053467\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10150 , train loss : 0.7093474864959717\n",
      "Step:10150 , test loss : 0.6921537518501282\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10200 , train loss : 0.6799631714820862\n",
      "Step:10200 , test loss : 0.6925386190414429\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10250 , train loss : 0.6960777640342712\n",
      "Step:10250 , test loss : 0.6906959414482117\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10300 , train loss : 0.697023868560791\n",
      "Step:10300 , test loss : 0.6950216889381409\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10350 , train loss : 0.7264634966850281\n",
      "Step:10350 , test loss : 0.6931540369987488\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10400 , train loss : 0.6853424310684204\n",
      "Step:10400 , test loss : 0.6928618550300598\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10450 , train loss : 0.6561446189880371\n",
      "Step:10450 , test loss : 0.6947230100631714\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10500 , train loss : 0.6579879522323608\n",
      "Step:10500 , test loss : 0.6925817728042603\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10550 , train loss : 0.6916453838348389\n",
      "Step:10550 , test loss : 0.6908302307128906\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10600 , train loss : 0.6694605350494385\n",
      "Step:10600 , test loss : 0.6912386417388916\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10650 , train loss : 0.7754365801811218\n",
      "Step:10650 , test loss : 0.6925477385520935\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10700 , train loss : 0.6904515624046326\n",
      "Step:10700 , test loss : 0.6937136650085449\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10750 , train loss : 0.6674230694770813\n",
      "Step:10750 , test loss : 0.6911555528640747\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10800 , train loss : 0.6605578064918518\n",
      "Step:10800 , test loss : 0.6920099258422852\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10850 , train loss : 0.6795940399169922\n",
      "Step:10850 , test loss : 0.6910237073898315\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10900 , train loss : 0.6680804491043091\n",
      "Step:10900 , test loss : 0.6933305263519287\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:10950 , train loss : 0.7008761763572693\n",
      "Step:10950 , test loss : 0.6922579407691956\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11000 , train loss : 0.7070757746696472\n",
      "Step:11000 , test loss : 0.6907117366790771\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11050 , train loss : 0.6941835880279541\n",
      "Step:11050 , test loss : 0.690788745880127\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11100 , train loss : 0.70367830991745\n",
      "Step:11100 , test loss : 0.692713737487793\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11150 , train loss : 0.6551281213760376\n",
      "Step:11150 , test loss : 0.6914609670639038\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11200 , train loss : 0.695586085319519\n",
      "Step:11200 , test loss : 0.6913447380065918\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11250 , train loss : 0.6645265221595764\n",
      "Step:11250 , test loss : 0.6912122964859009\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11300 , train loss : 0.7240397334098816\n",
      "Step:11300 , test loss : 0.6920171976089478\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11350 , train loss : 0.6965559124946594\n",
      "Step:11350 , test loss : 0.6910876631736755\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11400 , train loss : 0.6794335842132568\n",
      "Step:11400 , test loss : 0.6933420300483704\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11450 , train loss : 0.6982846260070801\n",
      "Step:11450 , test loss : 0.6978320479393005\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11500 , train loss : 0.6678303480148315\n",
      "Step:11500 , test loss : 0.6934985518455505\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11550 , train loss : 0.6587511897087097\n",
      "Step:11550 , test loss : 0.6925315260887146\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11600 , train loss : 0.6964519023895264\n",
      "Step:11600 , test loss : 0.6915149092674255\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11650 , train loss : 0.6755669116973877\n",
      "Step:11650 , test loss : 0.6951304078102112\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11700 , train loss : 0.68061763048172\n",
      "Step:11700 , test loss : 0.6917127370834351\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11750 , train loss : 0.6957347393035889\n",
      "Step:11750 , test loss : 0.691505491733551\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11800 , train loss : 0.6858569383621216\n",
      "Step:11800 , test loss : 0.6949569582939148\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11850 , train loss : 0.7030860781669617\n",
      "Step:11850 , test loss : 0.692943811416626\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11900 , train loss : 0.6782081127166748\n",
      "Step:11900 , test loss : 0.6915539503097534\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:11950 , train loss : 0.678164005279541\n",
      "Step:11950 , test loss : 0.6915907859802246\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12000 , train loss : 0.720208466053009\n",
      "Step:12000 , test loss : 0.6918717622756958\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12050 , train loss : 0.6773529052734375\n",
      "Step:12050 , test loss : 0.6908224821090698\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12100 , train loss : 0.69769287109375\n",
      "Step:12100 , test loss : 0.6958780884742737\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12150 , train loss : 0.726509153842926\n",
      "Step:12150 , test loss : 0.6927973031997681\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12200 , train loss : 0.6853225231170654\n",
      "Step:12200 , test loss : 0.6922764182090759\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12250 , train loss : 0.6893053650856018\n",
      "Step:12250 , test loss : 0.6924583315849304\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12300 , train loss : 0.6568184494972229\n",
      "Step:12300 , test loss : 0.6929605007171631\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12350 , train loss : 0.6592488288879395\n",
      "Step:12350 , test loss : 0.6929399371147156\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12400 , train loss : 0.6925161480903625\n",
      "Step:12400 , test loss : 0.6915460824966431\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12450 , train loss : 0.6535253524780273\n",
      "Step:12450 , test loss : 0.694090723991394\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12500 , train loss : 0.6601046919822693\n",
      "Step:12500 , test loss : 0.6917670369148254\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12550 , train loss : 0.6815222501754761\n",
      "Step:12550 , test loss : 0.6907091736793518\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12600 , train loss : 0.6801629662513733\n",
      "Step:12600 , test loss : 0.6921684145927429\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12650 , train loss : 0.6814193725585938\n",
      "Step:12650 , test loss : 0.6923425793647766\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12700 , train loss : 0.6887535452842712\n",
      "Step:12700 , test loss : 0.6911306381225586\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12750 , train loss : 0.7018625140190125\n",
      "Step:12750 , test loss : 0.6925244331359863\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12800 , train loss : 0.6707196831703186\n",
      "Step:12800 , test loss : 0.695758044719696\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12850 , train loss : 0.6744257211685181\n",
      "Step:12850 , test loss : 0.6909964680671692\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12900 , train loss : 0.6308538913726807\n",
      "Step:12900 , test loss : 0.6913213729858398\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:12950 , train loss : 0.7099824547767639\n",
      "Step:12950 , test loss : 0.6923037767410278\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13000 , train loss : 0.6808900237083435\n",
      "Step:13000 , test loss : 0.6941858530044556\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13050 , train loss : 0.6530086398124695\n",
      "Step:13050 , test loss : 0.69406658411026\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13100 , train loss : 0.6855823397636414\n",
      "Step:13100 , test loss : 0.694257378578186\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13150 , train loss : 0.6604150533676147\n",
      "Step:13150 , test loss : 0.6930510997772217\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13200 , train loss : 0.7558560371398926\n",
      "Step:13200 , test loss : 0.6913281083106995\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13250 , train loss : 0.698710024356842\n",
      "Step:13250 , test loss : 0.6933955550193787\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13300 , train loss : 0.6894161105155945\n",
      "Step:13300 , test loss : 0.6926564574241638\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13350 , train loss : 0.7379095554351807\n",
      "Step:13350 , test loss : 0.6914354562759399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13400 , train loss : 0.6775192022323608\n",
      "Step:13400 , test loss : 0.6907871961593628\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13450 , train loss : 0.6822918653488159\n",
      "Step:13450 , test loss : 0.6912351846694946\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13500 , train loss : 0.65579754114151\n",
      "Step:13500 , test loss : 0.6935627460479736\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13550 , train loss : 0.6813693046569824\n",
      "Step:13550 , test loss : 0.6921466588973999\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13600 , train loss : 0.6991084814071655\n",
      "Step:13600 , test loss : 0.6934143304824829\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13650 , train loss : 0.6589817404747009\n",
      "Step:13650 , test loss : 0.6926078796386719\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13700 , train loss : 0.6533656716346741\n",
      "Step:13700 , test loss : 0.6941794753074646\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13750 , train loss : 0.6586640477180481\n",
      "Step:13750 , test loss : 0.6921104192733765\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13800 , train loss : 0.7403357625007629\n",
      "Step:13800 , test loss : 0.6917397379875183\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13850 , train loss : 0.6723655462265015\n",
      "Step:13850 , test loss : 0.6913735866546631\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13900 , train loss : 0.6714683771133423\n",
      "Step:13900 , test loss : 0.6937772035598755\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:13950 , train loss : 0.6169281601905823\n",
      "Step:13950 , test loss : 0.6927648186683655\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14000 , train loss : 0.684222936630249\n",
      "Step:14000 , test loss : 0.6906928420066833\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14050 , train loss : 0.6815052032470703\n",
      "Step:14050 , test loss : 0.6920995712280273\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14100 , train loss : 0.679979145526886\n",
      "Step:14100 , test loss : 0.6921775341033936\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14150 , train loss : 0.6816789507865906\n",
      "Step:14150 , test loss : 0.6916247606277466\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14200 , train loss : 0.6718851923942566\n",
      "Step:14200 , test loss : 0.6933953762054443\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14250 , train loss : 0.6602494120597839\n",
      "Step:14250 , test loss : 0.6921029090881348\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14300 , train loss : 0.6601514220237732\n",
      "Step:14300 , test loss : 0.6913188695907593\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14350 , train loss : 0.6693041324615479\n",
      "Step:14350 , test loss : 0.6913956999778748\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14400 , train loss : 0.6581823229789734\n",
      "Step:14400 , test loss : 0.6926618814468384\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14450 , train loss : 0.6670870184898376\n",
      "Step:14450 , test loss : 0.6909540891647339\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14500 , train loss : 0.6870831251144409\n",
      "Step:14500 , test loss : 0.6963741779327393\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14550 , train loss : 0.6512483358383179\n",
      "Step:14550 , test loss : 0.6961228847503662\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14600 , train loss : 0.6713924407958984\n",
      "Step:14600 , test loss : 0.6916265487670898\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14650 , train loss : 0.7003659009933472\n",
      "Step:14650 , test loss : 0.6906905770301819\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14700 , train loss : 0.6613618731498718\n",
      "Step:14700 , test loss : 0.6920437812805176\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14750 , train loss : 0.681100606918335\n",
      "Step:14750 , test loss : 0.6927428841590881\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14800 , train loss : 0.6957479119300842\n",
      "Step:14800 , test loss : 0.6913752555847168\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14850 , train loss : 0.6720407605171204\n",
      "Step:14850 , test loss : 0.6906905174255371\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14900 , train loss : 0.6853563785552979\n",
      "Step:14900 , test loss : 0.6914845705032349\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:14950 , train loss : 0.6783432960510254\n",
      "Step:14950 , test loss : 0.6915262341499329\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15000 , train loss : 0.6577631831169128\n",
      "Step:15000 , test loss : 0.6928074955940247\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15050 , train loss : 0.703657865524292\n",
      "Step:15050 , test loss : 0.6909223794937134\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15100 , train loss : 0.7117774486541748\n",
      "Step:15100 , test loss : 0.6953050494194031\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15150 , train loss : 0.6806839108467102\n",
      "Step:15150 , test loss : 0.6917850375175476\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15200 , train loss : 0.6856030225753784\n",
      "Step:15200 , test loss : 0.6910604238510132\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15250 , train loss : 0.6656562089920044\n",
      "Step:15250 , test loss : 0.6924979090690613\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15300 , train loss : 0.6992023587226868\n",
      "Step:15300 , test loss : 0.6915246844291687\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15350 , train loss : 0.674555778503418\n",
      "Step:15350 , test loss : 0.6915834546089172\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15400 , train loss : 0.6892428994178772\n",
      "Step:15400 , test loss : 0.692237377166748\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15450 , train loss : 0.6443615555763245\n",
      "Step:15450 , test loss : 0.6917544603347778\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15500 , train loss : 0.6429352760314941\n",
      "Step:15500 , test loss : 0.6946824193000793\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15550 , train loss : 0.6759939789772034\n",
      "Step:15550 , test loss : 0.6944008469581604\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15600 , train loss : 0.658429741859436\n",
      "Step:15600 , test loss : 0.692661464214325\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15650 , train loss : 0.7088899612426758\n",
      "Step:15650 , test loss : 0.6919349431991577\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15700 , train loss : 0.6853489279747009\n",
      "Step:15700 , test loss : 0.691871702671051\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15750 , train loss : 0.7289307117462158\n",
      "Step:15750 , test loss : 0.6932072043418884\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15800 , train loss : 0.6914361715316772\n",
      "Step:15800 , test loss : 0.690818190574646\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15850 , train loss : 0.6931321620941162\n",
      "Step:15850 , test loss : 0.6924243569374084\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15900 , train loss : 0.6799988746643066\n",
      "Step:15900 , test loss : 0.6923184990882874\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:15950 , train loss : 0.6699379086494446\n",
      "Step:15950 , test loss : 0.6920592784881592\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16000 , train loss : 0.6901357769966125\n",
      "Step:16000 , test loss : 0.6937132477760315\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16050 , train loss : 0.7124133110046387\n",
      "Step:16050 , test loss : 0.6908778548240662\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16100 , train loss : 0.6893776059150696\n",
      "Step:16100 , test loss : 0.6924078464508057\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16150 , train loss : 0.7039374113082886\n",
      "Step:16150 , test loss : 0.6932888627052307\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16200 , train loss : 0.6800613403320312\n",
      "Step:16200 , test loss : 0.6923163533210754\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16250 , train loss : 0.6919559240341187\n",
      "Step:16250 , test loss : 0.6963440775871277\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16300 , train loss : 0.685941755771637\n",
      "Step:16300 , test loss : 0.6952462196350098\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16350 , train loss : 0.7057337164878845\n",
      "Step:16350 , test loss : 0.6939886212348938\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16400 , train loss : 0.6444594860076904\n",
      "Step:16400 , test loss : 0.6919397711753845\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16450 , train loss : 0.681685209274292\n",
      "Step:16450 , test loss : 0.6918857097625732\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16500 , train loss : 0.6458075046539307\n",
      "Step:16500 , test loss : 0.6915271282196045\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16550 , train loss : 0.6829180717468262\n",
      "Step:16550 , test loss : 0.6909058690071106\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16600 , train loss : 0.672515869140625\n",
      "Step:16600 , test loss : 0.6906972527503967\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16650 , train loss : 0.6551945805549622\n",
      "Step:16650 , test loss : 0.6934853196144104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16700 , train loss : 0.7104137539863586\n",
      "Step:16700 , test loss : 0.6925047039985657\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16750 , train loss : 0.6971288919448853\n",
      "Step:16750 , test loss : 0.6924013495445251\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16800 , train loss : 0.71773761510849\n",
      "Step:16800 , test loss : 0.691442608833313\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16850 , train loss : 0.7139682769775391\n",
      "Step:16850 , test loss : 0.6921765804290771\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16900 , train loss : 0.6854400634765625\n",
      "Step:16900 , test loss : 0.6914347410202026\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:16950 , train loss : 0.6667354702949524\n",
      "Step:16950 , test loss : 0.6910176873207092\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17000 , train loss : 0.6637203693389893\n",
      "Step:17000 , test loss : 0.690909743309021\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17050 , train loss : 0.6809619665145874\n",
      "Step:17050 , test loss : 0.6952929496765137\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17100 , train loss : 0.7096559405326843\n",
      "Step:17100 , test loss : 0.6955975890159607\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17150 , train loss : 0.7031657695770264\n",
      "Step:17150 , test loss : 0.6929640769958496\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17200 , train loss : 0.6945028305053711\n",
      "Step:17200 , test loss : 0.690868616104126\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17250 , train loss : 0.6582388281822205\n",
      "Step:17250 , test loss : 0.6924357414245605\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17300 , train loss : 0.6918970942497253\n",
      "Step:17300 , test loss : 0.6963199973106384\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17350 , train loss : 0.6907230019569397\n",
      "Step:17350 , test loss : 0.6943524479866028\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17400 , train loss : 0.6993147730827332\n",
      "Step:17400 , test loss : 0.6914514303207397\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17450 , train loss : 0.6774752736091614\n",
      "Step:17450 , test loss : 0.6920899748802185\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17500 , train loss : 0.6855529546737671\n",
      "Step:17500 , test loss : 0.6912854909896851\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17550 , train loss : 0.7157425880432129\n",
      "Step:17550 , test loss : 0.6911848187446594\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17600 , train loss : 0.6853700876235962\n",
      "Step:17600 , test loss : 0.6928350925445557\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17650 , train loss : 0.6951503753662109\n",
      "Step:17650 , test loss : 0.6941773891448975\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17700 , train loss : 0.681413471698761\n",
      "Step:17700 , test loss : 0.6912278532981873\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17750 , train loss : 0.6891883015632629\n",
      "Step:17750 , test loss : 0.6920850276947021\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17800 , train loss : 0.6804952621459961\n",
      "Step:17800 , test loss : 0.6908335089683533\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17850 , train loss : 0.705498993396759\n",
      "Step:17850 , test loss : 0.6937466859817505\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17900 , train loss : 0.6854736804962158\n",
      "Step:17900 , test loss : 0.6912865042686462\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:17950 , train loss : 0.6960034370422363\n",
      "Step:17950 , test loss : 0.691543698310852\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18000 , train loss : 0.6334891319274902\n",
      "Step:18000 , test loss : 0.6911910772323608\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18050 , train loss : 0.6771025657653809\n",
      "Step:18050 , test loss : 0.6930956244468689\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18100 , train loss : 0.7000871896743774\n",
      "Step:18100 , test loss : 0.693997859954834\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18150 , train loss : 0.6402673721313477\n",
      "Step:18150 , test loss : 0.6923378109931946\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18200 , train loss : 0.689071536064148\n",
      "Step:18200 , test loss : 0.6919980645179749\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18250 , train loss : 0.6900011897087097\n",
      "Step:18250 , test loss : 0.6933881044387817\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18300 , train loss : 0.63321453332901\n",
      "Step:18300 , test loss : 0.6930082440376282\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18350 , train loss : 0.6689026355743408\n",
      "Step:18350 , test loss : 0.692450225353241\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18400 , train loss : 0.6491178274154663\n",
      "Step:18400 , test loss : 0.6923766136169434\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18450 , train loss : 0.7141667008399963\n",
      "Step:18450 , test loss : 0.691002607345581\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18500 , train loss : 0.6923835277557373\n",
      "Step:18500 , test loss : 0.6915643215179443\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18550 , train loss : 0.6915881037712097\n",
      "Step:18550 , test loss : 0.6908650994300842\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18600 , train loss : 0.6792666912078857\n",
      "Step:18600 , test loss : 0.6938172578811646\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18650 , train loss : 0.6910396218299866\n",
      "Step:18650 , test loss : 0.6948022246360779\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18700 , train loss : 0.6988227963447571\n",
      "Step:18700 , test loss : 0.6929662227630615\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18750 , train loss : 0.6828932166099548\n",
      "Step:18750 , test loss : 0.6907815337181091\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18800 , train loss : 0.6890032887458801\n",
      "Step:18800 , test loss : 0.6918348073959351\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18850 , train loss : 0.7049932479858398\n",
      "Step:18850 , test loss : 0.6922352313995361\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18900 , train loss : 0.7495490908622742\n",
      "Step:18900 , test loss : 0.692377507686615\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:18950 , train loss : 0.6889688968658447\n",
      "Step:18950 , test loss : 0.6916345953941345\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19000 , train loss : 0.7101349830627441\n",
      "Step:19000 , test loss : 0.6926233768463135\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19050 , train loss : 0.6411917805671692\n",
      "Step:19050 , test loss : 0.690870463848114\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19100 , train loss : 0.678612232208252\n",
      "Step:19100 , test loss : 0.6916171312332153\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19150 , train loss : 0.6823707222938538\n",
      "Step:19150 , test loss : 0.6911619901657104\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19200 , train loss : 0.6792433857917786\n",
      "Step:19200 , test loss : 0.6937137842178345\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19250 , train loss : 0.6662765145301819\n",
      "Step:19250 , test loss : 0.6910564303398132\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19300 , train loss : 0.7308457493782043\n",
      "Step:19300 , test loss : 0.693280041217804\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19350 , train loss : 0.6808313727378845\n",
      "Step:19350 , test loss : 0.6916788220405579\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19400 , train loss : 0.6813656091690063\n",
      "Step:19400 , test loss : 0.6926844120025635\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19450 , train loss : 0.6973714232444763\n",
      "Step:19450 , test loss : 0.6924096941947937\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19500 , train loss : 0.7646119594573975\n",
      "Step:19500 , test loss : 0.6909902095794678\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19550 , train loss : 0.676510214805603\n",
      "Step:19550 , test loss : 0.6933817267417908\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19600 , train loss : 0.6812542080879211\n",
      "Step:19600 , test loss : 0.6926810145378113\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19650 , train loss : 0.6953346729278564\n",
      "Step:19650 , test loss : 0.6906900405883789\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19700 , train loss : 0.6887560486793518\n",
      "Step:19700 , test loss : 0.6907837390899658\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19750 , train loss : 0.703249454498291\n",
      "Step:19750 , test loss : 0.6916882991790771\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19800 , train loss : 0.700461208820343\n",
      "Step:19800 , test loss : 0.6919022798538208\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19850 , train loss : 0.6855700612068176\n",
      "Step:19850 , test loss : 0.691190242767334\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19900 , train loss : 0.6917709708213806\n",
      "Step:19900 , test loss : 0.6908661723136902\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:19950 , train loss : 0.7357133030891418\n",
      "Step:19950 , test loss : 0.6944449543952942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20000 , train loss : 0.6811973452568054\n",
      "Step:20000 , test loss : 0.6927298903465271\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20050 , train loss : 0.6692042350769043\n",
      "Step:20050 , test loss : 0.69229656457901\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20100 , train loss : 0.7537591457366943\n",
      "Step:20100 , test loss : 0.6911989450454712\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20150 , train loss : 0.6810053586959839\n",
      "Step:20150 , test loss : 0.6933779120445251\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20200 , train loss : 0.7037162184715271\n",
      "Step:20200 , test loss : 0.6916999220848083\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20250 , train loss : 0.698899507522583\n",
      "Step:20250 , test loss : 0.6913281679153442\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20300 , train loss : 0.653839111328125\n",
      "Step:20300 , test loss : 0.6933767199516296\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20350 , train loss : 0.6758859157562256\n",
      "Step:20350 , test loss : 0.6912489533424377\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20400 , train loss : 0.7669498920440674\n",
      "Step:20400 , test loss : 0.6919623017311096\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20450 , train loss : 0.6561098098754883\n",
      "Step:20450 , test loss : 0.6928602457046509\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20500 , train loss : 0.7001277208328247\n",
      "Step:20500 , test loss : 0.6914494037628174\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20550 , train loss : 0.6797959804534912\n",
      "Step:20550 , test loss : 0.692654013633728\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20600 , train loss : 0.6816112995147705\n",
      "Step:20600 , test loss : 0.6916285753250122\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20650 , train loss : 0.6790367960929871\n",
      "Step:20650 , test loss : 0.6913815140724182\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20700 , train loss : 0.6801931262016296\n",
      "Step:20700 , test loss : 0.6920784115791321\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20750 , train loss : 0.7122839093208313\n",
      "Step:20750 , test loss : 0.6911647319793701\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20800 , train loss : 0.6771600246429443\n",
      "Step:20800 , test loss : 0.6925815343856812\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20850 , train loss : 0.6568863987922668\n",
      "Step:20850 , test loss : 0.692937970161438\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20900 , train loss : 0.6774806380271912\n",
      "Step:20900 , test loss : 0.6923521757125854\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:20950 , train loss : 0.6855002045631409\n",
      "Step:20950 , test loss : 0.6939130425453186\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21000 , train loss : 0.7154035568237305\n",
      "Step:21000 , test loss : 0.6907198429107666\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21050 , train loss : 0.7143282890319824\n",
      "Step:21050 , test loss : 0.6940801739692688\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21100 , train loss : 0.7041209936141968\n",
      "Step:21100 , test loss : 0.6920614242553711\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21150 , train loss : 0.7353726029396057\n",
      "Step:21150 , test loss : 0.6911718845367432\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21200 , train loss : 0.6591938734054565\n",
      "Step:21200 , test loss : 0.693747878074646\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21250 , train loss : 0.716741144657135\n",
      "Step:21250 , test loss : 0.6921234130859375\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21300 , train loss : 0.6024913787841797\n",
      "Step:21300 , test loss : 0.6949058175086975\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21350 , train loss : 0.6811433434486389\n",
      "Step:21350 , test loss : 0.693115234375\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21400 , train loss : 0.6926428079605103\n",
      "Step:21400 , test loss : 0.6970474123954773\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21450 , train loss : 0.7472783923149109\n",
      "Step:21450 , test loss : 0.6926009058952332\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21500 , train loss : 0.6737431287765503\n",
      "Step:21500 , test loss : 0.6921849250793457\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21550 , train loss : 0.6736438274383545\n",
      "Step:21550 , test loss : 0.6910891532897949\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21600 , train loss : 0.7285555005073547\n",
      "Step:21600 , test loss : 0.6929771900177002\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21650 , train loss : 0.7159169912338257\n",
      "Step:21650 , test loss : 0.6946216225624084\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21700 , train loss : 0.676882266998291\n",
      "Step:21700 , test loss : 0.6928616166114807\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21750 , train loss : 0.6330664157867432\n",
      "Step:21750 , test loss : 0.6931910514831543\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21800 , train loss : 0.6748602986335754\n",
      "Step:21800 , test loss : 0.6921414136886597\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21850 , train loss : 0.7249449491500854\n",
      "Step:21850 , test loss : 0.6955744028091431\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21900 , train loss : 0.7006704807281494\n",
      "Step:21900 , test loss : 0.6920225620269775\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:21950 , train loss : 0.6710400581359863\n",
      "Step:21950 , test loss : 0.6909698247909546\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22000 , train loss : 0.656671941280365\n",
      "Step:22000 , test loss : 0.6911409497261047\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22050 , train loss : 0.6173142790794373\n",
      "Step:22050 , test loss : 0.6926195025444031\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22100 , train loss : 0.6784499883651733\n",
      "Step:22100 , test loss : 0.6916943788528442\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22150 , train loss : 0.6812618970870972\n",
      "Step:22150 , test loss : 0.6927949786186218\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22200 , train loss : 0.6275580525398254\n",
      "Step:22200 , test loss : 0.6942446231842041\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22250 , train loss : 0.7118526101112366\n",
      "Step:22250 , test loss : 0.6917926073074341\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22300 , train loss : 0.6815470457077026\n",
      "Step:22300 , test loss : 0.6917673349380493\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22350 , train loss : 0.7180452346801758\n",
      "Step:22350 , test loss : 0.691448450088501\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22400 , train loss : 0.6933924555778503\n",
      "Step:22400 , test loss : 0.692598819732666\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22450 , train loss : 0.6810951232910156\n",
      "Step:22450 , test loss : 0.6925938129425049\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22500 , train loss : 0.6354589462280273\n",
      "Step:22500 , test loss : 0.6910545229911804\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22550 , train loss : 0.6627081036567688\n",
      "Step:22550 , test loss : 0.6941579580307007\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22600 , train loss : 0.6989386081695557\n",
      "Step:22600 , test loss : 0.6933103203773499\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22650 , train loss : 0.663710355758667\n",
      "Step:22650 , test loss : 0.691287636756897\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22700 , train loss : 0.6767951846122742\n",
      "Step:22700 , test loss : 0.6909990906715393\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22750 , train loss : 0.688813328742981\n",
      "Step:22750 , test loss : 0.6907362937927246\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22800 , train loss : 0.6793310046195984\n",
      "Step:22800 , test loss : 0.6937081813812256\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22850 , train loss : 0.6668963432312012\n",
      "Step:22850 , test loss : 0.694037675857544\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22900 , train loss : 0.6591288447380066\n",
      "Step:22900 , test loss : 0.6914756298065186\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:22950 , train loss : 0.7010712623596191\n",
      "Step:22950 , test loss : 0.6922920346260071\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23000 , train loss : 0.6961209177970886\n",
      "Step:23000 , test loss : 0.6915386319160461\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23050 , train loss : 0.6662787199020386\n",
      "Step:23050 , test loss : 0.6921327114105225\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23100 , train loss : 0.6807770729064941\n",
      "Step:23100 , test loss : 0.6915227174758911\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23150 , train loss : 0.7015088200569153\n",
      "Step:23150 , test loss : 0.6909921765327454\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23200 , train loss : 0.6924092173576355\n",
      "Step:23200 , test loss : 0.6914610862731934\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23250 , train loss : 0.7016628384590149\n",
      "Step:23250 , test loss : 0.692567765712738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23300 , train loss : 0.68532395362854\n",
      "Step:23300 , test loss : 0.6917965412139893\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23350 , train loss : 0.6987216472625732\n",
      "Step:23350 , test loss : 0.6931845545768738\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23400 , train loss : 0.6535235047340393\n",
      "Step:23400 , test loss : 0.6944162249565125\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23450 , train loss : 0.663298487663269\n",
      "Step:23450 , test loss : 0.6921026706695557\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23500 , train loss : 0.6690099835395813\n",
      "Step:23500 , test loss : 0.6907201409339905\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23550 , train loss : 0.6568279266357422\n",
      "Step:23550 , test loss : 0.69325190782547\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23600 , train loss : 0.6713498830795288\n",
      "Step:23600 , test loss : 0.6916895508766174\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23650 , train loss : 0.6580175757408142\n",
      "Step:23650 , test loss : 0.691999614238739\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23700 , train loss : 0.6650713086128235\n",
      "Step:23700 , test loss : 0.6912865042686462\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23750 , train loss : 0.7010399103164673\n",
      "Step:23750 , test loss : 0.6920059323310852\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23800 , train loss : 0.6817887425422668\n",
      "Step:23800 , test loss : 0.6917464733123779\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23850 , train loss : 0.7047181129455566\n",
      "Step:23850 , test loss : 0.6933534145355225\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23900 , train loss : 0.6887550950050354\n",
      "Step:23900 , test loss : 0.6909089088439941\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:23950 , train loss : 0.6892658472061157\n",
      "Step:23950 , test loss : 0.6923010349273682\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24000 , train loss : 0.7009215354919434\n",
      "Step:24000 , test loss : 0.6920120716094971\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24050 , train loss : 0.6723250150680542\n",
      "Step:24050 , test loss : 0.6931599378585815\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24100 , train loss : 0.6814900636672974\n",
      "Step:24100 , test loss : 0.6922317743301392\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24150 , train loss : 0.6850640177726746\n",
      "Step:24150 , test loss : 0.6907415986061096\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24200 , train loss : 0.6942580342292786\n",
      "Step:24200 , test loss : 0.6926930546760559\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24250 , train loss : 0.6916224360466003\n",
      "Step:24250 , test loss : 0.69090336561203\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24300 , train loss : 0.6792570948600769\n",
      "Step:24300 , test loss : 0.6943386793136597\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24350 , train loss : 0.6733657121658325\n",
      "Step:24350 , test loss : 0.6924404501914978\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24400 , train loss : 0.6808934211730957\n",
      "Step:24400 , test loss : 0.6944467425346375\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24450 , train loss : 0.6333906650543213\n",
      "Step:24450 , test loss : 0.6933345198631287\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24500 , train loss : 0.6862276196479797\n",
      "Step:24500 , test loss : 0.6907693147659302\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24550 , train loss : 0.6943973302841187\n",
      "Step:24550 , test loss : 0.6931532621383667\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24600 , train loss : 0.653932511806488\n",
      "Step:24600 , test loss : 0.6946820616722107\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24650 , train loss : 0.6810686588287354\n",
      "Step:24650 , test loss : 0.6907991170883179\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24700 , train loss : 0.6857414245605469\n",
      "Step:24700 , test loss : 0.6943304538726807\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24750 , train loss : 0.6975929141044617\n",
      "Step:24750 , test loss : 0.6909485459327698\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24800 , train loss : 0.6902968287467957\n",
      "Step:24800 , test loss : 0.6939809322357178\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24850 , train loss : 0.6813310980796814\n",
      "Step:24850 , test loss : 0.6976291537284851\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24900 , train loss : 0.706261932849884\n",
      "Step:24900 , test loss : 0.6943717002868652\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:24950 , train loss : 0.703396201133728\n",
      "Step:24950 , test loss : 0.6963322162628174\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25000 , train loss : 0.7047340273857117\n",
      "Step:25000 , test loss : 0.69231778383255\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25050 , train loss : 0.704552948474884\n",
      "Step:25050 , test loss : 0.6907610297203064\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25100 , train loss : 0.678320586681366\n",
      "Step:25100 , test loss : 0.6907365322113037\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25150 , train loss : 0.6737015843391418\n",
      "Step:25150 , test loss : 0.6920156478881836\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25200 , train loss : 0.7295483946800232\n",
      "Step:25200 , test loss : 0.6931551098823547\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25250 , train loss : 0.6808905601501465\n",
      "Step:25250 , test loss : 0.6944230198860168\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25300 , train loss : 0.6810395121574402\n",
      "Step:25300 , test loss : 0.6907499432563782\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25350 , train loss : 0.6792100071907043\n",
      "Step:25350 , test loss : 0.6945613622665405\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25400 , train loss : 0.6981452703475952\n",
      "Step:25400 , test loss : 0.6911112666130066\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25450 , train loss : 0.7088909149169922\n",
      "Step:25450 , test loss : 0.6936161518096924\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25500 , train loss : 0.6973504424095154\n",
      "Step:25500 , test loss : 0.6909223198890686\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25550 , train loss : 0.6490092277526855\n",
      "Step:25550 , test loss : 0.6965512633323669\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25600 , train loss : 0.7099782228469849\n",
      "Step:25600 , test loss : 0.6925727725028992\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25650 , train loss : 0.680565357208252\n",
      "Step:25650 , test loss : 0.6918134689331055\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25700 , train loss : 0.692614734172821\n",
      "Step:25700 , test loss : 0.691809892654419\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25750 , train loss : 0.6853434443473816\n",
      "Step:25750 , test loss : 0.691891610622406\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25800 , train loss : 0.7000436782836914\n",
      "Step:25800 , test loss : 0.6917116045951843\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25850 , train loss : 0.6869447231292725\n",
      "Step:25850 , test loss : 0.6906827688217163\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25900 , train loss : 0.6986262798309326\n",
      "Step:25900 , test loss : 0.6912643313407898\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:25950 , train loss : 0.7534956932067871\n",
      "Step:25950 , test loss : 0.6927876472473145\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26000 , train loss : 0.7278193831443787\n",
      "Step:26000 , test loss : 0.6951714754104614\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26050 , train loss : 0.6888446807861328\n",
      "Step:26050 , test loss : 0.6907005906105042\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26100 , train loss : 0.6254677772521973\n",
      "Step:26100 , test loss : 0.6909486651420593\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26150 , train loss : 0.6452593207359314\n",
      "Step:26150 , test loss : 0.6922401785850525\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26200 , train loss : 0.6916533708572388\n",
      "Step:26200 , test loss : 0.695164680480957\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26250 , train loss : 0.7152614593505859\n",
      "Step:26250 , test loss : 0.6912010312080383\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26300 , train loss : 0.6924896836280823\n",
      "Step:26300 , test loss : 0.6918081045150757\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26350 , train loss : 0.6887881755828857\n",
      "Step:26350 , test loss : 0.6907622218132019\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26400 , train loss : 0.6371744871139526\n",
      "Step:26400 , test loss : 0.6928258538246155\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26450 , train loss : 0.7026931047439575\n",
      "Step:26450 , test loss : 0.6908235549926758\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26500 , train loss : 0.6853266954421997\n",
      "Step:26500 , test loss : 0.6923110485076904\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26550 , train loss : 0.6968604922294617\n",
      "Step:26550 , test loss : 0.6908217072486877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26600 , train loss : 0.6991801261901855\n",
      "Step:26600 , test loss : 0.6912767291069031\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26650 , train loss : 0.697735071182251\n",
      "Step:26650 , test loss : 0.6922649145126343\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26700 , train loss : 0.7182877659797668\n",
      "Step:26700 , test loss : 0.6915201544761658\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26750 , train loss : 0.6638100147247314\n",
      "Step:26750 , test loss : 0.6919122934341431\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26800 , train loss : 0.6681437492370605\n",
      "Step:26800 , test loss : 0.691412091255188\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26850 , train loss : 0.6466462016105652\n",
      "Step:26850 , test loss : 0.6913952231407166\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26900 , train loss : 0.6983193159103394\n",
      "Step:26900 , test loss : 0.6931325197219849\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:26950 , train loss : 0.6933987736701965\n",
      "Step:26950 , test loss : 0.692541778087616\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27000 , train loss : 0.6311253309249878\n",
      "Step:27000 , test loss : 0.6942331194877625\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27050 , train loss : 0.6608392000198364\n",
      "Step:27050 , test loss : 0.6958056092262268\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27100 , train loss : 0.6893898844718933\n",
      "Step:27100 , test loss : 0.6923435926437378\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27150 , train loss : 0.6136776804924011\n",
      "Step:27150 , test loss : 0.6930619478225708\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27200 , train loss : 0.6889356374740601\n",
      "Step:27200 , test loss : 0.6918197274208069\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27250 , train loss : 0.6667431592941284\n",
      "Step:27250 , test loss : 0.6918467879295349\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27300 , train loss : 0.6210600733757019\n",
      "Step:27300 , test loss : 0.6920892596244812\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27350 , train loss : 0.680998682975769\n",
      "Step:27350 , test loss : 0.6940047740936279\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27400 , train loss : 0.6854792833328247\n",
      "Step:27400 , test loss : 0.6912208199501038\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27450 , train loss : 0.6823374629020691\n",
      "Step:27450 , test loss : 0.6908724308013916\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27500 , train loss : 0.6925030946731567\n",
      "Step:27500 , test loss : 0.691896915435791\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27550 , train loss : 0.6746862530708313\n",
      "Step:27550 , test loss : 0.6906967759132385\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27600 , train loss : 0.7420223355293274\n",
      "Step:27600 , test loss : 0.6919158101081848\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27650 , train loss : 0.6996035575866699\n",
      "Step:27650 , test loss : 0.6908367276191711\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27700 , train loss : 0.6663923263549805\n",
      "Step:27700 , test loss : 0.6920133233070374\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27750 , train loss : 0.5806003212928772\n",
      "Step:27750 , test loss : 0.6943097114562988\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27800 , train loss : 0.6626207828521729\n",
      "Step:27800 , test loss : 0.6922475695610046\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27850 , train loss : 0.6787104606628418\n",
      "Step:27850 , test loss : 0.691354513168335\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27900 , train loss : 0.6971510052680969\n",
      "Step:27900 , test loss : 0.6908802390098572\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:27950 , train loss : 0.6792960166931152\n",
      "Step:27950 , test loss : 0.6910666227340698\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28000 , train loss : 0.6949588060379028\n",
      "Step:28000 , test loss : 0.6989695429801941\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28050 , train loss : 0.699747622013092\n",
      "Step:28050 , test loss : 0.6917325258255005\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28100 , train loss : 0.6764242649078369\n",
      "Step:28100 , test loss : 0.6935453414916992\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28150 , train loss : 0.7104809284210205\n",
      "Step:28150 , test loss : 0.6983698010444641\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28200 , train loss : 0.6530575752258301\n",
      "Step:28200 , test loss : 0.6909386515617371\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28250 , train loss : 0.6859636306762695\n",
      "Step:28250 , test loss : 0.6908331513404846\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28300 , train loss : 0.6808905601501465\n",
      "Step:28300 , test loss : 0.6941263675689697\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28350 , train loss : 0.6992299556732178\n",
      "Step:28350 , test loss : 0.6916236281394958\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28400 , train loss : 0.7117823362350464\n",
      "Step:28400 , test loss : 0.691927433013916\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28450 , train loss : 0.6818915605545044\n",
      "Step:28450 , test loss : 0.6918008327484131\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28500 , train loss : 0.6794560551643372\n",
      "Step:28500 , test loss : 0.6932910680770874\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28550 , train loss : 0.6732562780380249\n",
      "Step:28550 , test loss : 0.6926846504211426\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28600 , train loss : 0.6992104053497314\n",
      "Step:28600 , test loss : 0.6931577920913696\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28650 , train loss : 0.6622738838195801\n",
      "Step:28650 , test loss : 0.6915431022644043\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28700 , train loss : 0.6627228260040283\n",
      "Step:28700 , test loss : 0.6935369968414307\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28750 , train loss : 0.6866061687469482\n",
      "Step:28750 , test loss : 0.6906900405883789\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28800 , train loss : 0.6816987991333008\n",
      "Step:28800 , test loss : 0.6911225318908691\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28850 , train loss : 0.6760677695274353\n",
      "Step:28850 , test loss : 0.6911317110061646\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28900 , train loss : 0.6715313196182251\n",
      "Step:28900 , test loss : 0.6917001008987427\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:28950 , train loss : 0.679589033126831\n",
      "Step:28950 , test loss : 0.6928367614746094\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29000 , train loss : 0.6978354454040527\n",
      "Step:29000 , test loss : 0.6928200721740723\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29050 , train loss : 0.6724458336830139\n",
      "Step:29050 , test loss : 0.6929836869239807\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29100 , train loss : 0.6798112988471985\n",
      "Step:29100 , test loss : 0.6923711895942688\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29150 , train loss : 0.7089000344276428\n",
      "Step:29150 , test loss : 0.6917687654495239\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29200 , train loss : 0.6759131550788879\n",
      "Step:29200 , test loss : 0.6946347951889038\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29250 , train loss : 0.7366361618041992\n",
      "Step:29250 , test loss : 0.6915224194526672\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29300 , train loss : 0.6853378415107727\n",
      "Step:29300 , test loss : 0.691855251789093\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29350 , train loss : 0.6985681056976318\n",
      "Step:29350 , test loss : 0.6930246353149414\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29400 , train loss : 0.6432013511657715\n",
      "Step:29400 , test loss : 0.6918668746948242\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29450 , train loss : 0.6979999542236328\n",
      "Step:29450 , test loss : 0.6929921507835388\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29500 , train loss : 0.6372748613357544\n",
      "Step:29500 , test loss : 0.6962700486183167\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29550 , train loss : 0.6795899868011475\n",
      "Step:29550 , test loss : 0.6932951807975769\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29600 , train loss : 0.6806485056877136\n",
      "Step:29600 , test loss : 0.6908587217330933\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29650 , train loss : 0.6817430257797241\n",
      "Step:29650 , test loss : 0.6918867826461792\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29700 , train loss : 0.7199764847755432\n",
      "Step:29700 , test loss : 0.6917104721069336\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29750 , train loss : 0.6925408840179443\n",
      "Step:29750 , test loss : 0.6918660998344421\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29800 , train loss : 0.6853313446044922\n",
      "Step:29800 , test loss : 0.6918840408325195\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29850 , train loss : 0.6568026542663574\n",
      "Step:29850 , test loss : 0.6931722164154053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29900 , train loss : 0.6683348417282104\n",
      "Step:29900 , test loss : 0.6929360032081604\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:29950 , train loss : 0.6744080781936646\n",
      "Step:29950 , test loss : 0.6917780041694641\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30000 , train loss : 0.7016000747680664\n",
      "Step:30000 , test loss : 0.6923121809959412\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30050 , train loss : 0.6931736469268799\n",
      "Step:30050 , test loss : 0.6922388076782227\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30100 , train loss : 0.6595086455345154\n",
      "Step:30100 , test loss : 0.6932899355888367\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30150 , train loss : 0.6798127293586731\n",
      "Step:30150 , test loss : 0.6924237608909607\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30200 , train loss : 0.6809576749801636\n",
      "Step:30200 , test loss : 0.6933832764625549\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30250 , train loss : 0.6958553791046143\n",
      "Step:30250 , test loss : 0.6913844347000122\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30300 , train loss : 0.7281662821769714\n",
      "Step:30300 , test loss : 0.6932984590530396\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30350 , train loss : 0.6553471088409424\n",
      "Step:30350 , test loss : 0.6930743455886841\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30400 , train loss : 0.6744657158851624\n",
      "Step:30400 , test loss : 0.6909080147743225\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30450 , train loss : 0.6576123833656311\n",
      "Step:30450 , test loss : 0.6926966905593872\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30500 , train loss : 0.6812725067138672\n",
      "Step:30500 , test loss : 0.692874550819397\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30550 , train loss : 0.7065246105194092\n",
      "Step:30550 , test loss : 0.6945981383323669\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30600 , train loss : 0.6325792074203491\n",
      "Step:30600 , test loss : 0.693270742893219\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30650 , train loss : 0.6888586282730103\n",
      "Step:30650 , test loss : 0.6915684342384338\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30700 , train loss : 0.6955212950706482\n",
      "Step:30700 , test loss : 0.6940620541572571\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30750 , train loss : 0.7006886601448059\n",
      "Step:30750 , test loss : 0.6918405294418335\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30800 , train loss : 0.6977506279945374\n",
      "Step:30800 , test loss : 0.6927168369293213\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30850 , train loss : 0.7050876021385193\n",
      "Step:30850 , test loss : 0.6911752223968506\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30900 , train loss : 0.6359068751335144\n",
      "Step:30900 , test loss : 0.6926873326301575\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:30950 , train loss : 0.6973059773445129\n",
      "Step:30950 , test loss : 0.690980076789856\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31000 , train loss : 0.6792353987693787\n",
      "Step:31000 , test loss : 0.6912224888801575\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31050 , train loss : 0.6620299816131592\n",
      "Step:31050 , test loss : 0.6918442249298096\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31100 , train loss : 0.6811438798904419\n",
      "Step:31100 , test loss : 0.6930835247039795\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31150 , train loss : 0.6855747699737549\n",
      "Step:31150 , test loss : 0.6911882162094116\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31200 , train loss : 0.6998950839042664\n",
      "Step:31200 , test loss : 0.6917677521705627\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31250 , train loss : 0.6763864755630493\n",
      "Step:31250 , test loss : 0.6910964846611023\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31300 , train loss : 0.6967445015907288\n",
      "Step:31300 , test loss : 0.6917567849159241\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31350 , train loss : 0.7305122017860413\n",
      "Step:31350 , test loss : 0.6934024095535278\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31400 , train loss : 0.6966501474380493\n",
      "Step:31400 , test loss : 0.6917241215705872\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31450 , train loss : 0.6856155395507812\n",
      "Step:31450 , test loss : 0.6912180185317993\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31500 , train loss : 0.6392510533332825\n",
      "Step:31500 , test loss : 0.6923187375068665\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31550 , train loss : 0.6721103191375732\n",
      "Step:31550 , test loss : 0.6933556199073792\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31600 , train loss : 0.7012139558792114\n",
      "Step:31600 , test loss : 0.6910167336463928\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31650 , train loss : 0.6660333871841431\n",
      "Step:31650 , test loss : 0.6910449862480164\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31700 , train loss : 0.6919740438461304\n",
      "Step:31700 , test loss : 0.6910107135772705\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31750 , train loss : 0.666305422782898\n",
      "Step:31750 , test loss : 0.6942283511161804\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31800 , train loss : 0.679951012134552\n",
      "Step:31800 , test loss : 0.6924592852592468\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31850 , train loss : 0.6708351969718933\n",
      "Step:31850 , test loss : 0.6950516104698181\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31900 , train loss : 0.6719542741775513\n",
      "Step:31900 , test loss : 0.6915076375007629\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:31950 , train loss : 0.7018752694129944\n",
      "Step:31950 , test loss : 0.6928273439407349\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32000 , train loss : 0.7082473039627075\n",
      "Step:32000 , test loss : 0.6921199560165405\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32050 , train loss : 0.7060508728027344\n",
      "Step:32050 , test loss : 0.6925989389419556\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32100 , train loss : 0.7049906253814697\n",
      "Step:32100 , test loss : 0.6940042972564697\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32150 , train loss : 0.6605144143104553\n",
      "Step:32150 , test loss : 0.6918920278549194\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32200 , train loss : 0.6923495531082153\n",
      "Step:32200 , test loss : 0.6913946270942688\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32250 , train loss : 0.6377742886543274\n",
      "Step:32250 , test loss : 0.6909797191619873\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32300 , train loss : 0.7056217193603516\n",
      "Step:32300 , test loss : 0.6923174262046814\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32350 , train loss : 0.6858686208724976\n",
      "Step:32350 , test loss : 0.6908848285675049\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32400 , train loss : 0.679206371307373\n",
      "Step:32400 , test loss : 0.6946178078651428\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32450 , train loss : 0.6894189119338989\n",
      "Step:32450 , test loss : 0.6922110319137573\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32500 , train loss : 0.673386812210083\n",
      "Step:32500 , test loss : 0.6919496655464172\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32550 , train loss : 0.787865161895752\n",
      "Step:32550 , test loss : 0.6935577988624573\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32600 , train loss : 0.6808900237083435\n",
      "Step:32600 , test loss : 0.6941737532615662\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32650 , train loss : 0.6820158958435059\n",
      "Step:32650 , test loss : 0.6913090944290161\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32700 , train loss : 0.680341899394989\n",
      "Step:32700 , test loss : 0.6919659376144409\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32750 , train loss : 0.6927868723869324\n",
      "Step:32750 , test loss : 0.6920344829559326\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32800 , train loss : 0.6853305101394653\n",
      "Step:32800 , test loss : 0.6925768852233887\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32850 , train loss : 0.6989333033561707\n",
      "Step:32850 , test loss : 0.6914469599723816\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32900 , train loss : 0.6815932393074036\n",
      "Step:32900 , test loss : 0.691846489906311\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:32950 , train loss : 0.6809714436531067\n",
      "Step:32950 , test loss : 0.6907767057418823\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33000 , train loss : 0.6562698483467102\n",
      "Step:33000 , test loss : 0.693180501461029\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33050 , train loss : 0.6919194459915161\n",
      "Step:33050 , test loss : 0.6912320852279663\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33100 , train loss : 0.6887567639350891\n",
      "Step:33100 , test loss : 0.6910476684570312\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33150 , train loss : 0.7463152408599854\n",
      "Step:33150 , test loss : 0.6921804547309875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33200 , train loss : 0.6667815446853638\n",
      "Step:33200 , test loss : 0.6918557286262512\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33250 , train loss : 0.6685968041419983\n",
      "Step:33250 , test loss : 0.6914569139480591\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33300 , train loss : 0.6797025799751282\n",
      "Step:33300 , test loss : 0.692864179611206\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33350 , train loss : 0.6888158321380615\n",
      "Step:33350 , test loss : 0.6913224458694458\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33400 , train loss : 0.6916275024414062\n",
      "Step:33400 , test loss : 0.6962272524833679\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33450 , train loss : 0.7680370807647705\n",
      "Step:33450 , test loss : 0.6922317147254944\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33500 , train loss : 0.685316801071167\n",
      "Step:33500 , test loss : 0.6919141411781311\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33550 , train loss : 0.6961526274681091\n",
      "Step:33550 , test loss : 0.6914881467819214\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33600 , train loss : 0.7290937900543213\n",
      "Step:33600 , test loss : 0.6906970739364624\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33650 , train loss : 0.7197645902633667\n",
      "Step:33650 , test loss : 0.6923858523368835\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33700 , train loss : 0.7045496106147766\n",
      "Step:33700 , test loss : 0.6934175491333008\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33750 , train loss : 0.6792425513267517\n",
      "Step:33750 , test loss : 0.6943970918655396\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33800 , train loss : 0.7168372869491577\n",
      "Step:33800 , test loss : 0.6929086446762085\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33850 , train loss : 0.6755257844924927\n",
      "Step:33850 , test loss : 0.6969816088676453\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33900 , train loss : 0.6559560298919678\n",
      "Step:33900 , test loss : 0.6932956576347351\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:33950 , train loss : 0.7003412842750549\n",
      "Step:33950 , test loss : 0.6939691305160522\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34000 , train loss : 0.6854192018508911\n",
      "Step:34000 , test loss : 0.6932234764099121\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34050 , train loss : 0.7737715244293213\n",
      "Step:34050 , test loss : 0.6925556659698486\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34100 , train loss : 0.6757748126983643\n",
      "Step:34100 , test loss : 0.6952431201934814\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34150 , train loss : 0.6664069890975952\n",
      "Step:34150 , test loss : 0.6919233202934265\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34200 , train loss : 0.6803312301635742\n",
      "Step:34200 , test loss : 0.6918666958808899\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34250 , train loss : 0.6950666904449463\n",
      "Step:34250 , test loss : 0.6911303997039795\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34300 , train loss : 0.7006276249885559\n",
      "Step:34300 , test loss : 0.6945473551750183\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34350 , train loss : 0.6492869257926941\n",
      "Step:34350 , test loss : 0.6911960244178772\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34400 , train loss : 0.6610372066497803\n",
      "Step:34400 , test loss : 0.6909668445587158\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34450 , train loss : 0.6784042119979858\n",
      "Step:34450 , test loss : 0.6917528510093689\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34500 , train loss : 0.6999663710594177\n",
      "Step:34500 , test loss : 0.6916915774345398\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34550 , train loss : 0.6765429377555847\n",
      "Step:34550 , test loss : 0.6934106945991516\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34600 , train loss : 0.7078697085380554\n",
      "Step:34600 , test loss : 0.6906961798667908\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34650 , train loss : 0.7031205296516418\n",
      "Step:34650 , test loss : 0.6928280591964722\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34700 , train loss : 0.6888241767883301\n",
      "Step:34700 , test loss : 0.691105306148529\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34750 , train loss : 0.6726104021072388\n",
      "Step:34750 , test loss : 0.6913942098617554\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34800 , train loss : 0.6253650188446045\n",
      "Step:34800 , test loss : 0.6917208433151245\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34850 , train loss : 0.6856589317321777\n",
      "Step:34850 , test loss : 0.6943851113319397\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34900 , train loss : 0.6921391487121582\n",
      "Step:34900 , test loss : 0.6963086724281311\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:34950 , train loss : 0.6546574831008911\n",
      "Step:34950 , test loss : 0.693677544593811\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:35000 , train loss : 0.6992591619491577\n",
      "Step:35000 , test loss : 0.6938126683235168\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:35050 , train loss : 0.6584859490394592\n",
      "Step:35050 , test loss : 0.6932939291000366\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:35100 , train loss : 0.6590712666511536\n",
      "Step:35100 , test loss : 0.6924014687538147\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:35150 , train loss : 0.6573621034622192\n",
      "Step:35150 , test loss : 0.6912623047828674\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:35200 , train loss : 0.715711236000061\n",
      "Step:35200 , test loss : 0.6927281618118286\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:35250 , train loss : 0.7666662335395813\n",
      "Step:35250 , test loss : 0.6920889616012573\n",
      "# Save into ./models/ckpt_model/ckpt\n",
      "Step:35300 , train loss : 0.6779524683952332\n",
      "Step:35300 , test loss : 0.6917979121208191\n",
      "# Save into ./models/ckpt_model/ckpt\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = './models/ckpt_model/ckpt'\n",
    "batch_size = 64\n",
    "epoches = 5000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def generate_data(x,y,batch_size,epoches,shuffle=False):\n",
    "    data_len = len(x)\n",
    "    batch_num = data_len // batch_size + 1\n",
    "    for _ in range(epoches):\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(np.arange(data_len))\n",
    "            x = x[indices]\n",
    "            y = y[indices]\n",
    "        for batch in range(batch_num):\n",
    "            start_index = batch * batch_size\n",
    "            end_index = min(data_len, (batch + 1) * batch_size)\n",
    "            yield x[start_index:end_index], y[start_index:end_index]\n",
    "    \n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "input_x = np.array(list(Data['allVec']))\n",
    "input_y = np.array(list(Data['y_']))[:0]\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    model = DeepModel(learning_rate=1e-1)\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    ckpt_path = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    \n",
    "    if ckpt_path is None:\n",
    "        print('# Initilize model ...')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        saver.restore(sess,ckpt_path)\n",
    "    sess.run(model.global_step)\n",
    "    for x,y in generate_data(x_train,y_train,batch_size,epoches,True):\n",
    "        feed_dict = {\n",
    "            model.input_x : x,\n",
    "            model.input_y : y,\n",
    "            model.keep_dropout_rate : 0.8\n",
    "        }\n",
    "        _, loss, gs = sess.run([model.train_op,model.loss,model.global_step],feed_dict=feed_dict)\n",
    "        \n",
    "        if gs % 50 ==0:\n",
    "            print('Step:{} , train loss : {}'.format(gs,loss))\n",
    "            loss = sess.run(model.loss,feed_dict={model.input_x:x_test,model.input_y:y_test,model.keep_dropout_rate:1.0})\n",
    "            print('Step:{} , test loss : {}'.format(gs,loss))\n",
    "            saver.save(sess,ckpt_dir)\n",
    "            print('# Save into {}'.format(ckpt_dir))\n",
    "    print(\"# Finish \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor-t",
   "language": "python",
   "name": "tensor-t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
