{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 1 empty titles..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\ipykernel_launcher.py:116: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish build word_vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\tensorflow-t\\lib\\site-packages\\ipykernel_launcher.py:179: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Data1 = pd.read_csv('./data/TRAINSET_NEWS.csv')\n",
    "Data2 = pd.read_csv('./data/TRAINSET_STOCK.csv')\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    '''获取停用词'''\n",
    "    stop_words = []\n",
    "    for file in os.listdir('./stopwords'):\n",
    "        with open('./stopwords/{}'.format(file),encoding='utf8') as f:\n",
    "            stop_words.extend(f.read().splitlines())\n",
    "    return set(stop_words)\n",
    "\n",
    "stop_words = get_stopwords()\n",
    "\n",
    "def clear_data(data,is_nan=0):\n",
    "    # 注意这里有nan数据\n",
    "    '''数据的清洗'''\n",
    "    if data is np.nan:\n",
    "        data = ''\n",
    "        is_nan += 1\n",
    "        print('\\rHere are {} empty titles...'.format(is_nan),end='')\n",
    "    data = re.sub(r'[1-9]+月[0-9]+日至[0-9]+日','',data)\n",
    "    data = re.sub(r'(当地时间)*[0-9]+日','',data)\n",
    "    data = re.sub(r'[1-9]+月[0-9]+日([上|下]午)*','',data)\n",
    "    data = re.sub(r'[0-9]+日([上|下]午)*','',data)\n",
    "    data = re.sub(r'[0-9]+时[0-9]+分(许)*','',data)\n",
    "    data = re.sub(r'([0-9]+日)*([上|下]午)*[0-9]+时[0-9]+分(许)*','',data)\n",
    "    data = re.sub(r'[a-zA-Z0-9]+','',data)\n",
    "    data = re.sub(r'[,.，。、!:\"\\'：《》【】’%‘“”)(（·）—]','',data)\n",
    "    data = re.sub(r'\\s+',' ',data)\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    '''结巴分词'''\n",
    "    data = jieba.cut(data)\n",
    "    # todo token的时候进行停用词的去除\n",
    "    data = [word for word in data if word not in stop_words ]\n",
    "    \n",
    "#     data =  ' '.join(data) + '\\n'\n",
    "    return data\n",
    "\n",
    "def data_process(head,data,save_path=''):\n",
    "    '''数据处理第一大部分'''\n",
    "    cleared = data[head].apply(clear_data)\n",
    "    splited = cleared.apply(split_data)\n",
    "    \n",
    "    return splited\n",
    "\n",
    "#     with open('./{}'.format(save_path),'w',encoding='utf8') as f:\n",
    "#         f.writelines(splited_title)\n",
    "#         print('\\nTo {} processed.'.format(save_path))\n",
    "\n",
    "# 统计空title\n",
    "if not os.path.exists('./my_data'):\n",
    "    os.makedirs('./my_data')\n",
    "    print('创建文件夹({})成功'.format('/my_data'))\n",
    "        \n",
    "    \n",
    "# data_process('title',save_path='./my_data/titles_split.txt')\n",
    "# data_process('content',save_path='./my_data/contents_split.txt')\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_word2vec(data,n_dims=200,w2v_path='./models/w2v_model.pkl'):\n",
    "    '''进行word2vec模型的获取'''\n",
    "    if not os.path.exists(w2v_path):\n",
    "        print('\\n# Train the word2vec ')\n",
    "        core_count = cpu_count()\n",
    "        w2v = Word2Vec(size=n_dims,min_count=5,workers=core_count)\n",
    "        w2v.build_vocab(data)\n",
    "        w2v.train(data,total_examples=w2v.corpus_count,epochs=128)\n",
    "        w2v.save(w2v_path)\n",
    "        print('# Finish train the word2vec')\n",
    "    else:\n",
    "        w2v = Word2Vec.load(w2v_path)\n",
    "        \n",
    "    \n",
    "    return w2v\n",
    "\n",
    "def get_tf_idf(data,model_path='./models/tf_idf_model.pkl'):\n",
    "    '''进行tfidf模型的获取'''\n",
    "    if not os.path.exists(model_path):\n",
    "        tf_model = TfidfVectorizer(ngram_range=(1,2),token_pattern=r'(?u)\\b\\w+\\b\"')\n",
    "        tf_model.fit(data)\n",
    "        pickle.dump(tf_model,open(model_path,'wb'))\n",
    "        print('# Finish train the Tf-idf')\n",
    "    else:\n",
    "        tf_model = pickle.load(open(model_path,'rb'))\n",
    "    \n",
    "    return tf_model\n",
    "\n",
    "def build_tfidf_vec(sent,tfidf):\n",
    "    '''获取tfidf向量'''\n",
    "    return tfidf.transform([sent]).todense()[0]\n",
    "\n",
    "\n",
    "def build_word_vec(sent,w2v):\n",
    "    '''获取word2vec向量'''\n",
    "    vec = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec.append(w2v[word])\n",
    "        except:\n",
    "            vec.append(np.zeros(200))\n",
    "    if len(vec) != 0:\n",
    "        vec = np.mean(vec,axis=0)\n",
    "    else:\n",
    "        vec = np.zeros(200)\n",
    "    \n",
    "    return vec\n",
    "\n",
    "\n",
    "def convert2vec(data):    \n",
    "    '''把处理后的文本转换成向量'''\n",
    "    '''注意：再使用word2vec的时候split_data和使用tfidf的时候方法里面需要进行就该，split_data函数使用tfidf时加入代码\" \".join(data) '''\n",
    "    ## word2vec  \n",
    "    tmp1 = data_process('title',data).apply(list)\n",
    "    tmp2 = data_process('content',data).apply(list)\n",
    "    data.drop(['id','title','content'],axis=1,inplace=True)\n",
    "    w2v = get_word2vec(tmp1 + tmp2)\n",
    "\n",
    "    ## tf-idf   \n",
    "#     tmp1 = data_process('title',data)\n",
    "#     tmp2 = data_process('content',data)\n",
    "#     tfidf = get_tf_idf(tmp1 + tmp1)\n",
    "    \n",
    "    tmp_size = 200\n",
    "    for i in range(int(np.ceil(len(tmp1 + tmp2) / tmp_size))):\n",
    "        tmp1[i * tmp_size: i * tmp_size + tmp_size] = tmp1[i * tmp_size: i * tmp_size + tmp_size].apply(build_word_vec,args=(w2v,))\n",
    "        tmp2[i * tmp_size: i * tmp_size + tmp_size] = tmp2[i * tmp_size :i * tmp_size + tmp_size].apply(build_word_vec,args=(w2v,))\n",
    "        \n",
    "#         tmp1[i * tmp_size: i * tmp_size + tmp_size] = tmp1[i * tmp_size: i * tmp_size + tmp_size].apply(build_tfidf_vec,args=(tfidf,))\n",
    "#         tmp2[i * tmp_size: i * tmp_size + tmp_size] = tmp2[i * tmp_size :i * tmp_size + tmp_size].apply(build_tfidf_vec,args=(tfidf,))\n",
    "    \n",
    "    data['allVec'] = list(np.concatenate([list(tmp1),list(tmp2)],axis=1))  # 这里转换成list再存进df\n",
    "    \n",
    "    sum_vec = data.groupby(['date']).sum().reset_index()\n",
    "    count_vec = data.groupby(['date']).count().reset_index()['allVec']\n",
    "    \n",
    "    sum_vec['allVec'] = sum_vec['allVec'] / count_vec\n",
    "    \n",
    "    print('# Finish build word_vec')\n",
    "    \n",
    "    return sum_vec\n",
    "\n",
    "new_data = convert2vec(Data1)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Data2.y = Data2.y.astype(str)\n",
    "Data2.trade_date = Data2.trade_date.astype(str)\n",
    "\n",
    "Data = Data2.loc[:,['name','y','trade_date']].groupby('trade_date').sum()['y'].reset_index()\n",
    "Data.set_index(Data.trade_date,inplace=True)\n",
    "\n",
    "new_data['date'] = pd.to_datetime(new_data['date'],format='%Y%m%d')\n",
    "new_data.set_index(new_data['date'],inplace=True)\n",
    "\n",
    "#  把对应的过去一段时间内10天的数据合并进第一个表\n",
    "Data['allVec'] = None\n",
    "\n",
    "for date in Data.index:\n",
    "    temp_data = new_data.truncate(after=date)[-20:-10]\n",
    "    Data.set_value(date,'allVec',temp_data.allVec.mean())\n",
    "\n",
    "# 去除不完整数据\n",
    "Data = Data.dropna()\n",
    "\n",
    "\n",
    "def fix_y_1(y):\n",
    "    '''处理y值成格式1'''\n",
    "    def map_f(x):\n",
    "        if x == '1':\n",
    "            return [1,0]\n",
    "        else:\n",
    "            return [0,1]\n",
    "    return list(map(map_f,y))\n",
    "    \n",
    "def fix_y_2(y):\n",
    "    '''处理y值成格式2'''\n",
    "    labels = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1':\n",
    "            labels.append(i)\n",
    "    return labels\n",
    "\n",
    "Data['y_'] = Data['y'].apply(lambda x: list(map(int,x)))\n",
    "\n",
    "\n",
    "pre_date = ['20190402','20190403','20190404','20190408','20190409']\n",
    "def set_val_data(date_list,new_data):\n",
    "    val_data = pd.DataFrame(columns=['vec'],index=pre_date)\n",
    "\n",
    "    for date in date_list:\n",
    "        temp_data = new_data.truncate(before=20190402)[-20:-10]\n",
    "        val_data.set_value(date,'vec',temp_data.allVec.mean())\n",
    "    \n",
    "    return val_data\n",
    "\n",
    "val_data = []\n",
    "for i in [1,2,3,7,8]:\n",
    "    val_data.append(new_data.allVec[-20 + i: -10 + i].mean())\n",
    "val_data = pd.DataFrame(val_data)\n",
    "\n",
    "\n",
    "input_x = np.array(list(Data['allVec']))\n",
    "input_y = np.array(list(Data['y_']))[:,0]\n",
    "input_y\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fit 0...\n",
      "# fit 1...\n",
      "# fit 2...\n",
      "# fit 3...\n",
      "# fit 4...\n",
      "# fit 5...\n",
      "# fit 6...\n",
      "# fit 7...\n",
      "# fit 8...\n",
      "# fit 9...\n",
      "# fit 10...\n",
      "# fit 11...\n",
      "# fit 12...\n",
      "# fit 13...\n",
      "# fit 14...\n",
      "# fit 15...\n",
      "# fit 16...\n",
      "# fit 17...\n",
      "# fit 18...\n",
      "# fit 19...\n",
      "# fit 20...\n",
      "# fit 21...\n",
      "# fit 22...\n",
      "# fit 23...\n",
      "# fit 24...\n",
      "# fit 25...\n",
      "# fit 26...\n",
      "# fit 27...\n",
      "# fit 28...\n",
      "# fit 29...\n",
      "# fit 30...\n",
      "# fit 31...\n",
      "# fit 32...\n",
      "# fit 33...\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "result = []\n",
    "for i,ts in enumerate(Data2.ts_code.unique()):\n",
    "    input_x = np.array(list(Data['allVec']))\n",
    "    input_y = np.array(list(Data['y_']))[:,i]\n",
    "    input_x = ss.fit_transform(input_x)\n",
    "    \n",
    "    model = XGBClassifier(learn_rate=0.1,n_estimators=500,seed=64)\n",
    "    print('# fit {}...'.format(i))\n",
    "    model.fit(input_x,input_y)\n",
    "    pred = model.predict_proba(np.array(val_data))\n",
    "    \n",
    "    for _date,_pred in zip(pre_date,pred):\n",
    "        result.append([ts,_date,_pred[1]])\n",
    "result = pd.DataFrame(result,columns=['ts_code','trade_date','p'])\n",
    "result.to_csv('result2_ss.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-t",
   "language": "python",
   "name": "tensorflow-t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
